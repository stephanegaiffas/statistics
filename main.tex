
\documentclass[
	fontsize=11pt, % Base font size
	twoside=false, % Use different layouts for even and odd pages (in particular, if twoside=true, the margin column will be always on the outside)
	%open=any, % If twoside=true, uncomment this to force new chapters to start on any page, not only on right (odd) pages
	%chapterprefix=true, % Uncomment to use the word "Chapter" before chapter numbers everywhere they appear
	%chapterentrydots=true, % Uncomment to output dots from the chapter name to the page number in the table of contents
	numbers=noenddot, % Comment to output dots after chapter numbers; the most common values for this option are: enddot, noenddot and auto (see the KOMAScript documentation for an in-depth explanation)
	%draft=true, % If uncommented, rulers will be added in the header and footer
	%overfullrule=true, % If uncommented, overly long lines will be marked by a black box; useful for correcting spacing problems
]{kaobook}

% Set the language
\usepackage[english]{babel} % Load characters and hyphenation
\usepackage[english=british]{csquotes} % English quotes

\usepackage{bm}


% txfonts/pxfonts

% Load packages for testing
\usepackage{blindtext}
%\usepackage{showframe} % Uncomment to show boxes around the text area, margin, header and footer
%\usepackage{showlabels} % Uncomment to output the content of \label commands to the document where they are used

% Load the bibliography package
\usepackage{styles/kaobiblio}
\addbibresource{biblio.bib} % Bibliography file

% Load mathematical packages for theorems and related environments. NOTE: choose only one between 'mdftheorems' and 'plaintheorems'.
\usepackage{styles/mdftheorems}
%\usepackage{styles/plaintheorems}


\graphicspath{{images/}} % Paths in which to look for images

% \RequirePackage{times}

\DeclareMathOperator{\cA}{{\mathcal A}}
\DeclareMathOperator{\cB}{{\mathcal B}}
\DeclareMathOperator{\cC}{{\mathcal C}}
\DeclareMathOperator{\cD}{{\mathcal D}}
\DeclareMathOperator{\cE}{{\mathcal E}}
\DeclareMathOperator{\cF}{{\mathcal F}}
\DeclareMathOperator{\cG}{{\mathcal G}}
\DeclareMathOperator{\cM}{{\mathcal M}}
\DeclareMathOperator{\cN}{{\mathcal N}}
\DeclareMathOperator{\cP}{{\mathcal P}}
\DeclareMathOperator{\cX}{{\mathcal X}}
\DeclareMathOperator{\cY}{{\mathcal Y}}
\DeclareMathOperator{\cZ}{{\mathcal Z}}

\DeclareMathOperator{\bA}{{\boldsymbol A}}
\DeclareMathOperator{\bB}{{\boldsymbol B}}
\DeclareMathOperator{\bC}{{\boldsymbol C}}
\DeclareMathOperator{\bD}{{\boldsymbol D}}
\DeclareMathOperator{\bE}{{\boldsymbol E}}
\DeclareMathOperator{\bF}{{\boldsymbol F}}
\DeclareMathOperator{\bG}{{\boldsymbol G}}
\DeclareMathOperator{\bH}{{\boldsymbol H}}
\DeclareMathOperator{\bI}{{\boldsymbol I}}
\DeclareMathOperator{\bJ}{{\boldsymbol J}}
\DeclareMathOperator{\bK}{{\boldsymbol K}}
\DeclareMathOperator{\bL}{{\boldsymbol L}}
\DeclareMathOperator{\bM}{{\boldsymbol M}}
\DeclareMathOperator{\bN}{{\boldsymbol N}}
\DeclareMathOperator{\bO}{{\boldsymbol O}}
\DeclareMathOperator{\bP}{{\boldsymbol P}}
\DeclareMathOperator{\bQ}{{\boldsymbol Q}}
\DeclareMathOperator{\bR}{{\boldsymbol R}}
\DeclareMathOperator{\bS}{{\boldsymbol S}}
\DeclareMathOperator{\bT}{{\boldsymbol T}}
\DeclareMathOperator{\bU}{{\boldsymbol U}}
\DeclareMathOperator{\bV}{{\boldsymbol V}}
\DeclareMathOperator{\bW}{{\boldsymbol W}}
\DeclareMathOperator{\bX}{{\boldsymbol X}}
\DeclareMathOperator{\bY}{{\boldsymbol Y}}
\DeclareMathOperator{\bZ}{{\boldsymbol Z}}

\DeclareMathOperator{\ba}{{\boldsymbol a}}
\DeclareMathOperator{\bb}{{\boldsymbol b}}
\DeclareMathOperator{\bc}{{\boldsymbol c}}
\DeclareMathOperator{\bd}{{\boldsymbol d}}
\DeclareMathOperator{\be}{{\boldsymbol e}}
\renewcommand{\bf}{{\boldsymbol f}}
\DeclareMathOperator{\bg}{{\boldsymbol g}}
\DeclareMathOperator{\bh}{{\boldsymbol h}}
\DeclareMathOperator{\bi}{{\boldsymbol i}}
\DeclareMathOperator{\bj}{{\boldsymbol j}}
\DeclareMathOperator{\bk}{{\boldsymbol k}}
\DeclareMathOperator{\bl}{{\boldsymbol l}}
% \DeclareMathOperator{\bm}{{\boldsymbol m}}
\DeclareMathOperator{\bn}{{\boldsymbol n}}
\DeclareMathOperator{\bo}{{\boldsymbol o}}
\DeclareMathOperator{\bp}{{\boldsymbol p}}
\DeclareMathOperator{\bq}{{\boldsymbol q}}
\DeclareMathOperator{\br}{{\boldsymbol r}}
\DeclareMathOperator{\bs}{{\boldsymbol s}}
\DeclareMathOperator{\bt}{{\boldsymbol t}}
\DeclareMathOperator{\bu}{{\boldsymbol u}}
\DeclareMathOperator{\bv}{{\boldsymbol v}}
\DeclareMathOperator{\bw}{{\boldsymbol w}}
\DeclareMathOperator{\bx}{{\boldsymbol x}}
\DeclareMathOperator{\by}{{\boldsymbol y}}
\DeclareMathOperator{\bz}{{\boldsymbol z}}

\DeclareMathOperator{\bLambda}{{\boldsymbol \Lambda}}

\DeclareMathOperator{\bone}{\boldsymbol 1}

\DeclareMathOperator{\beps}{\boldsymbol \varepsilon}
\DeclareMathOperator{\bSigma}{\boldsymbol \Sigma}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\mode}{mode}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\med}{med}
\DeclareMathOperator{\pen}{pen}
\DeclareMathOperator{\inte}{int}
\DeclareMathOperator{\dom}{dom}


\DeclareMathOperator{\ber}{Bernoulli}
\DeclareMathOperator{\bet}{Beta}
\DeclareMathOperator{\bin}{Binomial}
\DeclareMathOperator{\chisq}{ChiSq}
\DeclareMathOperator{\expo}{Exponential}
\DeclareMathOperator{\fis}{Fisher}
\DeclareMathOperator{\gam}{Gamma}
\DeclareMathOperator{\mul}{Multinomial}
\DeclareMathOperator{\nor}{Normal}
\DeclareMathOperator{\stu}{Student}
\DeclareMathOperator{\uni}{Uniform}

\DeclareMathOperator{\new}{new}

\DeclareMathOperator{\remain}{remainder}



\DeclareMathOperator{\sigmoid}{sigmoid}
\DeclareMathOperator{\leb}{Lebesgue}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\DeclareMathOperator*{\spa}{span}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\cov}{cov}

\newcommand{\eps}{\varepsilon}

\renewcommand{\P}{\mathbb P}
\newcommand{\E}{\mathbb E}
\newcommand{\R}{\mathbb R}
\newcommand{\N}{\mathbb N}
\newcommand{\var}{\mathbb V}

\newcommand{\wh}{\widehat}
\newcommand{\wt}{\widetilde}

\newcommand{\ind}[1]{\mathbf 1_{#1}}
\newcommand{\grad}{\nabla}


\newcommand{\mgeq}{\succcurlyeq}
\newcommand{\mleq}{\preccurlyeq}
\newcommand{\goes}{\rightarrow}
\newcommand{\go}{\rightarrow}

\newcommand{\norm}[1]{\| #1 \|}
\newcommand{\inr}[1]{\langle #1 \rangle}

\newcommand{\gopro}{\overset{\P}{\rightarrow}}
\newcommand{\goas}{\overset{\text{as\ }}{\rightarrow}}
\newcommand{\goqr}{\overset{\text{$L^2$\ }}{\rightarrow}}
\newcommand{\gosto}{\leadsto}


\DeclareMathOperator{\conv}{conv}

% \newcommand{\lest}{ \underset{\text{st}}{\leq}}
\newcommand{\lest}{\preceq}
% \newcommand{\gest}{\underset{\text{st}}{\qeq}}
\newcommand{\gest}{\succeq}


\DeclareMathOperator{\sign}{sign}

% \makeindex[columns=3, title=Alphabetical Index, intoc] % Make LaTeX produce the files required to compile the index

% \makeglossaries % Make LaTeX produce the files required to compile the glossary

% \makenomenclature % Make LaTeX produce the files required to compile the nomenclature

% Reset sidenote counter at chapters
%\counterwithin*{sidenote}{chapter}

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	BOOK INFORMATION
%----------------------------------------------------------------------------------------

\titlehead{Some stuff about statistics}
\subject{Lecture notes for the ENS course of Statistics}

\title[Some stuff about Statistics]{Some stuff about Statistics}
% \subtitle{Customise this page according to your needs}

\author[St\'ephane Ga\"iffas"]{St\'ephane Ga\"iffas\thanks{}}

\date{\today}

\publishers{}

%----------------------------------------------------------------------------------------

\frontmatter % Denotes the start of the pre-document content, uses roman numerals

%----------------------------------------------------------------------------------------
%	OPENING PAGE
%----------------------------------------------------------------------------------------

%\makeatletter
%\extratitle{
%	% In the title page, the title is vspaced by 9.5\baselineskip
%	\vspace*{9\baselineskip}
%	\vspace*{\parskip}
%	\begin{center}
%		% In the title page, \huge is set after the komafont for title
%		\usekomafont{title}\huge\@title
%	\end{center}
%}
%\makeatother

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

% \makeatletter
% \uppertitleback{\@titlehead} % Header

% \lowertitleback{
% 	\textbf{Disclaimer}\\
% 	You can edit this page to suit your needs. For instance, here we have a no copyright statement, a colophon and some other information. This page is based on the corresponding page of Ken Arroyo Ohori's thesis, with minimal changes.
	
% 	\medskip
	
% 	\textbf{No copyright}\\
% 	\cczero\ This book is released into the public domain using the CC0 code. To the extent possible under law, I waive all copyright and related or neighbouring rights to this work.
	
% 	To view a copy of the CC0 code, visit: \\\url{http://creativecommons.org/publicdomain/zero/1.0/}
	
% 	\medskip
	
% 	\textbf{Colophon} \\
% 	This document was typeset with the help of \href{https://sourceforge.net/projects/koma-script/}{\KOMAScript} and \href{https://www.latex-project.org/}{\LaTeX} using the \href{https://github.com/fmarotta/kaobook/}{kaobook} class.
	
% 	The source code of this book is available at:\\\url{https://github.com/fmarotta/kaobook}
	
% 	(You are welcome to contribute!)
	
% 	\medskip
	
% 	\textbf{Publisher} \\
% 	First printed in May 2019 by \@publishers
% }
% \makeatother

%----------------------------------------------------------------------------------------
%	DEDICATION
%----------------------------------------------------------------------------------------

% \dedication{
% 	The harmony of the world is made manifest in Form and Number, and the heart and soul and all the poetry of Natural Philosophy are embodied in the concept of mathematical beauty.\\
% 	\flushright -- D'Arcy Wentworth Thompson
% }

%----------------------------------------------------------------------------------------
%	OUTPUT TITLE PAGE AND PREVIOUS
%----------------------------------------------------------------------------------------

% Note that \maketitle outputs the pages before here

% If twoside=false, \uppertitleback and \lowertitleback are not printed
% To overcome this issue, we set twoside=semi just before printing the title pages, and set it back to false just after the title pages
\KOMAoptions{twoside=semi}
\maketitle
\KOMAoptions{twoside=false}

%----------------------------------------------------------------------------------------
%	PREFACE
%----------------------------------------------------------------------------------------

\chapter*{Preface}
\addcontentsline{toc}{chapter}{Preface} % Add the preface to the table of contents as a chapter


The aim of this course is, as the title indicated, to learn some stuff about statistics, and to try to exhibit some good looking mathematics from this field of applied mathematics, beyond convincing you that statistics are useful\sidenote{We won't list here, exhaustively, the numerous fields that make a regular use of mathematical statistics: marketing, medicine and more broadly health, finance, insurance, banking, etc.}

We will try to provide, all along the course, at material featuring 60\% of classical and unavoidable material from a course about statistics, and 40\% of more recent research results and some open questions.

The tentative agenda for the course is as follows:

\begin{itemize}
 	\item Modelization and the main statistical inference problems (estimation, confidence regions and tests)
 	\item Gaussian vectors and the Gaussian linear model
 	\item Theoretical guarantees and the optimality of least-squares
 	\item Estimation methods: methods of moments, maximum likelihood and other things
 	\item Exponential models and generalized linear models, logistic regression (optimal rates and some open questions)
 	\item Tests and multiple tests
 \end{itemize} 

% I am of the opinion that every \LaTeX\xspace geek, at least once during 
% his life, feels the need to create his or her own class: this is what 
% happened to me and here is the result, which, however, should be seen as 
% a work still in progress. Actually, this class is not completely 
% original, but it is a blend of all the best ideas that I have found in a 
% number of guides, tutorials, blogs and tex.stackexchange.com posts. In 
% particular, the main ideas come from two sources:

% \begin{itemize}
% 	\item \href{https://3d.bk.tudelft.nl/ken/en/}{Ken Arroyo Ohori}'s 
% 	\href{https://3d.bk.tudelft.nl/ken/en/nl/ken/en/2016/04/17/a-1.5-column-layout-in-latex.html}{Doctoral 
% 	Thesis}, which served, with the author's permission, as a backbone 
% 	for the implementation of this class;
% 	\item The 
% 		\href{https://github.com/Tufte-LaTeX/tufte-latex}{Tufte-Latex 
% 			Class}, which was a model for the style.
% \end{itemize}

% The first chapter of this book is introductive and covers the most 
% essential features of the class. Next, there is a bunch of chapters 
% devoted to all the commands and environments that you may use in writing 
% a book; in particular, it will be explained how to add notes, figures 
% and tables, and references. The second part deals with the page layout 
% and design, as well as additional features like coloured boxes and 
% theorem environments.

% I started writing this class as an experiment, and as such it should be 
% regarded. Since it has always been indended for my personal use, it may 
% not be perfect but I find it quite satisfactory for the use I want to 
% make of it. I share this work in the hope that someone might find here 
% the inspiration for writing his or her own class.

\begin{flushright}
	\textit{St\'ephane Ga\"iffas}
\end{flushright}


%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LIST OF FIGURES/TABLES
%----------------------------------------------------------------------------------------

\begingroup % Local scope for the following commands

% Define the style for the TOC, LOF, and LOT
%\setstretch{1} % Uncomment to modify line spacing in the ToC
%\hypersetup{linkcolor=blue} % Uncomment to set the colour of links in the ToC
\setlength{\textheight}{23cm} % Manually adjust the height of the ToC pages

% Turn on compatibility mode for the etoc package
\etocstandarddisplaystyle % "toc display" as if etoc was not loaded
\etocstandardlines % toc lines as if etoc was not loaded

\tableofcontents % Output the table of contents

% \listoffigures % Output the list of figures

% Comment both of the following lines to have the LOF and the LOT on different pages
% \let\cleardoublepage\bigskip
% \let\clearpage\bigskip

% \listoftables % Output the list of tables

\endgroup

%----------------------------------------------------------------------------------------
%	MAIN BODY
%----------------------------------------------------------------------------------------

\mainmatter % Denotes the start of the main document content, resets page numbering and uses arabic numbers
\setchapterstyle{kao} % Choose the default chapter heading style


% \input{chap02_statistical_models}

% \input{chap03_statistical_inference}

% \input{chap04_linear_regression}

% \input{chap05_bayesian_statistics}

% \input{chap06_lasso}

\setchapterpreamble[u]{\margintoc}
\chapter{High dimensional statistics and sparsity}
\label{chap:high_dimensional_statistics}

This chapter is about some recent advances in the field of high-dimensional statistics, which corresponds to a setting where the sample size $n$ is smaller than the number of features $d$.
Let us consider again the Gaussian linear model (see Chapter~\ref{chap:linear_regression}), where we observe labels satisfying
\begin{equation*}
	Y_i = f(X_i) + \eps_i
\end{equation*}
for $i=1, \ldots, n$, where $X_i \in \R^d$ are vectors of features that we assume deterministic, where $\eps_1, \ldots, \eps_n$ are i.i.d $\nor(0, \sigma^2)$ random variables and where $f$ is the regression function that we want to estimate.


\paragraph{Sparse estimation.}

We consider a set $\cF = \{ f_1, \ldots, f_M \}$ of functions called a \emph{dictionary}, with $M$ which can be much larger than the sample size $n$.  
We want to learn from data an estimator of $f$ of the form
\begin{equation*}
	f_\theta(x) = \sum_{j=1}^M \theta_j f_j(x)
\end{equation*}
with the following properties: the \emph{empirical} estimation error
\begin{equation*}
	\frac 1n \sum_{i=1}^n (f_\theta(X_i) - f(X_i))^2
\end{equation*}
is small and the sparsity of $\theta$, namely
\begin{equation*}
	\norm{\theta}_0 = |J(\theta)| = | \{ j=1, \ldots, M : \theta_j \neq 0\} |
\end{equation*}
where $|J|$ stands for the cardinality of a set $J$, is small compared to $M$.
If we are able to satisfy both points, we say that we can find a \emph{sparse} linear combination of elements of $\cF$ to estimate $f$.
This task is called \emph{sparse coding} or \emph{sparse estimation}, since it would allow to select a subset of elements from a typically redundant dictionary $\cF$ to estimate~$f$.
Of course, if $M = d$ and $f_j(x) = x_j$, we recover the standard linear regression model, where $f_\theta(x) = x^\top \theta$.

Let us introduce a bunch of notations before diving into the main matter.
Here, the features matrix is a $n \times M$ matrix given by
\begin{equation*}
	\bX = 
	\begin{bmatrix}
		f_1(X_1) & \cdots & f_M(X_1) \\
		\vdots & \ddots & \vdots \\
		f_1(X_n) & \cdots & f_M(X_n)
	\end{bmatrix}	
	=
	\begin{bmatrix}
		\bX_1^\top \\
		\vdots \\
		\bX_n^\top 
	\end{bmatrix}
	= 
	\begin{bmatrix}
		\bX^1 \cdots \bX^M
	\end{bmatrix}.
\end{equation*}
Let us introduce also
\begin{equation*}
	\by =
	\begin{bmatrix}
		Y_1 \\
		\vdots \\
		Y_n
	\end{bmatrix},
	\quad
	\bf = 
	\begin{bmatrix}
		f(X_1) \\
		\vdots \\
		f(X_n)
	\end{bmatrix},
	\quad
	\bf_\theta =	
	\begin{bmatrix}
		f_\theta(X_1) \\
		\vdots \\
		f_\theta(X_n)
	\end{bmatrix},
	\quad
	\beps = 
	\begin{bmatrix}
		\eps_1 \\
		\vdots \\
		\eps_n
	\end{bmatrix}.
\end{equation*}
The problem can be rewritten as the Gaussian linear model
\begin{equation*}
	\by = \bX \theta + \beps,
\end{equation*}
from Chapter~\ref{chap:linear_regression}, however this time we can have $M \gg n$, namely the matrix $\bX$ can be overdetermined, which means that the dictionary $\cF$ is redundant.
The notation $\norm{u}$ will stand for the Euclidean norm of $u \in \R^n$.

\paragraph{Oracle inequalities.}

We are looking for an estimator $\wh \theta_n$ such that $\norm{\wh \theta_n}_0 \ll M$ and
\begin{equation}
	\label{eq:oracle-remainder}
	\norm{\bf_{\wh \theta_n} - \bf}^2 \leq \inf_{\theta \in \R^M} \Big\{ 
	\norm{\bf_\theta - \bf}^2 + \remain \Big\}
\end{equation}
where $\remain$ is, ideally, a small quantity that might depend on $n, \cF$ and $\sigma^2$.
If $\remain$ is small, then such an inequality would prove that the estimator $f_{\wh \theta_n}$ performs almost as well as the best linear combination of elements from the dictionary $\inf_{\theta \in \R^M} \norm{\bf_\theta - \bf}^2$.
Such an inequality is called an \emph{oracle inequality}, an \emph{oracle} being a best-performing linear combination $f^\star = f_{\theta^\star}$ with $\theta^\star \in \argmin_{\theta \in \R^M} \norm{\bf_\theta - \bf}^2$. 

This raises the following questions:
\begin{itemize}
	\item How can we construct a sparse estimator $\wh \theta_n$ ?
	\item What is the value of $\remain$ in the inequality~\eqref{eq:oracle-remainder} ?
\end{itemize}
We will deal with this problem using a penalization.
We already talked about the Ridge penalization in Chapter~\ref{??}, which corresponds to the estimator
\begin{equation}
	\label{eq:chap-lasso-ridge-estimator}
	\wh \theta_n^{\mathsf{ridge}} = \argmin_{\theta \in \R^M} 
	\Big\{ \frac 1n \sum_{i=1}^n (Y_i - f_\theta(X_i))^2 + \frac{\lambda}{2} \norm{\theta}_2^2 \Big\},
\end{equation}
where $\norm{\theta}_2$ is the Euclidean norm of $\theta$, also called $\ell_2$ norm.
We say in Chapter~? that this penalization can be understood as an isotropic Gaussian prior in the Gaussian linear model.

In this Chapter, we will consider another penalization: the $\ell_1$ norm, since as explined in what follows, it allows to formulate convex problem leading to a sparse estimator $\wh \theta_n$, coming from a \emph{convex relaxation} principle. BLABAL

The estimator we will study is given by
\begin{equation}
	\label{eq:lasso-def}
		\wh \theta_n^{\mathsf{ridge}} = \argmin_{\theta \in \Theta} 
		\Big\{ \frac 1n \sum_{i=1}^n (Y_i - f_\theta(X_i))^2 + \lambda \norm{\theta}_1 \Big\},
\end{equation}
where $\norm{\theta}_1 = \sum_{j=1}^M |\theta_j|$ is the $\ell_1$ norm of $\theta$ and where $B \subset \R^M$ is a convex set, main examples being
\begin{itemize}
 	\item The whole set $B = \R^M$ (no constraints on $\theta$)
 	\item The set $B = [0, +\infty)^M$ (positivity constraints on $\theta$)
 \end{itemize}


It is tempting to use as a penalization the "$\ell_0$ norm", but this leads to a problem where we would need to try out all subset $J \subset \{ 1, \ldots, M \}$ and train a linear model on it, BLABLA, which means $2^M$ problems to solve.

The $\ell_1$ can be understood as a \emph{convex relaxation} of $\ell_0$.
Indeed, the \emph{convex envelope}%
\sidenote{The convex envelope is the largest convex function which minorizes BLABLA}
of the function $g_0(x) = 1_{x \neq 0}$ over the interval $[-1, 1]$ is given by $g_1(x) = |x|$, so that the convex envelope of $x \mapsto \norm{x}_0$ over $[-1, 1]^M$ is $\norm{x}_1$.

\begin{marginfigure}
	\includegraphics{assets/l0_l1.pdf}
	\caption{The convex envelope of $x \mapsto \norm{x}_0$ over $[-1, 1]$ is $x \mapsto \norm{x}_1$.}
	\label{fig:l0-l1}
\end{marginfigure}


\todo{Laplace Prior}

The $\ell_1$ norm therefore appears naturally as a convex relaxation of $\ell_0$.
Another way of understanding it is to consider the following constrained optimization problem
\begin{align*}
	\min \quad &\norm{x}_0 \\ 
	\text{such that} \quad &x \in C \quad \text{and} \quad \norm{x}_\infty \leq R
\end{align*}
which can be reformulated as
\begin{align*}
	\min \quad &\bone^\top u \\ 
	\text{such that} \quad &u \in \{ 0, 1 \}^M, \quad |x_i| \leq R u_i 
	\quad \text{for all} \quad i=1, \ldots, M \\
	&x \in C
\end{align*}
Such an optimization problem, called a "linear mixed integer program" is hard to solve, since it requires to try out all possible vectors $u \in \{ 0, 1 \}^M$.
A convex relaxation of this problem is
\begin{align*}
	\min \quad &\bone^\top u \\ 
	\text{such that} \quad &u \in [0, 1]^M, \quad |x_i| \leq R u_i 
	\quad \text{for all} \quad i=1, \ldots, M \\
	&x \in C
\end{align*}
which can be rewritten as
\begin{align*}
	\min \quad &\frac 1R \norm{x}_1 \\ 
	\text{such that} \quad &x \in C \quad \text{and} \quad \norm{x}_\infty 
	\leq R,
\end{align*}
where we see that, once again, the $\ell_1$ norm naturally appear.

\paragraph{Soft-thresholding.}

A straightforward computation allows to understand that the $\ell_1$ norm induces sparsity.
Indeed, we can see that%
\sidenote{TODO: insert proof}
\begin{equation}
	\label{eq:soft-thresholding1}
	\argmin_{a \in R} \frac12 (a - b)^2 + \lambda |a| 
	= \sign(b) (|b| - \lambda)_+,
\end{equation}
where $x_+ = \max(x, 0)$ and $\sign(x) = 1$ if $x > 0$, $\sign(x) = -1$ if $x < 0$ and $\sign(0) = 0$.
This proves that%
\sidenote{Since the function is separable}
\begin{equation}
	\label{eq:soft-thresholding2}
	\argmin_{a \in \R^M} \frac12 \norm{a - b}_2^2 + \lambda \norm{a}_1 
	T_\lambda(b) \quad \text{where} \quad 
	(T_\lambda(b))_j = \sign(b_j) \odot (|b_j| - \lambda)_+,
\end{equation}
The operator $T_\lambda : \R^M \rightarrow \R^M$ is called the soft-thresholding operator.
See Figure~\ref{fig:soft-thresholding}
\begin{marginfigure}
	\includegraphics{assets/soft_thresholding.pdf}
	\caption{Soft-thresholding and shrinkage with $\lambda = 1$ on a single coordinate.}
	\label{fig:soft-thresholding}
\end{marginfigure}


It is the operator involved in the proximal gradient algorithm
\todo{descente de gradient proximal}
This function is called soft thresholding, and we see that it indeed induces sparsity in $\theta$.
The fact that the $\ell_1$ norm induces sparsity (namely coordinates with 0) comes precisely from the fact that the absolute value is not differentiable at $0$.
It can also be understood geometrically by the fact that the $\ell_1$ norm has "spiky" corners at $\pm e_j$ for $j=1, \ldots, M$: when we project a point onto an $\ell_1$ ball, we are likely to obtained a projection that ends at one of the spiky points or a , which are sparse points  BLABLA

\todo{dessin boule l1}

All of the above discussions motivate the use of the $\ell_1$ norm to induce sparsity.
Let us therefore consider the estimator given by
\begin{equation}
	\label{eq:lasso-def2}
	\wh \theta_n = \argmin_{\theta \in \Theta} \Big\{ 
	\frac 1n \sum_{i=1}^n (Y_i - f_\theta(X_i))^2 
	+ \lambda \norm{\theta}_1 
	\Big\}.
\end{equation}
From now on, we will assume that the features are standardized, which means that the columns of $\bX$ satisfy $\norm{\bX^j}_2^2 = 1$ for all $j=1, \ldots, M$.

This estimator is called the Lasso BLABLA + reference + reference francis + expliciter l'optimisation par descente de gradient, descente de gradient par coordonnees

Before studying the we first need to provide some tools from convex optimization.

\todo{Chemin regularization du Lasso}

\section{Some tools from convex optimization} % (fold)
\label{sec:some_tools_from_convex_optimization}

\todo{tous les $x$ sont des $u$ ou $v$ et $\phi$ pour fonction cvx}

Let us consider a convex function $\phi : \R^d \rightarrow \R$.
A fundamental notion which generalizes the gradient is the subdifferential, as explained in the next definition.
\begin{definition}
	\label{def:subdifferential}
	We say that $g \in \R^d$ is a \emph{subgradient} at $u \in \R^d$ if and only if
	\begin{equation}
	 	\phi(v) - \phi(u) \geq g^\top (v - u)
	 \end{equation} 
	 for any $v \in \R^d$.
	 The set of all subgradients
	 \begin{equation*}
	 	\partial \phi(u) = \{ g \in \R^d : \phi(v) - \phi(u) \geq g^\top (v -u) \quad \text{for all} v in \R^d \}
	 \end{equation*}
	 is called the subdifferential of $\phi$ at $u$.
\end{definition}
For instance, we have for the absolute value that
\begin{equation*}
	\partial |u| = 
	\begin{cases}
		\{ 1 \} &\text{ if } x  > 0 \\
		\{ -1 \} &\text{ if } x < 0 \\
		[0, 1] &\text{ if } x  = 0.
	\end{cases}
\end{equation*}
\todo{dessin sous-differentielle}
Whenever $\phi$ is differentiable at $u$, we have obviously that $\partial \phi(u) = \{  \grad \phi(u) \}$.
Another obvious%
\sidenote{TODO: proof}
claim is that
\begin{equation*}
	u^\star \in \argmin_{u \in \R^d} \phi(u) \quad \text{if and only if} \quad 0 \in \partial \phi(u^\star).
\end{equation*}
Also, it is easy to see that
\begin{equation*}
	\partial \Big( \sum_{k=1}^K \alpha_k \phi_k(x) \Big) =  \alpha_k \sum_{k=1}^K \partial \phi_k(x)
\end{equation*}
whenever $\alpha_k \geq 0$ and $\phi_k$ is convex for all $k=1, \ldots, K$.
Another nice formula allows to express the subdifferential of a maximum of convex functions with the subdifferential of each function.
Indeed, if $\phi(x) = \max_{k=1}^K \phi_k(x)$, we have
\begin{equation*}
	\partial \phi(x) = \conv\Big( \bigcup_{i=1}^d \Big\{ \partial \phi_k(x) : \phi_k(x) = \phi(x) \Big\} \Big)
\end{equation*}
as illustrated in Figure~???
\todo{mettre figure}
which gives in particular for two differentiable functions $\phi_1 : \R \rightarrow \R$ and $\phi_2 : \R \rightarrow \R$
\begin{equation*}
	\partial \max(\phi_1, \phi_2)(x) = 
	\begin{cases}
	\{ \phi_2'(x) \} &\text{ if } \phi_2(x) > \phi_1(x) \\
	\{ \phi_1'(x) \} &\text{ if } \phi_2(x) < \phi_1(x) \\
	[ \phi_1'(x), \phi_2'(x) ] &\text{ if } \phi_2(x) = \phi_1(x) \\
	\end{cases}
\end{equation*}
\todo{dessin sous-diff du max}
Another important notation in convex optimization is the indicator function of a convex set $C \subset \R^d$
\begin{equation*}
	\delta_C(x) = \begin{cases}
		0 &\text{if } x \in C \\
		+\infty &\text{if } x \notin C.
	\end{cases}
\end{equation*}
If allows to reformulate a \emph{constrained} problem as an \emph{unconstrained} one, namely to rewrite
\begin{equation*}
	x^\star \in \argmin_{x \in C} \phi(x) \quad \text{ as } \quad x^\star \in \argmin_{x \in \R^d} \{ \phi(x) + \delta_C(x) \} 
\end{equation*}
which means that $0 \in \partial \phi(x^\star) + \partial \delta_C(x^\star)$, namely that there is $g^\star \in \partial \phi(x^\star)$ such that $-g^\star \in \partial \delta_C(x^\star)$.
But it is easy to understand what the subdifferential of the indicator function $\delta_C$ is, since $g \in \partial \delta_C(x^\star)$ with $x^\star \in C$ means%
\sidenote{using the definition of the subdifferential ????}
that
\begin{equation*}
	\delta_C(x) - \delta_C(x^\star) \geq q^\top (x - x^\star) \quad \text{for all} \quad x \in \R^d,
\end{equation*}
but $\delta_C(x^\star) = 0$ since $x^\star \in C$ so\todo{pas propre} that the subdifferential of the indicator function is given by
\begin{equation*}
	\partial \delta_C(v) = \{ g \in H : g^\top (v - u) \leq 0 \quad 
	\text{ for all } u \in C\}
\end{equation*}
which is the \emph{normal cone} to $C$ at $u$.
\todo{dessin code normal}
This proves that an optimality criterion for the problem ??? is given by
\begin{equation*}
	\exists g^\star \in \partial \phi(x^\star) \quad \text{ such that } \quad (g^\star)^\top (u - u^\star) \leq 0
\end{equation*}
\todo{j'ai un moins devant dans mes note}

Another property about the subdifferential is the following.
\begin{proposition}[Monotonicity of the subdifferential]
	Given a convex function $\phi : \R^d \rightarrow \R$, any $u_1, u_2 \in \R^d$ and any $g_1 \in \partial \phi(u_1)$ and $g_2 \in \phi(u_2)$, we have
	\begin{equation*}
		(u_1 - u_2)^\top (g_1 - g_2) \geq 0.
	\end{equation*}
\end{proposition}
The proof is straightforward.%
\sidenote{Just use Definition~? to write that $\phi(u_2) - \phi(u_1) \geq g_1^\top(u_2 - u_1)$ and that $\phi(u_1) - \phi(u_2) \geq g_2^\top(u_1 - u_2)$ and add the two.}

\paragraph{Subdifferential of $\ell_1$ norm.} 

Let us give as an example the computation of the subdifferential of the $\ell_1$ norm.
Put $\phi(u) = \norm{u}_1$ and note that it can be rewritten as
\begin{equation*}
	\norm{u}_1 = \max \big\{ e^\top u : e \in \{ -1, 1 \}^d \big\} = \max_{i=1, \ldots, 2^d} \phi_i(u)
\end{equation*}
where we introduced $\phi_i(u) = e_i^\top u$ for $e_1, \ldots, e_{2^d}$ the elements of $\{ -1, 1 \}^d$.
Note that given $u \in \R^d$, we can choose $e(u) \in \R^d$ such that $e(u)_j = 1$ if $u_j > 0$, $e(u)_j = -1$ if $u_j < 0$ and $e(u)_j = 1$ \emph{or} $e(u)_j = -1$ if $u_j = 0$, in order to obtain that $e(u)^\top u = \norm{u}_1$.
Let us introduce the set
\begin{equation*}
	I(u) = \big\{ i \in \{ 1, \ldots 2^d \} : e_i^\top u = \norm{u}_1 \big\}.
\end{equation*}
Each function $\phi_i$ is differentiable and $\grad \phi_i(u) = e_i$. 
So, we can apply ??? to obtain that
\begin{align*}
	\partial \norm{u}_1 &= \conv\Big( \bigcup_{i \in I(u)} \{ e_i \} \Big) \\
	&= \big\{ \sign(u) + f : f \in \R^d, \; \norm{f}_\infty \leq 1, \; f^\top u = 0 
	\big\},
\end{align*}
where we recall that $\sign$ is given by ???
For instance if $d = 4$ and $u = [17 \;\; -42 \;\; 0 \;\; 3]^\top$ then $\partial \norm{u}_1 = \{ 1 \} \times \{ -1 \} \times [-1, 1] \times \{ 1 \}$.
\todo{introduire ou pas ici les notations supp etc} ?

\section{Oracle inequalities for the Lasso} % (fold)

The material used in this Section is based on Bickel Ritov Tsybakov and Lunici Koltchinskii Tsyb... ~\sidecite{}.
We consider the Gaussian linear model
\begin{equation*}
	Y_i = f(X_i) + \eps_i
\end{equation*}
where $X_1, \ldots, X_n$ are deterministic and where $\eps_1, \ldots, \eps_n$ is iid and $\nor(0, \sigma^2)$.
We consider a finite dictionary $\cF = \{ f_1, \ldots, f_M \}$ and introduce
\begin{equation*}
	f_\theta(x) = \sum_{j=1}^M \theta_j f_j(x).
\end{equation*}
Note that this means that $f_\theta(X_i) = \theta^\top \bX_i$, where $\bX_i$ is the $i$-th row of the features matrix
\begin{equation*}
	\bX = \begin{bmatrix}
		f_1(X_1) & \cdots & f_M(X_1) \\
		\vdots & \ddots & \vdots \\ 
		f_1(X_n) & \cdots & f_M(X_n) \\
	\end{bmatrix}
\end{equation*}
Throughout the section, we will assume that the columns are standardize, namely that $\norm{\bX^j}_2 = 1$, or equivalently that $\norm{f_j}_n = 1$.\todo{notations}
Introduce the Lasso estimator
\begin{equation}
	\label{eq:lasso-def-oracle-sec}
	\begin{split}
	\wh \theta_n &= \argmin_{\theta \in \Theta} \Big\{ \frac 1n \sum_{i=1}^n (Y_i - f_\theta(X_i))^2 + \lambda \norm{\theta}_1 \Big\} \\
	&= \argmin_{\theta \in \Theta} \Big\{ 
	\frac 1n \norm{\by - \bX \theta}_2^2 + \lambda \norm{\theta}_1 
	\Big\},
	\end{split}
\end{equation}
for some convex set $\Theta \subset \R^M$, where 
\begin{equation*}
	\lambda = 2 \sigma \sqrt{\frac{2(x + \log M)}{n}}
\end{equation*}
with $x > 0$ to be explained later and with $\sigma > 0$ the standard deviation of the noise. \todo{notation pour $\bf$ et autres}
\begin{theorem}
	\label{thm:oracle-slow}
	If $\wh \theta_n$ is given by~\eqref{eq:lasso-def-oracle-sec}, we have that
	\begin{equation*}
		\norm{\bf_{\wh \theta_n} - \bf}_n^2 \leq \inf_{\theta \in \Theta} 
		\Big\{ \norm{\bf_{\theta} - \bf}_n^2  + 2 \lambda \norm{\theta}_1 \Big\}
	\end{equation*}
	with a probability larger than $1 - e^{-x}$.
\end{theorem}
This inequality is called a slow oracle inequality since it is of the order $1 / \sqrt{n}$.
The proof of Theorem~\ref{thm:oracle-slow} is given in Section~\ref{sec:lasso-proofs} below.
In order to obtain a faster rate, of order $1/n$, we need an extra assumption on the matrix of features $\bX$.
Let us introduce the $M \times M$ matrix 
\begin{equation*}
	\bG = \frac 1n \bX^\top \bX = 
	\Big[ \frac 1n \inr{\bf_j, \bf_{j'}}_n \Big]_{1 \leq j, j' \leq M}.
\end{equation*}
Note that if $M > n$, then we have that
\begin{equation*}
	\min_{t \in \R^M} \frac{t^\top \bG t}{\norm{t}_2} 
	= \min_{t \in \R^M} \frac{\norm{\bX t}_2}{\sqrt n \norm{t}_2} = 0
\end{equation*}
since $\bX : \R^M \rightarrow \R^n$ and $\ker(\bX) \neq \{ 0 \}$ hence its smallest eigenvalue is zero.

The assumption we ask going to require asks that the smallest \emph{restrained} eigenvalue to \emph{sparse} vectors is positive.
For $\theta \in \R^M$ and $c_0 > 0$, let us introduce the cone
\begin{equation*}
	C_{\theta, c_0} = \{ t \in \R^M : \norm{t_{J(\theta)^\complement}}_1 \leq c_0 \norm{t_{J(\theta)}}_1 \}
\end{equation*}
where $J(\theta) = \{ j \in \{1, \ldots, M\} : \theta_j \neq 0 \}$ and $t_J$ stands for the vector with coordinates $(t_J)_j = t_j$ if $j \in J$ and $(t_J)_j = 0$ for $j \notin J$ and $J^\complement = \{ 1, \ldots, M \} - J$.
If $t \in C_{\theta, c_0}$, then both the vectors $t$ and $\theta$ almost share the same support, since the coefficients of $t_{J(\theta)}$ dominate those of $t_{J(\theta)^\complement}$.
Then, we can introduce 
\begin{equation*}
	\mu_{c_0}(\theta) = \inf \Big\{ \mu > 0 : 
	\norm{t_{J(\theta)}}_2 \leq \frac{\mu}{\sqrt n} \norm{\bX t}_2 \Big\}.
\end{equation*}
Note that the function $c_0 \mapsto \mu_{c_0}(\theta)$ is decreasing.
If $c_0 = \infty$, then $C_{\theta, c_0} = \R^M$, while if $c_0 = 0$ then
$C_{\theta, c_0} = \{ t \in \R^M : J(t) = J(\theta) \}$ and in this case
\begin{equation*}
	\mu_{c_0}(\theta) = \frac{1}{\lambda_{\min}(\bG_{J(\theta) \times J(\theta)})}
\end{equation*}
the inverse of the smallest eigenvalue of the submatrix $(\bG)_{J \times J}$ with $J = J(\theta)$ corresponding to the subset of rows and columns with index in $J$.
\begin{theorem}
	\label{thm:oracle-fast}
	If $\wh \theta_n$ is given by~\eqref{eq:lasso-def-oracle-sec}, we have that
	\begin{equation*}
		\norm{\bf_{\wh \theta_n} - \bf}_n^2 \leq \inf_{\theta \in \Theta} 
		\bigg\{ \norm{\bf_{\theta} - \bf}_n^2  + c \mu_3(\theta)^2 \sigma^2 \frac{x + \log M}{n} \norm{\theta}_0 \bigg\}
	\end{equation*}
	with a probability larger than $1 - e^{-x}$, where $c = ?$, where $\mu_3(\theta)$ is given by ??? and $\norm{\theta}_0$ is the sparsity of $\theta$ given by ??
\end{theorem}
The proof of Theorem~\ref{thm:oracle-fast} is given in Section~\ref{sec:lasso-proofs} below.
It is a remarkable theorem, since it show that $\wh \theta_n$, which is a convex problem, is almost as good as the best \emph{sparse} representation of $f$ using the dictionary $\cF$.
Indeed, the rate obtained herein is of order $(\log M) \norm{\theta}_0 / n$, namely the ambient dimension $M$ appears only through $\log M$, while $\norm{\theta}_0$ corresponds to the "useful" dimension given by the number of elements of $\cF$ useful to be included in the linear combination $f_\theta$.

What is remarkable is that Lasso achieved, according to Theorem~\ref{thm:oracle-fast}, a balance between an \emph{approximation} or \emph{estimation} term $\norm{\bf_{\theta} - \bf}_n^2$ and a \emph{complexity} terms which involves only the sparsity of $\theta$.

\begin{definition}[Restricted eigenvalues]
	We say that $\bX$ satisfies the $\text{RE}(s, c_0)$ assumption for some $c_0 > 0$ and some $s \in \{ 1, \ldots, M \}$ whenever
	\begin{equation*}
		\kappa(s, c_0) = 
		\min_{\substack{J \subset \{ 1, \ldots, M \} \\ 
			  |J| \leq s}}
			\;
		\min_{\substack{t \neq 0 \\ 
		\norm{t_{J^\complement}}_1 \leq c_0 \norm{t_J}_1}}
		\frac{\norm{\bX t}_2}{\sqrt n \norm{t_J}_2} > 0.
	\end{equation*}
\end{definition}
Note that we have
\begin{equation*}
	\kappa(s, c_0) = \inf_{\substack{t \in \R^M \setminus \{ 0 \}\\ 
	\norm{t}_0 \leq s}}
	\; \frac{1}{\mu_{c_0}(t)}.
\end{equation*}
Note that whenever $\bX$ satisfies $\text{RE}(s, 1)$, then any sub-matrix formed by 
any subset of $2 s$ columns of $\bX$ has full rank.%
\sidenote{Suppose by contradiction that there is $t \in \R^M$ such that $\norm{t}_0 = 2s$ and $\bX t = 0$. Then, we can choose disjoint set $J_0, J_1 \subset \{ 1, \ldots, M\}$ such that $J(t) = J_0 \cup J_1$ with $|J_0| = s$ and $|J_1| = s$ and such that $\norm{t_{J_1}}_1 \leq \norm{t_{J_0}}_1$. 
But obviously $\norm{t_{J_1}}_1 = \norm{t_{J_0^\complement}}_1$ so $\norm{t_{J_0^\complement}}_1 \leq \norm{t_{J_0}}_1$, which contradicts the $\text{RE}(s, 1)$ assumption.}

An immediate corollary of Theorem~\ref{thm:oracle-fast} is the following oracle inequality, which holds under the $\text{RE}(s, 3)$ assumption:
\begin{equation*}
	\norm{\bf_{\wh \theta_n} - \bf}_n^2 \leq 
	\inf_{\substack{\theta \in \Theta \\ \norm{\theta}_0 \leq s}}
	\bigg\{ \norm{\bf_{\theta} - \bf}_n^2  + \frac{c \sigma^2}{\kappa(s, 3)^2} 
	\; s \; \frac{x + \log M}{n} \bigg\}.
\end{equation*}
In this inequality, the convergence rate is of order $(s \log M) / n$, where $s$ is the sparsity of the best $\theta$.
Several remarks are necessary at this point.
\begin{itemize}
	\item The rate of convergence depends on the ambient dimension $M$ only through $\log M$ and depends linearly on the sparsity $s$ of $\theta \in \R^M$. This is a remarkable property called \emph{dimension reduction} or \emph{adaptation to the sparsity} of the Lasso estimator.
	\item This is not the optimal rate, the minimax optimal rate among $s$-sparse vector being $s \log(M / s) / n$, see ???
	\item There several improvements of these oracle inequalities in literature: beyond Gaussian noise, using the integrated estimator error $\int_{\R^M} (f_{\wh \theta_n}(x) - f(x))^2 P_X(dx)$ instead of the empirical one used here, and we can remove the dependency of $\lambda$ on the confidence level $x > 0$, see ??
	\item From Theorem~\ref{thm:oracle-fast} we can deduce bounds on the estimator error $\norm{\wh \theta_n - \theta^\star}_p$ of the true parameter $\theta^\star$ and the we can give guarantees on the \emph{signed support recovery} of the parameter, through controls on the probability 
	$\P[ \sign(\wh \theta_n) = \sign(\theta^\star)]$, see~???
\end{itemize}

\section{Proofs} % (fold)
\label{sec:lasso-proofs}

Let us recall the following set of notations: $\norm{x}_p$ stands for the $\ell_p$ norm of a vector $x$ and
\begin{equation*}
	\by =
	\begin{bmatrix}
		Y_1 \\
		\vdots \\
		Y_n
	\end{bmatrix},
	\quad
	\bf = 
	\begin{bmatrix}
		f(X_1) \\
		\vdots \\
		f(X_n)
	\end{bmatrix},
	\quad
	\bf_\theta =	
	\begin{bmatrix}
		f_\theta(X_1) \\
		\vdots \\
		f_\theta(X_n)
	\end{bmatrix},
	\quad
	\beps = 
	\begin{bmatrix}
		\eps_1 \\
		\vdots \\
		\eps_n
	\end{bmatrix}.
\end{equation*}
Also, we will write \todo{empirical norm notation} for all vectors
\todo{introduire le produit scalaire empirique}

\subsection{Proof of Theorem~\ref{thm:oracle-slow}} % (fold)
\label{sub:proof_of_theorem_thm:oracle-slow}

Let us start with the noise. It is fairly easy, since
\begin{equation*}
	\frac 1n \sum_{i=1}^n \eps_i f_j(X_i) \sim 
	\nor\Big( 0, \sigma^2 \frac{\norm{\bf_j}_n^2}{n} \Big) 
	= \nor\Big( 0, \frac{\sigma^2}{n} \Big).
\end{equation*}
So, recalling that $\P[|Z| \geq z] \leq 2e^{-z^2 / 2}$ whenever $Z \sim \nor(0, 1)$ for any $z > 0$%
\sidenote{proof}
we obtain
\begin{equation*}
	\P \bigg[ \Big| \frac 1n \sum_{i=1}^n \eps_i f_j(X_i) \Big| \geq \sigma \sqrt{\frac{2 x}{n}} \bigg] \leq 2 e^{-x}
\end{equation*}
and using an union bound, we obtain that the event
\begin{equation*}
	A = \bigcap_{j=1}^M \bigg\{ \Big| \frac 1n \sum_{i=1}^n \eps_i f_j(X_i) \Big| \geq \sigma \sqrt{\frac{2 (x + \log M)}{n}} \bigg\}
\end{equation*}
satisfies $\P[A] \geq 1 - 2e^{-x}$.
The definition of $\wh \theta_n$ entails that
\begin{equation}
	\label{eq:proof-oracle-slow-step1}
	\norm{\by - \bf_{\wh \theta_n}}_n^2 + \lambda \norm{\wh \theta_n}_1 
	\leq \norm{\by - \bf_\theta}_n^2 + \lambda \norm{\theta}_1
\end{equation}
for any $\theta \in \Theta$ and an easy computation gives
\begin{equation}
	\label{eq:proof-oracle-slow-step2}
	\begin{split}
	&\norm{\by - \bf_{\wh \theta_n}}_n^2 - \norm{\by - \bf_\theta}_n^2 \\
	&= \norm{\bf_{\wh \theta_n}}_n^2 + \norm{\bf}_n^2 + 2 \inr{\by, \bf_{\theta} - \bf_{\wh \theta_n}}_n \\
	&= \norm{\bf_{\wh \theta_n}}_n^2 + \norm{\bf}_n^2 + 2 \inr{\bf, \bf_{\theta} - \bf_{\wh \theta_n}}_n + 2 \inr{\beps, \bf_{\theta} - \bf_{\wh \theta_n}}_n \\
	&= \norm{\bf_{\wh \theta_n} - \bf}_n^2 - \norm{\bf_\theta - \bf}_n^2 + 
	2 \inr{\beps, \bf_{\theta} - \bf_{\wh \theta_n}}_n.
	\end{split}
\end{equation}
But on the event $A$, we have that
\begin{equation}
	\label{eq:proof-oracle-slow-step3}
	\begin{split}
	| 2 \inr{\beps, \bf_{\theta} - \bf_{\wh \theta_n}}_n | 
	&= \Big| \frac 2n \sum_{j=1}^M ((\wh \theta_n)_j - \theta_j ) 
	\sum_{i=1}^n \eps_i f_j(X_i) \Big| \\
	&\leq \sum_{j=1}^M | (\wh \theta_n)_j - \theta_j | 2 \sigma \sqrt{\frac{2 (x + \log M)}{n}} \\
	&= \lambda \norm{\wh \theta_n - \theta}_1,	
	\end{split}
\end{equation}
so that, combining Inequalities~\eqref{eq:proof-oracle-slow-step1},~\eqref{eq:proof-oracle-slow-step2} and~\eqref{eq:proof-oracle-slow-step3}, we end up with
\begin{align*}
	\norm{\bf_{\wh \theta_n} - \bf}_n^2 &\leq \norm{\bf_\theta - \bf}_n^2 
	+ \lambda \norm{\wh \theta_n - \theta}_1 
	+ \lambda \norm{\theta}_1 
	- \lambda \norm{\wh \theta_n}_1 \\
	&\leq \norm{\bf_\theta - \bf}_n^2 + 2 \lambda \norm{\theta}_1,
\end{align*}
which concludes the proof of Theorem~\ref{thm:oracle-slow}. $\hfill \square$


\subsection{Proof of Theorem~\ref{thm:oracle-fast}} % (fold)
\label{sub:proof_of_theorem_thm:oracle-fast}

\todo{type the proof of the Theorem}

% subsection proof_of_theorem_thm:oracle-fast (end)


% subsection proof_of_theorem_thm:oracle-slow (end)

% section proofs (end)
% section oracle_inequalities_for_the_lasso (end)

% section some_tools_from_convex_optimisation (end)


% paragraph paragraph_name (end)

% \input{chap07_mle_exponential_models}

% section exponential_models (end)

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

% The bibliography needs to be compiled with biber using your LaTeX editor, or on the command line with 'biber main' from the template directory

% \defbibnote{bibnote}{Here are the references in citation order.\par\bigskip} % Prepend this text to the bibliography
\printbibliography[heading=bibintoc, title=Bibliography] % Add the bibliography heading to the ToC, set the title of the bibliography and output the bibliography note

%----------------------------------------------------------------------------------------
%	NOMENCLATURE
%----------------------------------------------------------------------------------------

% The nomenclature needs to be compiled on the command line with 'makeindex main.nlo -s nomencl.ist -o main.nls' from the template directory

% \nomenclature{$c$}{Speed of light in a vacuum inertial frame}
% \nomenclature{$h$}{Planck constant}

% \renewcommand{\nomname}{Notation} % Rename the default 'Nomenclature'
% \renewcommand{\nompreamble}{The next list describes several symbols that will be later used within the body of the document.} % Prepend this text to the nomenclature

% \printnomenclature % Output the nomenclature

%----------------------------------------------------------------------------------------
%	GREEK ALPHABET
% 	Originally from https://gitlab.com/jim.hefferon/linear-algebra
%----------------------------------------------------------------------------------------

% \vspace{1cm}

% {\usekomafont{chapter}Greek Letters with Pronounciation} \\[2ex]
% \begin{center}
% 	\newcommand{\pronounced}[1]{\hspace*{.2em}\small\textit{#1}}
% 	\begin{tabular}{l l @{\hspace*{3em}} l l}
% 		\toprule
% 		Character & Name & Character & Name \\ 
% 		\midrule
% 		$\alpha$ & alpha \pronounced{AL-fuh} & $\nu$ & nu \pronounced{NEW} \\
% 		$\beta$ & beta \pronounced{BAY-tuh} & $\xi$, $\Xi$ & xi \pronounced{KSIGH} \\ 
% 		$\gamma$, $\Gamma$ & gamma \pronounced{GAM-muh} & o & omicron \pronounced{OM-uh-CRON} \\
% 		$\delta$, $\Delta$ & delta \pronounced{DEL-tuh} & $\pi$, $\Pi$ & pi \pronounced{PIE} \\
% 		$\epsilon$ & epsilon \pronounced{EP-suh-lon} & $\rho$ & rho \pronounced{ROW} \\
% 		$\zeta$ & zeta \pronounced{ZAY-tuh} & $\sigma$, $\Sigma$ & sigma \pronounced{SIG-muh} \\
% 		$\eta$ & eta \pronounced{AY-tuh} & $\tau$ & tau \pronounced{TOW (as in cow)} \\
% 		$\theta$, $\Theta$ & theta \pronounced{THAY-tuh} & $\upsilon$, $\Upsilon$ & upsilon \pronounced{OOP-suh-LON} \\
% 		$\iota$ & iota \pronounced{eye-OH-tuh} & $\phi$, $\Phi$ & phi \pronounced{FEE, or FI (as in hi)} \\
% 		$\kappa$ & kappa \pronounced{KAP-uh} & $\chi$ & chi \pronounced{KI (as in hi)} \\
% 		$\lambda$, $\Lambda$ & lambda \pronounced{LAM-duh} & $\psi$, $\Psi$ & psi \pronounced{SIGH, or PSIGH} \\
% 		$\mu$ & mu \pronounced{MEW} & $\omega$, $\Omega$ & omega \pronounced{oh-MAY-guh} \\
% 		\bottomrule
% 	\end{tabular} \\[1.5ex]
% 	Capitals shown are the ones that differ from Roman capitals.
% \end{center}

%----------------------------------------------------------------------------------------
%	GLOSSARY
%----------------------------------------------------------------------------------------

% The glossary needs to be compiled on the command line with 'makeglossaries main' from the template directory

% \newglossaryentry{computer}{
% 	name=computer,
% 	description={is a programmable machine that receives input, stores and manipulates data, and provides output in a useful format}
% }

% Glossary entries (used in text with e.g. \acrfull{fpsLabel} or \acrshort{fpsLabel})
% \newacronym[longplural={Frames per Second}]{fpsLabel}{FPS}{Frame per Second}
% \newacronym[longplural={Tables of Contents}]{tocLabel}{TOC}{Table of Contents}

% \setglossarystyle{listgroup} % Set the style of the glossary (see https://en.wikibooks.org/wiki/LaTeX/Glossary for a reference)
% \printglossary[title=Special Terms, toctitle=List of Terms] % Output the glossary, 'title' is the chapter heading for the glossary, toctitle is the table of contents heading

%----------------------------------------------------------------------------------------
%	INDEX
%----------------------------------------------------------------------------------------

% The index needs to be compiled on the command line with 'makeindex main' from the template directory

% \printindex % Output the index

%----------------------------------------------------------------------------------------
%	BACK COVER
%----------------------------------------------------------------------------------------

% If you have a PDF/image file that you want to use as a back cover, uncomment the following lines

%\clearpage


\end{document}


