%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% kaobook
% LaTeX Template
% Version 1.2 (4/1/2020)
%
% This template originates from:
% https://www.LaTeXTemplates.com
%
% For the latest template development version and to make contributions:
% https://github.com/fmarotta/kaobook
%
% Authors:
% Federico Marotta (federicomarotta@mail.com)
% Based on the doctoral thesis of Ken Arroyo Ohori (https://3d.bk.tudelft.nl/ken/en)
% and on the Tufte-LaTeX class.
% Modified for LaTeX Templates by Vel (vel@latextemplates.com)
%
% License:
% CC0 1.0 Universal (see included MANIFEST.md file)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
	fontsize=11pt, % Base font size
	twoside=false, % Use different layouts for even and odd pages (in particular, if twoside=true, the margin column will be always on the outside)
	%open=any, % If twoside=true, uncomment this to force new chapters to start on any page, not only on right (odd) pages
	%chapterprefix=true, % Uncomment to use the word "Chapter" before chapter numbers everywhere they appear
	%chapterentrydots=true, % Uncomment to output dots from the chapter name to the page number in the table of contents
	numbers=noenddot, % Comment to output dots after chapter numbers; the most common values for this option are: enddot, noenddot and auto (see the KOMAScript documentation for an in-depth explanation)
	%draft=true, % If uncommented, rulers will be added in the header and footer
	%overfullrule=true, % If uncommented, overly long lines will be marked by a black box; useful for correcting spacing problems
]{kaobook}

% Set the language
\usepackage[english]{babel} % Load characters and hyphenation
\usepackage[english=british]{csquotes} % English quotes

% Load packages for testing
\usepackage{blindtext}
%\usepackage{showframe} % Uncomment to show boxes around the text area, margin, header and footer
%\usepackage{showlabels} % Uncomment to output the content of \label commands to the document where they are used

% Load the bibliography package
\usepackage{styles/kaobiblio}
\addbibresource{main.bib} % Bibliography file

% Load mathematical packages for theorems and related environments. NOTE: choose only one between 'mdftheorems' and 'plaintheorems'.
\usepackage{styles/mdftheorems}
%\usepackage{styles/plaintheorems}


\graphicspath{{images/}} % Paths in which to look for images

% \RequirePackage{times}

\DeclareMathOperator{\cA}{\mathcal A}
\DeclareMathOperator{\cB}{\mathcal B}
\DeclareMathOperator{\cC}{\mathcal C}
\DeclareMathOperator{\cD}{\mathcal D}
\DeclareMathOperator{\cE}{\mathcal E}
\DeclareMathOperator{\cF}{\mathcal F}
\DeclareMathOperator{\cG}{\mathcal G}
\DeclareMathOperator{\cM}{\mathcal M}
\DeclareMathOperator{\cN}{\mathcal N}
\DeclareMathOperator{\cP}{\mathcal P}

\DeclareMathOperator{\bA}{\boldsymbol A}
\DeclareMathOperator{\bB}{\boldsymbol B}
\DeclareMathOperator{\bI}{\boldsymbol I}

\DeclareMathOperator{\bSigma}{\boldsymbol \Sigma}


\DeclareMathOperator{\nor}{Normal}
\DeclareMathOperator{\ber}{Bernoulli}
\DeclareMathOperator{\bin}{Binomial}
\DeclareMathOperator{\mul}{Multinomial}
\DeclareMathOperator{\expo}{Exponential}
\DeclareMathOperator{\uni}{Uniform}

\DeclareMathOperator{\sigmoid}{sigmoid}

\DeclareMathOperator{\leb}{Lebesgue}

\newcommand{\eps}{\varepsilon}

\renewcommand{\P}{\mathbb P}
\newcommand{\E}{\mathbb E}
\newcommand{\R}{\mathbb R}
\newcommand{\var}{\mathbb V}

\newcommand{\wh}{\widehat}

\newcommand{\ind}[1]{\mathbf 1_{#1}}
\newcommand{\grad}{\nabla}


\newcommand{\mgeq}{\succcurlyeq}
\newcommand{\mleq}{\preccurlyeq}
\newcommand{\goes}{\rightarrow}
\newcommand{\go}{\rightarrow}

\newcommand{\norm}[1]{\|#1\|}

\newcommand{\gopro}{\overset{\P}{\rightarrow}}
\newcommand{\goas}{\overset{\text{as\ }}{\rightarrow}}
\newcommand{\goqr}{\overset{\text{$L^2$\ }}{\rightarrow}}
\newcommand{\gosto}{\leadsto}

\newcommand{\lest}{\underset{\text{st}}{\leq}}
\newcommand{\gest}{\underset{\text{st}}{\qeq}}



% \makeindex[columns=3, title=Alphabetical Index, intoc] % Make LaTeX produce the files required to compile the index

% \makeglossaries % Make LaTeX produce the files required to compile the glossary

% \makenomenclature % Make LaTeX produce the files required to compile the nomenclature

% Reset sidenote counter at chapters
%\counterwithin*{sidenote}{chapter}

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	BOOK INFORMATION
%----------------------------------------------------------------------------------------

\titlehead{Some stuff about statistics}
\subject{Lecture notes for the ENS course of Statistics}

\title[Some stuff about Statistics]{Some stuff about Statistics}
% \subtitle{Customise this page according to your needs}

\author[St\'ephane Ga\"iffas"]{St\'ephane Ga\"iffas\thanks{}}

\date{\today}

\publishers{}

%----------------------------------------------------------------------------------------

\frontmatter % Denotes the start of the pre-document content, uses roman numerals

%----------------------------------------------------------------------------------------
%	OPENING PAGE
%----------------------------------------------------------------------------------------

%\makeatletter
%\extratitle{
%	% In the title page, the title is vspaced by 9.5\baselineskip
%	\vspace*{9\baselineskip}
%	\vspace*{\parskip}
%	\begin{center}
%		% In the title page, \huge is set after the komafont for title
%		\usekomafont{title}\huge\@title
%	\end{center}
%}
%\makeatother

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

% \makeatletter
% \uppertitleback{\@titlehead} % Header

% \lowertitleback{
% 	\textbf{Disclaimer}\\
% 	You can edit this page to suit your needs. For instance, here we have a no copyright statement, a colophon and some other information. This page is based on the corresponding page of Ken Arroyo Ohori's thesis, with minimal changes.
	
% 	\medskip
	
% 	\textbf{No copyright}\\
% 	\cczero\ This book is released into the public domain using the CC0 code. To the extent possible under law, I waive all copyright and related or neighbouring rights to this work.
	
% 	To view a copy of the CC0 code, visit: \\\url{http://creativecommons.org/publicdomain/zero/1.0/}
	
% 	\medskip
	
% 	\textbf{Colophon} \\
% 	This document was typeset with the help of \href{https://sourceforge.net/projects/koma-script/}{\KOMAScript} and \href{https://www.latex-project.org/}{\LaTeX} using the \href{https://github.com/fmarotta/kaobook/}{kaobook} class.
	
% 	The source code of this book is available at:\\\url{https://github.com/fmarotta/kaobook}
	
% 	(You are welcome to contribute!)
	
% 	\medskip
	
% 	\textbf{Publisher} \\
% 	First printed in May 2019 by \@publishers
% }
% \makeatother

%----------------------------------------------------------------------------------------
%	DEDICATION
%----------------------------------------------------------------------------------------

% \dedication{
% 	The harmony of the world is made manifest in Form and Number, and the heart and soul and all the poetry of Natural Philosophy are embodied in the concept of mathematical beauty.\\
% 	\flushright -- D'Arcy Wentworth Thompson
% }

%----------------------------------------------------------------------------------------
%	OUTPUT TITLE PAGE AND PREVIOUS
%----------------------------------------------------------------------------------------

% Note that \maketitle outputs the pages before here

% If twoside=false, \uppertitleback and \lowertitleback are not printed
% To overcome this issue, we set twoside=semi just before printing the title pages, and set it back to false just after the title pages
\KOMAoptions{twoside=semi}
\maketitle
\KOMAoptions{twoside=false}

%----------------------------------------------------------------------------------------
%	PREFACE
%----------------------------------------------------------------------------------------

% \chapter*{Preface}
% \addcontentsline{toc}{chapter}{Preface} % Add the preface to the table of contents as a chapter

% I am of the opinion that every \LaTeX\xspace geek, at least once during 
% his life, feels the need to create his or her own class: this is what 
% happened to me and here is the result, which, however, should be seen as 
% a work still in progress. Actually, this class is not completely 
% original, but it is a blend of all the best ideas that I have found in a 
% number of guides, tutorials, blogs and tex.stackexchange.com posts. In 
% particular, the main ideas come from two sources:

% \begin{itemize}
% 	\item \href{https://3d.bk.tudelft.nl/ken/en/}{Ken Arroyo Ohori}'s 
% 	\href{https://3d.bk.tudelft.nl/ken/en/nl/ken/en/2016/04/17/a-1.5-column-layout-in-latex.html}{Doctoral 
% 	Thesis}, which served, with the author's permission, as a backbone 
% 	for the implementation of this class;
% 	\item The 
% 		\href{https://github.com/Tufte-LaTeX/tufte-latex}{Tufte-Latex 
% 			Class}, which was a model for the style.
% \end{itemize}

% The first chapter of this book is introductive and covers the most 
% essential features of the class. Next, there is a bunch of chapters 
% devoted to all the commands and environments that you may use in writing 
% a book; in particular, it will be explained how to add notes, figures 
% and tables, and references. The second part deals with the page layout 
% and design, as well as additional features like coloured boxes and 
% theorem environments.

% I started writing this class as an experiment, and as such it should be 
% regarded. Since it has always been indended for my personal use, it may 
% not be perfect but I find it quite satisfactory for the use I want to 
% make of it. I share this work in the hope that someone might find here 
% the inspiration for writing his or her own class.

% \begin{flushright}
% 	\textit{Federico Marotta}
% \end{flushright}


%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LIST OF FIGURES/TABLES
%----------------------------------------------------------------------------------------

% \begingroup % Local scope for the following commands

% % Define the style for the TOC, LOF, and LOT
% %\setstretch{1} % Uncomment to modify line spacing in the ToC
% %\hypersetup{linkcolor=blue} % Uncomment to set the colour of links in the ToC
% \setlength{\textheight}{23cm} % Manually adjust the height of the ToC pages

% % Turn on compatibility mode for the etoc package
% \etocstandarddisplaystyle % "toc display" as if etoc was not loaded
% \etocstandardlines % toc lines as if etoc was not loaded

% \tableofcontents % Output the table of contents

% \listoffigures % Output the list of figures

% % Comment both of the following lines to have the LOF and the LOT on different pages
% \let\cleardoublepage\bigskip
% \let\clearpage\bigskip

% \listoftables % Output the list of tables

% \endgroup

%----------------------------------------------------------------------------------------
%	MAIN BODY
%----------------------------------------------------------------------------------------

\mainmatter % Denotes the start of the main document content, resets page numbering and uses arabic numbers
\setchapterstyle{kao} % Choose the default chapter heading style


\input{chap01_introduction}

\input{chap02_statistical_models}



\setchapterpreamble[u]{\margintoc}
\chapter{Statistical inference}
\label{chap:statistical_inference}

In this Chapter, we will consider all along the simple Bernoulli model, where we have iid samples $X_1, \ldots, X_n$ distributed as $\ber(\theta)$ with $\theta \in (0, 1)$.
Let us start with the first \emph{inference} problem: the \emph{estimation} problem.

\section{Estimation} % (fold)
\label{sec:estimation}

We want to \emph{infer} $\theta$, or \emph{estimate} it by finding a statistic which is a measurable function of $(X_1, \ldots, X_n)$%
\sidenote{Once again, since we are doing statistics, the only thing we are allowed to use is the data.}%
or a measurable function of $S_n = \sum_{i=1}^n X_i$ thereof, since $S_n$ is sufficient, see Section~\ref{sec:statistics}.
We will denote such a statistic as
\begin{equation*}
 	\wh \theta_n = \wh \theta(X_1, \ldots, X_n).
\end{equation*}
This function \emph{does not depend} on $\theta$, but its distribution does.
Ideally, we want $\wh \theta_n$ to be ``close'' to $\theta$, since we want a good estimator, so that the first thing we need to do is to quantify ``closeness''.
For instance, we could want $|\wh \theta_n - \theta|$ to be close to $0$ with a large probability, since we do not forget that $\wh \theta_n$ is a random variable, as a function of the data $(X_1, \ldots, X_n)$.
The most natural distance is arguably the Euclidean one, in this context the $L^2$ distance, 
which leads to the \emph{quadratic risk}.%
\sidenote{Although the quadratic risk corresponds to a \emph{squared} $L^2$ norm.}
\begin{definition}[Quadratic risk]
	\label{def:quadratic_risk}
	Consider a statistical model with data $X$ and set of parameters $\Theta \subset \R$ and an estimator $\wh \theta(X)$. 
	The quadratic risk of $\wh \theta$ is given by
	\begin{equation*}
		R(\wh \theta, \theta) = \E_\theta[ (\wh \theta - \theta)^2 ] = \int_E (\wh \theta(x) - \theta)^2 P_\theta(dx).
	\end{equation*}
	We consider the quadratic risk as a function $\Theta \goes \R^+$ of the parameter given by $\theta \mapsto R(\wh \theta, \theta)$.
\end{definition}
At this point, it's useful to recall some classical inequalities on the queues of random variables.
The Markov inequality tells us that if $Y$ is a real random variable such that $\E |Y|^p < +\infty$ for some $p > 0$ then
\begin{equation*}
	\P(|Y| > t) \leq \frac{\E |Y|^p}{t^p}
\end{equation*}
for any $t > 0$.
This tells us that the more $Y$ has moments%
\sidenote{We say that $Y$ as moments up to order $p$ if $\E |Y|^p < +\infty$. 
Note that this entails $\E |Y|^q < +\infty$ for any $q < p$ since $\E |Y|^p = \E (|Y|^q)^{p/q} \geq (\E |Y|^q)^{p/q}$ using Jensen's inequality.}%
the more the queue of $Y$ is tight (it goes faster to $0$ with $t \goes +\infty$).
Markov's inequality with $p=2$ entails 
\begin{equation}
	\label{eq:l2_entrails_proba}
	\P(|\wh \theta - \theta| > t) \leq \frac{R(\wh \theta, \theta)}{t^2}
\end{equation}
which tells us that whenever the quadratic risk is small, then $\wh \theta$ is close to $\theta$ with a large probability.

Whenever $R(\wh \theta_n, \theta) \rightarrow 0$ with $n \rightarrow +\infty$, we will write $\wh \theta_n \goqr \theta$, which stands for convergence in $L^2$ norm, which entails, because of Inequality~\eqref{eq:l2_entrails_proba}, that $\wh \theta_n \gopro \theta$, which stands for convergence in probability.\sidenote{More precisely, in $\P_\theta$-probability, namely $\P_\theta[|\wh \theta_n - \theta| > \eps] \rightarrow 0$ as $n \rightarrow +\infty$ for any $\eps > 0$, but we will write $\wh \theta_n \gopro \theta$ in order to keep the notations as simple as possible.}%
\begin{definition}
	\label{def:consistent}
	We say that $\wh \theta_n$ is \emph{consistent} whenever $\P_\theta[|\wh \theta_n - \theta| > \eps] \rightarrow 0$ as $n \rightarrow +\infty$ for any $\eps > 0$ and any $\theta \in \Theta$.
	We say that it is strongly consistent whenever $\P_\theta[\wh \theta_n \rightarrow \theta] = 1$ for any $\theta \in \Theta$.
\end{definition}
In Definitions~\ref{def:quadratic_risk} and~\ref{def:consistent} above, if $\Theta \subset \R^d$, it suffices to replace $|\cdot|$ by the Euclidean norm $\norm{\cdot}_2$.%
\sidenote{We can use any norm on $\R^d$ in Definition~\ref{def:consistent}, and let us recall that $\norm{x}_2 = \sqrt{x^\top x} = (\sum_{j=1}^d x_j^2)^{1/2}$.}%

\paragraph{Bias variance decomposition.} % (fold)

The \emph{bias-variance decomposition} is the following decomposition of the quadratic risk between two terms: a bias term denoted $b(\wh \theta_n, \theta)$ (squared in the formula) and a variance term:
\begin{equation}
	\label{eq:bias-variance-decomposition}
	\begin{split}
	R(\wh \theta_n, \theta) &= \E_\theta[(\wh \theta_n - \theta)^2] = (\E_\theta \wh \theta_n - \theta)^2 + \var_\theta(\wh \theta_n) \\
	&= b(\wh \theta_n, \theta)^2 + \var_\theta(\wh \theta_n).		
	\end{split}
\end{equation}
When $b(\wh \theta_n, \theta) = 0$ for all $\theta \in \Theta$ we say that the estimator $\wh \theta_n$ is \emph{unbiased}.
This means that this estimator will not tend to over or under-estimate $\theta$. 

\paragraph{Back to Bernoulli.} % (fold)

% paragraph back_to_bernoulli (end)Back to Bernoulli
Going back to the $\ber(\theta)$ model, we consider the estimator $\wh \theta_n = S / n = \sum_{i=1}^n X_i / n$.
We already know many things about this estimator:
\begin{enumerate}
	\item We have $\E_\theta [\wh \theta_n] = \theta$ which means that $\wh \theta_n$ is unbiased;
	\item The bias-variance decomposition gives
	\begin{equation}
		\label{eq:bernoulli-quadratic-risk}
	 	R(\wh \theta_n, \theta) = \var(\wh \theta_n) = \frac{\theta (1 - \theta)}{n} \leq 
	 	\frac{1}{4 n} \rightarrow 0
	 \end{equation}
	 which means that $\wh \theta_n \goqr \theta$ and which entails that $\wh \theta_n$ it is consistent;
	\item The law of large number tells us that $\wh \theta_n \goas \theta$, hence $\wh \theta_n$ is strongly consistent;
	\item The central limit theorem tells us that
	\begin{equation}
	\label{eq:tcl-bernoulli}
	\sqrt n (\wh \theta_n - \theta) \leadsto \nor(0, \theta(1 - \theta)).
	\end{equation}
\end{enumerate}
The points 2--4 from above are all different ways of saying that when $n$ is large enough, then $\wh \theta_n$ is close to $\theta$.
In practice, an estimator leads to a value: for the Bernoulli model with $n=100$ and $42$ ones you end up with a single estimated value $0.42$.
But what if we want to include uncertainty in this estimation ?
Namely how confident are we about this $0.42$ value ?
Moreover, what do we mean by ``when $n$ large enough'', can we quantify this better ?
All these questions can be answered by considering another inference problem: confidence intervals.

\section{Confidence intervals} % (fold)
\label{sec:confidence_intervals}

Here, we don't only want to build an estimator $\wh \theta_n$ but also to quantify the uncertainty of this estimation.
Combining Inequalities~\eqref{eq:l2_entrails_proba} and~\eqref{eq:bernoulli-quadratic-risk} leads to
\begin{equation*}
	\P_\theta[ |\wh \theta_n - \theta| > t] \leq \frac{1}{4 n t^2}
\end{equation*}
so that for $\alpha \in (0, 1)$ and the choice $t_\alpha = 1 / (2 \sqrt{n \alpha})$ we have 
\begin{equation*}
	\P_\theta \{ \theta \in [ \wh \theta^L, \wh \theta^R ] \} \geq 1 - \alpha,
\end{equation*}
where
\begin{equation*}
	\wh \theta^L =  \wh \theta_n - \frac{1}{2 \sqrt{n \alpha}} \quad \text{ and } \quad \wh \theta^R =  \wh \theta_n + \frac{1}{2 \sqrt{n \alpha}}.
\end{equation*}
Therefore, if we choose $\alpha = 0.05 = 5\%$, we know that $\theta \in [\wh \theta^L, \wh \theta^R]$ with a probability larger than $95\%$.
We say in this case that the interval $[\wh \theta^L, \wh \theta^R]$ is a \emph{confidence interval} with coverage $95\%$.%
\sidenote{If we toss the coin $1000$ times and get $420$ heads, the realization of this confidence interval at $95\%$ is $[0.35, 0.49]$.}

Note that if $\alpha = 0$ then we have no other choice than using the whole $\R$ as a confidence interval: $\alpha$ allows to give some slack, so that we can build a non-absurdly  large confidence interval. 
We typically have that $|\wh \theta^R - \wh \theta^L|$ increases as $\alpha$ decreases, since a smaller $\alpha$ means more confidence, hence a larger interval.
On the contrary, $|\wh \theta^R - \wh \theta^L|$ should decrease with the sample size $n$.
\begin{definition}[Confidence interval]
	Consider a statistical model with data $X$ and set of parameters $\Theta \subset \R$. 
	Fix a \emph{confidence level} $\alpha \in (0, 1)$ and consider two statistics $\wh \theta^L(X)$ and $\wh \theta^R(X)$. Whenever 
	\begin{equation}
		\label{eq:coverage-property}
		\P_\theta\{ \theta \in [\wh \theta^L(X), \wh \theta^R(X)] \} \geq 1 - \alpha
	\end{equation}
	for any $\theta \in \Theta$, we say that $[\wh \theta^L(X), \wh \theta^R(X)]$ is a \emph{confidence interval} at level $1 - \alpha$.
\end{definition}
Inequality~\eqref{eq:coverage-property} is called the \emph{coverage} property of the confidence interval.
More generally, when $\Theta \subset \R^d$, we will say that $S(X)$ is a \emph{confidence set} if it is a statistic satisfying the coverage property $\P_\theta\{ \theta \in S(X) \} \geq 1 - \alpha$ for any $\theta \in \Theta$.
\begin{remark}
	Whenever we need only an upper or lower bound on $\theta$ (for instance, when we need to check statistically that some toxicity level is below some threshold), we build a \emph{unilateral} or \emph{one-sided} confidence interval, where we choose either $\wh \theta^L = -\infty$ ($0$ for the Bernoulli model) or $\wh \theta^R = +\infty$ ($1$ for the Bernoulli model).
	Indeed, at a fixed level $1 - \alpha$, the bound of a one-sided confidence interval is tighter than the same sided bound corresponding to a two-sided interval. 
\end{remark}
But, we can do better for the Bernoulli model (or any model where $X$ is bounded) thanks to the following Hoeffding inequality.
\begin{theorem}
	\label{thm:hoeffding}
	Let $X_1, \ldots, X_n$ be independent random variables such that $X_i \in [a_i, b_i]$ almost surely and let $S = \sum_{i=1}^n X_i$. Then,
	\begin{equation*}
		\P[ S \geq \E S + t] \leq \exp\Big( - \frac{2 t^2}{\sum_{i=1}^n (b_i - a_i)^2} \Big)
	\end{equation*}
	holds for any $t >0$.
\end{theorem}
Theorem~\ref{thm:hoeffding} is something called a deviation inequality: it provides a control on the probability of deviation of $S$ with respect to its mean.
It shows that bounded random variables are \emph{sub-Gaussian}, since it shows that the queue of $S - \E S$ is bounded by $\exp(-c t^2)$ for some constant $c$ (that depends on $n$).
The proof of this inequality is provided in Chapter~??\todo{insert reference} below.
\todo{Provide the proof of Hoeffding}

\paragraph{Back to Bernoulli.} % (fold)

% paragraph back_to_bernoulli (end)
Let's apply Theorem~\ref{thm:hoeffding} to the Bernoulli model $X_i \sim \ber(\theta)$ so that $a_i = 0$, $b_i = 1$ and therefore $\P[ S \geq \E S + t] \leq e^{-2 t^2 / n}$.
Using again Theorem~\ref{thm:hoeffding} with $X_i$ replaced by $-X_i$ together with an union bound%
\sidenote{}%
leads to $\P[ | S - \E S | \geq t] \leq 2 e^{-2 t^2 / n}$.
So, for some $\alpha \in (0, 1)$, we obtain another confidence interval, since the following coverage property holds:
\begin{equation*}
	\P \bigg[ \wh \theta_n - \sqrt{\frac{\log(2 / \alpha)}{2n}} \leq \theta \leq \wh \theta_n 
	+ \sqrt{\frac{\log(2 / \alpha)}{2n}} \bigg] \geq 1 - \alpha.
\end{equation*}
This proves that $[\wh \theta \pm \sqrt{\log(2 / \alpha) / (2n)}]$ is a confidence interval at level $1 - \alpha$.%
\sidenote{For $1000$ tosses and $420$ heads, the realization of this interval at level $95\%$ is $[0.37, 0.46]$. It's a bit more precise than the previous one, which was based on Markov's inequality.}
Let's compare the two confidence intervals we obtained so far for the Bernoulli model.
We have of course
\begin{equation*}
	\frac{1}{2 \sqrt{n \alpha}} > \sqrt{\frac{\log(2 / \alpha)}{2n}} 
\end{equation*}
for $\alpha$ small enough\todo{etre plus precis ici}, although both sides are $O(1 / \sqrt n)$.
Only the dependence on the level $\alpha$ is improved with the confidence interval obtained through Hoeffding's inequality, since it exploits the sub-Gaussianity of the Bernoulli distribution, while the first confidence interval only used the upper bound~\eqref{eq:l2_entrails_proba} on the variance.%
\sidenote{There is yet another way to build a (non-asymptotic) confidence interval, using a more computational approach, see for instance ????}

\paragraph{Asymptotic coverage.}

For both the previous confidence intervals, we adopted a \emph{non-asymtotic} approach: the coverage properties hold for any value of $n \geq 1$.
This is possible since we know things about the distribution of $S$, which is a simple $\bin(n, \theta)$ distribution.
However, in general, the \emph{exact} distribution of an estimator $\wh \theta_n$ cannot always be made explicit, and in such cases, we often use Gaussian approximations, thanks to the central limit theorem. 
Let's do this for the Bernoulli model.
Indeed, we know from~\eqref{eq:tcl-bernoulli} that
\begin{equation}
	\label{eq:portemanteau-bernoulli}
	\P_\theta \bigg[ \sqrt{\frac{n}{\theta (1 - \theta)}} (\wh \theta_n - \theta) \in I
	\bigg] \rightarrow \P[Z \in I]
\end{equation}
where $Z \sim \nor(0, 1)$ for any closed interval $I \subset \R$.%
\sidenote{This uses the porte-manteau theorem, which says that $X_n \gosto X$ if and only if $\P(X_n \in A) \goes \P(X \in A)$ for any Borelian set $A$ such that $\P(X \in \partial A) = 0$, where $\partial A$ stands for the boundary of $A$.}%
Using $I = [-q_\alpha, q_\alpha]$ with $q_\alpha = \Phi^{-1}(1 - \alpha / 2)$ we end up%
\sidenote{We recall that $\Phi^{-1}$ is the inverse of the distribution function $\Phi(x) = \P( Z \leq x)$ of the $\nor(0, 1)$ distribution, that we call the \emph{quantile} function of $\nor(0, 1)$.}%
with the fact that
\begin{equation}
	\label{eq:not-and-ic}
	\P_\theta\bigg\{ \theta \in \Big[ \wh \theta_n \pm q_\alpha \sqrt{\frac{\theta(1 - \theta)}{n}} \Big] \bigg\} \rightarrow 1 - \alpha.
\end{equation}
This is interesting, but not enough to build a confidence interval, since the interval in Equation~\eqref{eq:not-and-ic} depends on $\theta$ through the variance term $\theta(1 - \theta)$.
Indeed, a confidence interval must be something that does \emph{not} depend on $\theta$.
We need to work a little bit more in order to remove the dependence on $\theta$ from this interval. 
We do the same as before: we use the fact that $\theta (1 - \theta) \leq 1 / 4$ for any $\theta \in [0, 1]$, so that
\begin{equation*}
	\liminf_n \P_\theta \bigg\{ \theta \in \Big [\wh \theta_n \pm \frac{q_\alpha}{2 \sqrt n} \Big] \bigg\} \geq 1 - \alpha.
\end{equation*}
This is what we call a confidence interval \emph{asymptotically of level} $1 - \alpha$ constructed \emph{by excess}.

In the above construction of an asymptotic confidence interval, we used the central limit theorem to approximate the $\bin(n, \theta)$ distribution (the distribution of $S = \sum_{i=1}^n X_i$) by a Gaussian distribution.
This requires $n$ to be ``large enough'', but the central limit theorem does not tell us how large.
We can quantify this better by trying to quantify how close the distribution function of $S$ is to the one of $\nor(0, 1)$, using the following theorem.
\begin{theorem}[Berry-Esseen]
	Let $X_1, \ldots, X_n$ be i.i.d random variables such that $\E X_i = 0$ and $\var(X_i) = \sigma^2$ and introduce the distribution function%
	\marginnote{BLABLABLA}%
	\begin{equation*}
		F_n(x) = \P \bigg[ \frac{\sum_{i=1}^n X_i}{\sqrt{n \sigma^2}} \leq x \bigg]
	\end{equation*}
	for any $x \in \R$. Then, the following inequality holds:
	\begin{equation*}
		\sup_{x \in \R} |F_n(x) - \Phi(x)| \leq \frac{c \kappa}{\sigma^3 \sqrt n},
	\end{equation*}
	where $\kappa = \E |X_1|^3$ (assumed finite) and where $c$ is a purely numerical constant (the best known one is $c = 0.4748$).
\end{theorem}
We provide a proof with a worse constant $c$ in Section~??? below.\todo{insert proof}
Note that there is also a lower bound with $c \geq 0.4097$, as explained in ?? and that the best constant $c = 0.4748$ for the upper bound is from ???.
Note also that a similar result holds if the $X_i$ are independent but not identically distributed.

For Bernoulli with have $\E|X_1|^3 = \theta$ and $\sigma^3 = (\theta(1 - \theta))^{3/2}$ so that 
\begin{equation*}
	|F_n(x) - \Phi(x)| \leq \frac{3}{\sqrt{n \theta (1 - \theta)^3}}
\end{equation*}
which shows that the approximation by the Gaussian distribution deteriorates whenever $\theta$ is close to $0$ or $1$.

\paragraph{Reparametrization.} % (fold)

Another asymptotic approach allowing to get rid of the dependence on $\theta$ in the confidence interval~\eqref{eq:not-and-ic} is based on the idea of reparametrization.
Indeed, given a statistical model $\{ P_\theta : \theta \in \Theta \}$ and a bijective function $g : \Theta \goes \Lambda$ we can use instead the ``reparametrized''model $\{ Q_\lambda : \lambda \in \Lambda \}$ where $Q_\lambda = P_{g^{-1}(\lambda)}$.
Let us also remark that if $[\wh \theta^L, \wh \theta^R]$ is a confidence interval for $\theta$ at level $1 - \alpha$ and if $g$ is an increasing function, then 
$[g(\wh \theta^L), g(\wh \theta^R]$ is a confidence interval for $g(\theta)$ at level $1 - \alpha$.
Now, a natural question is to understand if the convergences we used above (in particular, the convergence in distribution involved in the central limit theorem) is stable under such a reparametrization.
\begin{example}
	\label{ex:expo}
	Consider a iid dataset $X_1, \ldots, X_n$ with distribution $\expo(\theta)$ with scale parameter $\theta > 0$, namely the distribution $P_\theta(dx) = \theta e^{-\theta x} \ind{x \geq 0} dx$. 
	We have $\E(X_1) = 1 / \theta$ and $\var(X_1) = 1 / \theta^2$, so that using the law of large numbers and the central limit theorem we have
	\begin{equation*}
		\bar X_n \goas \theta^{-1} \quad \text{ and } \quad \sqrt n (\bar X_n - \theta^{-1}) \leadsto \nor(0, \theta^{-2})
	\end{equation*}
	when $n \rightarrow +\infty$.
	Since $x \mapsto 1 / x$ is a continuous function on $(0, +\infty)$, we know that $(\bar X_n)^{-1} \goas \theta$ so that a strongly consistent estimator is given by $\wh \theta = (\bar X_n)^{-1}$.
	But what can we say about the convergence in distribution of 
	$\sqrt n (\wh \theta_n - \theta)$ ?
\end{example}
This is answered by so-called $\Delta$-method, described in the next theorem.
\begin{theorem}[$\Delta$-method]
	\label{thm:delta-method}
 	Let $(Z_n)_{n \geq 1}$ be a sequence of real random variables and assume that 
	$a_n(Z_n - z) \leadsto Z$, where $(a_n)_{n \geq 1}$ is a positive sequence such that $a_n \goes +\infty$, where $z \in \R$ and where $Z$ is a real random variable.
 	If $g$ is a function defined on a neighborhood of $z$ and differentiable at $z$, we have 
 	\begin{equation}
 		a_n (g(Z_n) - g(z)) \leadsto g'(z) Z
 	\end{equation}
 	as $n \goes +\infty$.
\end{theorem}
The proof easily follows from a first-order Taylor expansion of $g$ around~$z$, as explained in Section~??? below. \todo{insert proof}
A particularly useful case for statistics is when $Z$ is Gaussian.
For instance, if $\sqrt n (\wh \theta_n - \theta) \leadsto \nor(0, \sigma(\theta)^2)$, we have
\begin{equation*}
	\sqrt n (g(\wh \theta) - g(\theta)) \leadsto 
	\nor(0, \sigma(\theta)^2 (g'(\theta))^2)
\end{equation*}
whenever $g$ satisfies the conditions of Theorem~\ref{thm:delta-method}.
Going back to the $\expo(\theta)$ example from Example~\ref{ex:expo}, we obtain with $g(x) = 1 / x$ and since $\wh \theta = g(\bar X_n)$ that $\sqrt n (\wh \theta - \theta) \leadsto \nor(0, \theta^2)$.
Another result which provides stability for the convergence in distribution under a smooth mapping is the so-called Slutsky theorem.
\begin{theorem}[Slutsky]
	\label{thm:slutsky}
	Let $(X_n)_{n \geq 1}$ and $(Y_n)_{n \geq 1}$ be sequences of real random variables such that $X_n \leadsto X$ and $Y_n \leadsto y$ where $X$ is some real random variable and $y \in \R$.
	Then, we have that $Y_n \gopro y$ and $(X_n, Y_n) \leadsto (X, y)$ as $n \goes +\infty$. In particular, we have $f(X_n, Y_n) \leadsto f(X, y)$ for any continuous function $f$.
\end{theorem}
The proof of Theorem~\ref{thm:slutsky} is given in Section~? below.\todo{insert proof}
The $\Delta$-method provides stability for the convergence in distribution when a differentiable function is applied to a sequence, while Slutsky theorem provides ``algebraic'' stability when combining two sequences converging respectively in distribution and probability.
\sidenote{Be careful with convergence in distribution. Please keep in mind that this mode of convergence is about the convergence of the distributions and not the convergence of the random variables (hence its name). The notation $X_n \gosto X$ is rather misleading but convenient. In particular, nothing can be said in general about $f(X_n, Y_n)$ when we know that $X_n \gosto X$ and $Y_n \gosto Y$ (unless $X_n$ and $X_n$ are independent sequences).}

\paragraph{Back again to Bernoulli.}

We have $\wh \theta_n \gopro \theta$, so that $(\wh \theta_n (1 - \wh \theta_n)^{1/2} \gopro (\theta (1 -  \theta))^{1/2}$ since $x \mapsto (x(1-x))^{1/2}$ is continuous on $[0, 1]$ and let us write
\begin{equation*}
	\frac{\sqrt n (\wh \theta_n - \theta)}{\sqrt{\wh \theta_n (1 - \wh \theta_n)}} 
	= \frac{\sqrt n (\wh \theta_n - \theta)}{\sqrt{ \theta (1 -  \theta)}} \times 
	\sqrt{\frac{ \theta (1 -  \theta)}{\wh \theta_n (1 - \wh \theta_n)}} =: A_n \times B_n.
\end{equation*}
We know that $A_n \gosto \nor(0, 1)$ and that $B_n \gopro 1$.
Therefore, using Theorem~\ref{thm:slutsky} leads%
\sidenote{With $f(x, y) = xy$.}%
to
\begin{equation*}
	\sqrt{\frac{n}{\wh \theta_n (1 - \wh \theta_n)}} (\wh \theta_n - \theta) \gosto \nor(0, 1).
\end{equation*}
Yes, we just replaced $\theta$ by $\wh \theta_n$ in the variance term $\theta(1 - \theta)$ of the limit~\eqref{eq:portemanteau-bernoulli}, but doing so required Slutsky's theorem to prove this rigorously, and this provides us another confidence interval with asymptotic coverage given by
\begin{equation*}
	\P_\theta \bigg\{ \theta \in \Big[ \wh \theta_n \pm q_\alpha \sqrt{\frac{\wh \theta_n (1 - \wh \theta_n)}{n}} \Big] \bigg\} \goes 1 - \alpha
\end{equation*}
as $n \goes +\infty$.%
\sidenote{With $1000$ tosses and $420$ heads, the realization of this confidence interval at level $95\%$ is $[0.38, 0.45]$.}%


\section{Tests} % (fold)
\label{sec:tests}

Let us consider in this section, once again a model $\{ P_\theta : \theta \in \Theta \}$ and a sample $X$.
Here, we want to decide between to hypothesis $H_0$ and $H_1$, where
\begin{equation*}
	H_i \text{ corresponds to the fact that } \theta \in \Theta_i
\end{equation*}
for $i \in \{ 0, 1 \}$, where $\{ \Theta_0, \Theta_1 \}$ is a partition of $\Theta$.

Let us give, to help understand this complex concept of statistical testing, a morbid example.
Imagine that you need to decide if a patient has cancer or not.
The patient has cancer if some parameter $\theta \in (0, 1)$ about him satisfies 
$\theta \geq 0.42$.
We \emph{choose} $\Theta_0 = [0.42, 1]$ and $\Theta_1 = [0, 0.42)$.
We need to construct a testing function $\varphi : E \goes \{ 0, 1 \}$ that maps $X \mapsto \phi(X)$, our decision being the value of $\varphi(X)$.
We are correct if $\varphi(X) = i$ whenever $\theta \in \Theta_i$ for $i \in \{ 0, 1 \}$ and incorrect whenever  $\varphi(X) = 1 - i$ whenever $\theta \in \Theta_i$.
We have two types of errors: the \emph{Type-I error} or \emph{first-order error}, given by 
\begin{equation}
	\label{eq:type-1-error}
	\P_\theta[ \varphi(X) = 1] = \E_\theta \varphi(X) \text{ if } 
	\theta \in \Theta_0
\end{equation}
and the \emph{Type-II error} or \emph{second-order error}, given by 
\begin{equation}
	\label{eq:type-2-error}
	\P_\theta[ \varphi(X) = 0] = 1 - \E_\theta \varphi(X) \text{ if } 
	\theta \in \Theta_1.
\end{equation}
In the above cancer detection example, if the Type I error corresponds to 
\todo{redonner l'exemple du cancer precisement et clairement, dire que ce n'est pas symetrique et qu'il faut bien chosir les hypthese}

The function $\beta : \Theta \goes [0, 1]$ that maps $\theta \mapsto \beta(\theta) = \E_\theta \varphi(X)$ is called the \emph{power function} of the test $\varphi$.
\todo{definition}

Ideally, we would like $\beta(\theta) \approx 0$ for $\theta \in \Theta_1$ and $\beta(\theta) \approx 1$ for $\theta \in ???$ \todo{correct} but this is impossible: if $\Theta$ is a connex set then $\Theta_0$ and $\Theta_1$ share a common frontier and $\beta$ must therefore be discontinuous on it, while $\beta$ is in general a continuous function.
Therefore, It is hard to make both Type I and Type II errors at the same time, namely to make $\beta$ small both on $\Theta_0$ and $\Theta_1$

\paragraph{The Neyman and Pearson approach: dissymetrization of the problem} % (fold)

The now commonly adopted awy to perform statistical tests is through the Neyman Pearson approach, by using a ``dissymetrization'' of the problem.
The convention is to choose the hypothesis $H_0$ as the one that we must not miss: we choose $H_0$ in a way that the the Type I error (we have $\varphi(X)=1$ while $\theta \in \Theta_0$) is more serious than the Type II error (we have $\varphi(X)=0$ while $\theta \in \Theta_1$).
Type I error is always the rejection of $H_0$ while it's true, Type II error is always acceptation of $H_0$ while it is false, the only thing we need to decide is what are $H_0$ and $H_1$.

Going back to our morbid example, it is natural to define $H_0$ as ``the patient has cancer'' and $H_1$ as ``the patient does not have cancer''.
Type I error means in this case ``we decide that the patient does not have cancer while he actually has it'' and Type II error means ``we decide that the patient has cancer while he does not have it''.
In this example, common sense tells us that with this choice, we have indeed that the Type I error is more serious than the Type II. 
This can be debated (it always can), since telling a patient he has cancer while he has not can lead to a depression that could have be avoided, and entail heavy useless treatments. 
Of course this discussion describes nothing realistic, and is only intended to help the reader understand two things about hypothesis testing: the choice of the hypothesis is a modelization choice that must be guided by common sense, and the two errors are highly non-symmetric.


We call $H_0$ the \emph{null hypothesis} and $H_1$ the \emph{alternative hypothesis}.
When $\varphi(X) = 0$ we say that we \emph{accept $H_0$} or simply that the test \emph{accepts} while $\varphi(X) = 1$ means that the test rejects $H_0$.
Let us wrap up all of this in the following definition.
\begin{definition}
	Given a statistical testing problem with hypothesis
	\begin{equation*}
		H_0 : \theta \Theta_0 \text{ and } H_1 : \theta \Theta_1
	\end{equation*}
	and a testing function $\varphi : E \goes \{ 0, 1 \}$, we define the following things:
	\begin{enumerate}
		\item We call the set $R = \{ x \in E : \varphi(x) = 1 \}$ the \emph{rejection set} or \emph{reject region} of the test $\varphi$, and we call $R^\complement$ the \emph{acceptation region}
		\item The function $\beta : \Theta \goes [0, 1]$ given by $\beta(\theta) = \E_\theta [\varphi(X)]$ is call the \emph{power function} of the test
		\item The restrictions of this function $\beta : \Theta_0 \goes [0, 1]$ is call the \emph{Type I error} or \emph{first order error} while the restriction $\beta : \Theta_1 \goes [0, 1]$ is called the power of the test. The function $1 - \beta : \Theta_1 \goes [0, 1]$ is called \emph{Type II error} or \emph{second order error}.
		\item Whenever $\sup_{\theta \in \Theta_0} \beta(\theta) \leq \alpha$ for some fixed $\alpha \in (0, 1)$, we say that the test \emph{has level} $\alpha$.
	\end{enumerate}
\end{definition}


The idea of dissymetrization of the hypothesis is as follows: given level $\alpha \in (0, 1)$ (something like $1\%$, $5\%$ or $10\%$) we look for a test at level $\alpha$ whose whose power is as large as possible for $\theta \in \Theta_1$.
This means that, the test is built so that the Type I error is controlled, while we don't control the Type II error, but only study it.
We can simply compare Type II error and choose the statistical test which maximizes it, under the fixed shared constraint on the Type I error....

Example. Back to Bernoulli, where $X_1, \ldots, X_n$ i.i.d distributed as $\ber(\theta)$ and $S = \sum_{i=1}^n X_i \sim \bin(n, \theta)$.
We consider the following hypothesis:
\begin{equation*}
	H_0 : \theta \leq \theta_0 \quad \text{ against } \quad H_1 : \theta > \theta_0
\end{equation*}
so that $\Theta = (0, 1)$ and $\Theta_0 = (0, \theta_0]$ and $\Theta_1 = (\theta_0, 1)$.
We studied before the estimator $\wh \theta = \bar X_n$, and know that it is a good estimator (we'll see later that we can't really improve it...)
A natural idea is therefore to reject $H_0$ if $\wh \theta$ is too large.

Recipe. We build a test by defining its rejection set $R$. The shape of the rejection set can be easily guessed by common sense: we simply look at the alternative hypothesis $H_1$. 
Since we want to reject when $\theta > \theta_0$, we want to consider a rejection set $R = \{ \wh \theta > c \}$, for some constant $c$ chosen so that the Type I error is controlled by $\alpha$.
Note that choosing $c = \theta_0$ is a bad idea: using the central limit theorem, we see that $\P_{\theta_0}(\wh \theta - \theta_0) \goes 1/2$, so we need to give some more slack (by increasing the constant $c$ by some amount) so that the Type I error can be indeed smaller than $\alpha$.
We understand at this point that $c$ will depend on $\alpha, \theta_0$ and the sample size $n$.
So, let's look for $c$ such that $\sup_{\theta \leq \theta_0} \P_\theta[S > n c] \leq \alpha$ (one again $c = c(n, \alpha, \theta_0$)
\begin{equation*}
	\P_\theta[\wh \theta > c] = \P_\theta[S > n c] = \P [\bin(n, \theta) > nc] = \beta(\theta)
\end{equation*}
So we need to understand the variations of $\theta \mapsto \beta(\theta)$.
Intuitively, $\beta(\theta)$ should be increasing with $\theta$, since when $\theta$ increases, we get more ones, so $S$ is larger.
This can be nicely formalized using the \emph{stochastic ordering}.
\begin{definition}
	Let $P$ and $Q$ be two probability measures on the same probability space. We say that \emph{$Q$ stochastically dominates $P$}, that we denote $P \lest Q$, whenever one of the following equivalent points is granted:
	\begin{enumerate}
		\item There are two real random variables $X, Y$ (on the same probability space) such that $X \sim P$ and $Y \sim Q$ such that $\P(X \leq Y) = 1$
		\item We have $F_P(x) \geq F_Q(x)$ for any $x$ (where $F_P$ is the distribution function of $P$, namely $F_P(x) = \P((-\infty, x])$) which means that $P[()] \leq Q[(x, +\infty)]$
		\item We have $F_P^-(p) \leq F_Q^-(p)$ for any $p \in [0, 1]$ where $F_P^-(p) = \inf \{ x \in \R : F_P(x) \geq p \}$ is the generalized inverse of $F_P$(recall that it is a cad-lag function)
		\item For any non-decreasing and bounded function $f$ we have $\int f dP \leq \int f dQ$.
	\end{enumerate}
\end{definition}

\begin{proof}
	The proofs of $(2) \Leftrightarrow (3) \equiv (4)$ are quite standard and not really interesting. $(1) \Rightarrow (2)$ is obvious. The proof of $(2) \Rightarrow (1)$ is nice and interesting by itself, since it uses something called a "coupling" argument, which is a very powerful technique used in probability theory REFREF.
	More precisely, we use below something called a ``quantile coupling''.
	Let us consider $U \sim \uni([0, 1])$ \todo{define uniform} on some probability space and define $X = F_P^-(U)$ and $Y = F_P^-(U)$.
	We have by construction that $X \sim F_P$ and $Y \sim F_Q$. \todo{recall some properties of generalized inverse}. But then
	\begin{equation*}
		\P(X \leq Y) = \P(F_P^-(U) \leq F_P^-(U)) = 1
	\end{equation*}
	since 3. tells us that $F_P^-(p) \leq F_P^-(p)$ for any $p in [0, 1]$.
\end{proof}
This proof is remarkably simple, but very tricky and beautiful, and the basis of the powerful coupling argument in probability theory.

A first example is the fact that $\expo(\lambda_2) \lest \expo(\lambda_1)$ whenever $\lambda_1 \leq \lambda_2$.
Another example will prove more useless for our statistical test for the $\ber$ model.
\begin{example}
	We have that $\ber(n, \theta_1) \lest \ber(n, \theta_2)$ whenever $\theta_1 \leq \theta_2$. How can we prove this ? By using again a coupling argument.
	Let us consider $U_1, \ldots, U_n$ i.i.d $\uni([0, 1])$ and define $S_i = \# \{ k : U_k \leq \theta_i \}$ for $i \in \{ 1, 2 \}$. By construction we have that $S_i \sim \bin(n, \theta_i)$, and that $S_1 \leq S_2$ a.s. This proves that $\ber(n, \theta_1) \lest \ber(n, \theta_2)$.
\end{example}
A consequence of Example~? together with definition~? is that $F_{\bin(n, \theta_2)} \leq F_{\bin(n, \theta_1)}$ whenever $\theta_1 \leq \theta_2$.
So, going back to the Type I error control, we can now derive the following control:
\begin{equation*}
	\sup_{\theta \leq \theta_0} \P_\theta[ \wh \theta > c] = \sup_{\theta \leq \theta_0} (1 - F_{\bin(n, \theta)}(n c)) \leq (1 - F_{\bin(n, \theta_0)}(n c)).
\end{equation*}
We can therefore find out $c$ as small as possible so that $1 - F_{\bin(n, \theta_0)}(n c) \leq \alpha$.
This can be done easily numerically, since we don't have a general explicit solution for such a $c$ in general.
Otherwise, we can use (but it is less precise, hence leading to a slighlty less powerfull test)
\begin{equation*}
	\P_{\theta_0} [\wh \theta > c] = \P_{\theta_0}[S - n \theta_0 > c'] \leq e^{-2 t^2 / n} = \alpha
\end{equation*}
choosing $t = \sqrt{n \log(1 / alpha) / 2}$. 
We can therefore consider the test with rejection set $R = \{ \wh \theta \geq \theta_0 + \sqrt{ \frac{\log(1 / \alpha)}{2n}}\}$, to obtain a test at level $\alpha$.

We could also use an asymptotic approach using a rejection set $R = \{ \wh \theta > \theta_0  + \sqrt{\frac{\theta_0 (1 - \theta_0)}{n}} \Phi^{-1}(1 - \alpha) \} = \theta_0 + \delta_n$ which is asymptotically of level $\alpha$, using similar arguments as what we did before, and using the central limit theorem.

What about the power of the test ?
We know that $\wh \theta \goas \theta$ and that $\delta_n \go 0$.
So, \textbf{under $H_1$, namely whenever $\theta > 0$} we have
\begin{equation*}
	\beta(\theta) = \P_\theta[ \wh \theta > \theta_0 + \delta_n] \go 1
\end{equation*}
once again WHEN $\theta > \theta_0$.
This means that the power of the test indeed goes to $1$.
In this case, we say that the test is \emph{consistent} or \emph{convergent}.

\begin{remark}
 	Note that the convergence of $\beta(\theta)$ is not uniform in $\theta$ since the limit is discontinuous while $\beta(\theta)$ is continuous.
\end{remark}

An interesting vocabulary os as folos.
We say that $(\wh \theta - \theta) / \sqrt{n \theta_0(1 - \theta_0)}$ is an asymptotically \emph{ancillary} statistic, since its asymptoyic distributiond oes not depend on the parameter.
The read understands now that the construction of confidence intervals or statistical test relies on the construction and study or ancillary (resp. asymptotically ancillary)statistics, when building confidence intervals or test (resp confidence intervals with asymptotic coverage or tests asymptotically of controlled level).
This allows to tune the IC or tests, BLABLA

Whenever $X \sim P_\theta$ and the distribution of $f_\theta(X)$ does not depend on $\theta$, then we say that $f_\theta(X)$ is an \emph{ancillary} statistic.


There is of course a strong connection between confidence intervals and tests, as explained in the following proposition.
\begin{proposition}
	If $D(X)$ is a confidence region of level $1 - \alpha$, namely $\P_\theta[ \theta \in D(X)] \geq 1 - \alpha$ then the test $\varphi(X) = 1$ iff $D(X) \cap \Theta_0  = \emptyset$ is of level $\alpha$.
\end{proposition}
\begin{proof}
	We have
	\begin{equation*}
		\P_\theta [\varphi(X) = 1] = \P_\theta[ D(X) \cap \Theta_0 = \emptyset] \leq \P_\theta[ \theta \notin D(X) ] \leq \alpha.
	\end{equation*}
	for any $\theta \in \Theta_0$.
\end{proof}

For $\Theta \subset \R$ we will often end up in one of the following situations.

\begin{table}
\begin{tabular}{|c|c|c|c|}
	$\Theta_0 = \{ \theta_0 \}$ & $\Theta_0 = [\theta_0, +\infty)$ & $\Theta_0 = (-\infty, \Theta_0]$ & $\Theta_0 = [\theta_0 - \delta, \theta_0 + \infty]$ \\
	Simple hypothesis & \multicolumn{3}{c}{Multiple hypothesis} \\
	& \multicolumn{2}{c}{One-sided hypothesis} & Two-sided hypothesis
\end{tabular}
	\caption{Standard null hypothesis.}
	\label{tab:standard-null-hypothesis}
\end{table}


Recipe. A one-sided test can be obtained using a one-sided confidence interval in the oppositive direction of $\Theta_0$. A test with two-sided null hypothesis can be obtained using a (two-sided confidence interval).
Example are for $H_0 : \theta = \theta_0$ versus $H_1 : \theta > \theta_0$ we use $R = \{ \wh \theta > \theta_0 + c \}$ while for $H_0 : \theta = \theta_0$ versus $H_1 : \theta \neq \theta_0$ we use $R = \{ | \wh \theta - \theta_0 | > c \}$ .

\todo{optimality of test, rule to build optimal tests...}


p-value. For any test, if $\alpha$ is very small, the test has no choice but to accept $H_0$, since it has almost no slack to eventually be wrong about $H_0$.
The only way to build a test with $\alpha = 0$ if to set $\varphi = 0$, namely it never rejects, namely for the cancer case from above, we tell all patient that they have cancer. It's a strategy where we never miss cancers (Type I error is zero)
but arguably it's not a perfect one.
But in such a case, the power of the test is constantly equal to $0$, leading to the worst possible Type II error (equal to $1$).

We need to give some slack to the test by taking $\alpha > 0$ so that we have a chance to obtain a test with some power, this gives to the test a small and controlled margin.

When $X$ and the testing regression are fix, we therefore expect that when $\alpha$ is small (close to $0$) then the test will accept (since it has no margin to reject), while when $\alpha$ increases up to a certain point, the test rejects.
\todo{mettre dessin}
There is, therefore, a value of $\alpha$ starting from which the test starts to reject the assumption.
This value is called the $p$-value of the test.

\paragraph{About $p$-values.} % (fold)

% paragraph about_ (end)
Let $R_\alpha$ be the rejection region of a test at level $\alpha$, namely we have $\sup_{\theta \in \Theta_0} \P_\theta(R_\alpha) \leq \alpha < \alpha'$ for any $\alpha' > \alpha$, so that $R_\alpha$ also is a rejection region at level $\alpha'$.
Generally (but not always) we have $R_\alpha \subset R_\alpha'$ whenever $\alpha < \alpha'$.
In such a case, we can define the $p$-value of the test as
\begin{equation*}
	\alpha(X) = \inf \{ \alpha \in [0, 1] : X \in R_\alpha \}.
\end{equation*}
Let us compute the $p$-value for the previous test for the $\ber$ model.
\begin{example}
	For the test $H_0 : \theta \leq \theta_0$ versus $H_1 : \theta > \theta_0$ we use the rejection set $R_\alpha = \{ \wh \theta > \theta_0 + \Phi^{-1}(1 - \alpha) \sqrt{\frac{\theta_0 (1 - \theta_0)}{n}} \}$.
	In this case we have
	\begin{align*}
		\alpha(X) &= \inf \{ \alpha \in [0, 1] : \wh \theta > \theta_0 + \theta_0 + \Phi^{-1}(1 - \alpha) \sqrt{\frac{\theta_0 (1 - \theta_0)}{n}}  \} \\
		&= \inf \{ \alpha \in [0, 1] : \alpha > 1 - \Phi\Big(  \sqrt{\frac{n}{\theta_0 (1 - \theta_0)}} (\wh \theta - \theta_0) \Big)  \} \\
		&= 1 - \Phi\Big(  \sqrt{\frac{n}{\theta_0 (1 - \theta_0)}} (\wh \theta - \theta_0) \Big)
	\end{align*}
\end{example}


\begin{recipe}
	In practice, we never choose the level $\alpha$, but we compute the $p$-value using the definition of the test and the data. The value of the $p$-value gives somehow, how much we are willing to believe in $H_0$.
\end{recipe}


\begin{warning}
	In practice, we never choose the level $\alpha$, but we compute the $p$-value using the definition of the test and the data. The value of the $p$-value gives somehow, how much we are willing to believe in $H_0$.
\end{warning}


\todo{Exemple mise sur le marche de medicaments, les p values, attention c'est aussi critique, etc...}

For instance, if $\alpha(X) \leq 10^3$ then we are strongly rejecting $H_0$, since it would require a level $\alpha < 10^3$ to accept $H_0$, which is very small. 
If $\alpha(X) = 3\%$, the result of the test is rather ambiguous while $\alpha(X) = 30\%$ is a strong acceptation of $H_0$.
What researchers  do when making a scientific decision based on statistical tests it to publish the $p$-value of the tests they perform, in physics you need ???? \todo{doner exemples}


\pagelayout{wide}

\section{Proofs} % (fold)
\label{sec:proofs}

This Section gathers the proofs of some results provided in this chapter.
This Section gathers the proofs of some results provided in this chapter.
This Section gathers the proofs of some results provided in this chapter.
This Section gathers the proofs of some results provided in this chapter.
This Section gathers the proofs of some results provided in this chapter.
This Section gathers the proofs of some results provided in this chapter.

\pagelayout{margin}

% section proofs (end)

% paragraph paragraph_name (end)


% section tests (end)

% section confidence_intervals (end)



\end{document}
