%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% kaobook
% LaTeX Template
% Version 1.2 (4/1/2020)
%
% This template originates from:
% https://www.LaTeXTemplates.com
%
% For the latest template development version and to make contributions:
% https://github.com/fmarotta/kaobook
%
% Authors:
% Federico Marotta (federicomarotta@mail.com)
% Based on the doctoral thesis of Ken Arroyo Ohori (https://3d.bk.tudelft.nl/ken/en)
% and on the Tufte-LaTeX class.
% Modified for LaTeX Templates by Vel (vel@latextemplates.com)
%
% License:
% CC0 1.0 Universal (see included MANIFEST.md file)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
	fontsize=11pt, % Base font size
	twoside=false, % Use different layouts for even and odd pages (in particular, if twoside=true, the margin column will be always on the outside)
	%open=any, % If twoside=true, uncomment this to force new chapters to start on any page, not only on right (odd) pages
	%chapterprefix=true, % Uncomment to use the word "Chapter" before chapter numbers everywhere they appear
	%chapterentrydots=true, % Uncomment to output dots from the chapter name to the page number in the table of contents
	numbers=noenddot, % Comment to output dots after chapter numbers; the most common values for this option are: enddot, noenddot and auto (see the KOMAScript documentation for an in-depth explanation)
	%draft=true, % If uncommented, rulers will be added in the header and footer
	%overfullrule=true, % If uncommented, overly long lines will be marked by a black box; useful for correcting spacing problems
]{kaobook}

% Set the language
\usepackage[english]{babel} % Load characters and hyphenation
\usepackage[english=british]{csquotes} % English quotes

\usepackage{bm}

% Load packages for testing
\usepackage{blindtext}
%\usepackage{showframe} % Uncomment to show boxes around the text area, margin, header and footer
%\usepackage{showlabels} % Uncomment to output the content of \label commands to the document where they are used

% Load the bibliography package
\usepackage{styles/kaobiblio}
\addbibresource{biblio.bib} % Bibliography file

% Load mathematical packages for theorems and related environments. NOTE: choose only one between 'mdftheorems' and 'plaintheorems'.
\usepackage{styles/mdftheorems}
%\usepackage{styles/plaintheorems}


\graphicspath{{images/}} % Paths in which to look for images

% \RequirePackage{times}

\DeclareMathOperator{\cA}{{\mathcal A}}
\DeclareMathOperator{\cB}{{\mathcal B}}
\DeclareMathOperator{\cC}{{\mathcal C}}
\DeclareMathOperator{\cD}{{\mathcal D}}
\DeclareMathOperator{\cE}{{\mathcal E}}
\DeclareMathOperator{\cF}{{\mathcal F}}
\DeclareMathOperator{\cG}{{\mathcal G}}
\DeclareMathOperator{\cM}{{\mathcal M}}
\DeclareMathOperator{\cN}{{\mathcal N}}
\DeclareMathOperator{\cP}{{\mathcal P}}
\DeclareMathOperator{\cX}{{\mathcal X}}
\DeclareMathOperator{\cY}{{\mathcal Y}}
\DeclareMathOperator{\cZ}{{\mathcal Z}}

\DeclareMathOperator{\bA}{{\boldsymbol A}}
\DeclareMathOperator{\bB}{{\boldsymbol B}}
\DeclareMathOperator{\bC}{{\boldsymbol C}}
\DeclareMathOperator{\bD}{{\boldsymbol D}}
\DeclareMathOperator{\bE}{{\boldsymbol E}}
\DeclareMathOperator{\bF}{{\boldsymbol F}}
\DeclareMathOperator{\bG}{{\boldsymbol G}}
\DeclareMathOperator{\bH}{{\boldsymbol H}}
\DeclareMathOperator{\bI}{{\boldsymbol I}}
\DeclareMathOperator{\bJ}{{\boldsymbol J}}
\DeclareMathOperator{\bK}{{\boldsymbol K}}
\DeclareMathOperator{\bL}{{\boldsymbol L}}
\DeclareMathOperator{\bM}{{\boldsymbol M}}
\DeclareMathOperator{\bN}{{\boldsymbol N}}
\DeclareMathOperator{\bO}{{\boldsymbol O}}
\DeclareMathOperator{\bP}{{\boldsymbol P}}
\DeclareMathOperator{\bQ}{{\boldsymbol Q}}
\DeclareMathOperator{\bR}{{\boldsymbol R}}
\DeclareMathOperator{\bS}{{\boldsymbol S}}
\DeclareMathOperator{\bT}{{\boldsymbol T}}
\DeclareMathOperator{\bU}{{\boldsymbol U}}
\DeclareMathOperator{\bV}{{\boldsymbol V}}
\DeclareMathOperator{\bW}{{\boldsymbol W}}
\DeclareMathOperator{\bX}{{\boldsymbol X}}
\DeclareMathOperator{\bY}{{\boldsymbol Y}}
\DeclareMathOperator{\bZ}{{\boldsymbol Z}}

\DeclareMathOperator{\ba}{{\boldsymbol a}}
\DeclareMathOperator{\bb}{{\boldsymbol b}}
\DeclareMathOperator{\bc}{{\boldsymbol c}}
\DeclareMathOperator{\bd}{{\boldsymbol d}}
\DeclareMathOperator{\be}{{\boldsymbol e}}
% \DeclareMathOperator{\bf}{{\boldsymbol f}}
\DeclareMathOperator{\bg}{{\boldsymbol g}}
\DeclareMathOperator{\bh}{{\boldsymbol h}}
\DeclareMathOperator{\bi}{{\boldsymbol i}}
\DeclareMathOperator{\bj}{{\boldsymbol j}}
\DeclareMathOperator{\bk}{{\boldsymbol k}}
\DeclareMathOperator{\bl}{{\boldsymbol l}}
% \DeclareMathOperator{\bm}{{\boldsymbol m}}
\DeclareMathOperator{\bn}{{\boldsymbol n}}
\DeclareMathOperator{\bo}{{\boldsymbol o}}
\DeclareMathOperator{\bp}{{\boldsymbol p}}
\DeclareMathOperator{\bq}{{\boldsymbol q}}
\DeclareMathOperator{\br}{{\boldsymbol r}}
\DeclareMathOperator{\bs}{{\boldsymbol s}}
\DeclareMathOperator{\bt}{{\boldsymbol t}}
\DeclareMathOperator{\bu}{{\boldsymbol u}}
\DeclareMathOperator{\bv}{{\boldsymbol v}}
\DeclareMathOperator{\bw}{{\boldsymbol w}}
\DeclareMathOperator{\bx}{{\boldsymbol x}}
\DeclareMathOperator{\by}{{\boldsymbol y}}
\DeclareMathOperator{\bz}{{\boldsymbol z}}

\DeclareMathOperator{\bLambda}{{\boldsymbol \Lambda}}

\DeclareMathOperator{\bone}{\boldsymbol 1}

\DeclareMathOperator{\beps}{\boldsymbol \varepsilon}
\DeclareMathOperator{\bSigma}{\boldsymbol \Sigma}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\mode}{mode}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\med}{med}


\DeclareMathOperator{\ber}{Bernoulli}
\DeclareMathOperator{\bet}{Beta}
\DeclareMathOperator{\bin}{Binomial}
\DeclareMathOperator{\chisq}{ChiSq}
\DeclareMathOperator{\expo}{Exponential}
\DeclareMathOperator{\fis}{Fisher}
\DeclareMathOperator{\gam}{Gamma}
\DeclareMathOperator{\mul}{Multinomial}
\DeclareMathOperator{\nor}{Normal}
\DeclareMathOperator{\stu}{student}
\DeclareMathOperator{\uni}{Uniform}


\DeclareMathOperator{\sigmoid}{sigmoid}
\DeclareMathOperator{\leb}{Lebesgue}
\DeclareMathOperator*{\argmin}{argmin}

\DeclareMathOperator*{\spa}{span}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\cov}{cov}

\newcommand{\eps}{\varepsilon}

\renewcommand{\P}{\mathbb P}
\newcommand{\E}{\mathbb E}
\newcommand{\R}{\mathbb R}
\newcommand{\N}{\mathbb N}
\newcommand{\var}{\mathbb V}

\newcommand{\wh}{\widehat}
\newcommand{\wt}{\widetilde}

\newcommand{\ind}[1]{\mathbf 1_{#1}}
\newcommand{\grad}{\nabla}


\newcommand{\mgeq}{\succcurlyeq}
\newcommand{\mleq}{\preccurlyeq}
\newcommand{\goes}{\rightarrow}
\newcommand{\go}{\rightarrow}

\newcommand{\norm}[1]{\|#1\|}
\newcommand{\inr}[1]{\langle #1 \rangle}

\newcommand{\gopro}{\overset{\P}{\rightarrow}}
\newcommand{\goas}{\overset{\text{as\ }}{\rightarrow}}
\newcommand{\goqr}{\overset{\text{$L^2$\ }}{\rightarrow}}
\newcommand{\gosto}{\leadsto}


% \newcommand{\lest}{ \underset{\text{st}}{\leq}}
\newcommand{\lest}{\preceq}
% \newcommand{\gest}{\underset{\text{st}}{\qeq}}
\newcommand{\gest}{\succeq}



% \makeindex[columns=3, title=Alphabetical Index, intoc] % Make LaTeX produce the files required to compile the index

% \makeglossaries % Make LaTeX produce the files required to compile the glossary

% \makenomenclature % Make LaTeX produce the files required to compile the nomenclature

% Reset sidenote counter at chapters
%\counterwithin*{sidenote}{chapter}

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	BOOK INFORMATION
%----------------------------------------------------------------------------------------

\titlehead{Some stuff about statistics}
\subject{Lecture notes for the ENS course of Statistics}

\title[Some stuff about Statistics]{Some stuff about Statistics}
% \subtitle{Customise this page according to your needs}

\author[St\'ephane Ga\"iffas"]{St\'ephane Ga\"iffas\thanks{}}

\date{\today}

\publishers{}

%----------------------------------------------------------------------------------------

\frontmatter % Denotes the start of the pre-document content, uses roman numerals

%----------------------------------------------------------------------------------------
%	OPENING PAGE
%----------------------------------------------------------------------------------------

%\makeatletter
%\extratitle{
%	% In the title page, the title is vspaced by 9.5\baselineskip
%	\vspace*{9\baselineskip}
%	\vspace*{\parskip}
%	\begin{center}
%		% In the title page, \huge is set after the komafont for title
%		\usekomafont{title}\huge\@title
%	\end{center}
%}
%\makeatother

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

% \makeatletter
% \uppertitleback{\@titlehead} % Header

% \lowertitleback{
% 	\textbf{Disclaimer}\\
% 	You can edit this page to suit your needs. For instance, here we have a no copyright statement, a colophon and some other information. This page is based on the corresponding page of Ken Arroyo Ohori's thesis, with minimal changes.
	
% 	\medskip
	
% 	\textbf{No copyright}\\
% 	\cczero\ This book is released into the public domain using the CC0 code. To the extent possible under law, I waive all copyright and related or neighbouring rights to this work.
	
% 	To view a copy of the CC0 code, visit: \\\url{http://creativecommons.org/publicdomain/zero/1.0/}
	
% 	\medskip
	
% 	\textbf{Colophon} \\
% 	This document was typeset with the help of \href{https://sourceforge.net/projects/koma-script/}{\KOMAScript} and \href{https://www.latex-project.org/}{\LaTeX} using the \href{https://github.com/fmarotta/kaobook/}{kaobook} class.
	
% 	The source code of this book is available at:\\\url{https://github.com/fmarotta/kaobook}
	
% 	(You are welcome to contribute!)
	
% 	\medskip
	
% 	\textbf{Publisher} \\
% 	First printed in May 2019 by \@publishers
% }
% \makeatother

%----------------------------------------------------------------------------------------
%	DEDICATION
%----------------------------------------------------------------------------------------

% \dedication{
% 	The harmony of the world is made manifest in Form and Number, and the heart and soul and all the poetry of Natural Philosophy are embodied in the concept of mathematical beauty.\\
% 	\flushright -- D'Arcy Wentworth Thompson
% }

%----------------------------------------------------------------------------------------
%	OUTPUT TITLE PAGE AND PREVIOUS
%----------------------------------------------------------------------------------------

% Note that \maketitle outputs the pages before here

% If twoside=false, \uppertitleback and \lowertitleback are not printed
% To overcome this issue, we set twoside=semi just before printing the title pages, and set it back to false just after the title pages
\KOMAoptions{twoside=semi}
\maketitle
\KOMAoptions{twoside=false}

%----------------------------------------------------------------------------------------
%	PREFACE
%----------------------------------------------------------------------------------------

% \chapter*{Preface}
% \addcontentsline{toc}{chapter}{Preface} % Add the preface to the table of contents as a chapter

% I am of the opinion that every \LaTeX\xspace geek, at least once during 
% his life, feels the need to create his or her own class: this is what 
% happened to me and here is the result, which, however, should be seen as 
% a work still in progress. Actually, this class is not completely 
% original, but it is a blend of all the best ideas that I have found in a 
% number of guides, tutorials, blogs and tex.stackexchange.com posts. In 
% particular, the main ideas come from two sources:

% \begin{itemize}
% 	\item \href{https://3d.bk.tudelft.nl/ken/en/}{Ken Arroyo Ohori}'s 
% 	\href{https://3d.bk.tudelft.nl/ken/en/nl/ken/en/2016/04/17/a-1.5-column-layout-in-latex.html}{Doctoral 
% 	Thesis}, which served, with the author's permission, as a backbone 
% 	for the implementation of this class;
% 	\item The 
% 		\href{https://github.com/Tufte-LaTeX/tufte-latex}{Tufte-Latex 
% 			Class}, which was a model for the style.
% \end{itemize}

% The first chapter of this book is introductive and covers the most 
% essential features of the class. Next, there is a bunch of chapters 
% devoted to all the commands and environments that you may use in writing 
% a book; in particular, it will be explained how to add notes, figures 
% and tables, and references. The second part deals with the page layout 
% and design, as well as additional features like coloured boxes and 
% theorem environments.

% I started writing this class as an experiment, and as such it should be 
% regarded. Since it has always been indended for my personal use, it may 
% not be perfect but I find it quite satisfactory for the use I want to 
% make of it. I share this work in the hope that someone might find here 
% the inspiration for writing his or her own class.

% \begin{flushright}
% 	\textit{Federico Marotta}
% \end{flushright}


%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LIST OF FIGURES/TABLES
%----------------------------------------------------------------------------------------

% \begingroup % Local scope for the following commands

% % Define the style for the TOC, LOF, and LOT
% %\setstretch{1} % Uncomment to modify line spacing in the ToC
% %\hypersetup{linkcolor=blue} % Uncomment to set the colour of links in the ToC
% \setlength{\textheight}{23cm} % Manually adjust the height of the ToC pages

% % Turn on compatibility mode for the etoc package
% \etocstandarddisplaystyle % "toc display" as if etoc was not loaded
% \etocstandardlines % toc lines as if etoc was not loaded

% \tableofcontents % Output the table of contents

% \listoffigures % Output the list of figures

% % Comment both of the following lines to have the LOF and the LOT on different pages
% \let\cleardoublepage\bigskip
% \let\clearpage\bigskip

% \listoftables % Output the list of tables

% \endgroup

%----------------------------------------------------------------------------------------
%	MAIN BODY
%----------------------------------------------------------------------------------------

\mainmatter % Denotes the start of the main document content, resets page numbering and uses arabic numbers
\setchapterstyle{kao} % Choose the default chapter heading style


\input{chap01_introduction}

\input{chap02_statistical_models}

\input{chap03_statistical_inference}

\input{chap04_linear_regression}


\setchapterpreamble[u]{\margintoc}
\chapter{Bayesian statistics}
\label{chap:bayesian_statistics}

Let us go back to the statistical problems we considered in Chapter~\ref{chap:statistical_inference}.
In each case, we had a set of decisions $A$, for instance a set of parameter $A = \Theta \subset \R^d$ for estimation problems, a set of binary decisions $A = \{ 0, 1 \}$ for statistial tests, etc..

\section{Elements of decision theory} % (fold)
\label{sec:elements_of_decision_theory}


We have data $X \in E$ where $E$ is the values set of the observations.
Given a statistical procedure $\delta : E \go A$, we \emph{decide} $\delta(X) \in A$.
In order to assess or to quantify a decision, we need a \emph{loss function} $\ell : \Theta \times A \go \R$.
This means that if the true parameter $\theta \in \Theta$ and if we decide $a \in A$ then we incur a loss $\ell(\theta, a)$.
\begin{example}
	For the estimation of a scalar parameter we have $\Theta = \R = A$ and $\ell(\theta, \theta') = (\theta - \theta')$ which corresponds to the quadratic risk introducion in Definitino~\ref{def:quadratic_risk}. 
\end{example}

\begin{example}
	For a statistical test with hypothesis $H_0 : \theta \in \Theta_0$ and $H_1 : \theta \in \Theta_1$ with $\{ \Theta_0, \Theta_1 \}$ a partition of the set of parameters $\Theta$, we can consider the loss satisfying $\ell(\theta, 0) = 0$ if $\theta \in \Theta_0$, $\ell(\theta, 01) = 0$ if $\theta \in \Theta_1$ and $\ell(\theta, 0) = c_0$ if $\theta \in \Theta_1$, $\ell(\theta, 1) = c_1$ if $\theta \in \Theta_0$, where both constants $c_0, c_1 > 0$ allow to tune we importance we given to the Type~I and Type~II errors.
\end{example}

\begin{definition}
	Consider a statistical experiment with data $X \in E$ and a set of parameters $\Theta$, a set $A$ of decisions and a loss function $\ell : \Theta \times A \go \R$. The \emph{risk} of a statistical procedure $\delta : E \go A$ is the function $\Theta \go \R$ given by
	\begin{equation*}
		R(\theta, \delta) = \E_\theta[ \ell(\theta, \delta(X))]
	\end{equation*}
	for any $\theta \in \Theta$.
\end{definition}

\begin{example}
	For estimation with $A = \Theta = \R$ we can choose $\ell(\theta, \theta')$ so that $R(\theta, \delta)$ is the quadratic risk, but we can used $\ell(\theta, \theta') = |\theta - \theta'|^p$ for some $p \geq 1$ as well. For tests we have $R(\theta, \delta) = c_i \P_\theta[\delta(X) = 1 - i]$ if $\theta \in \Theta_i$ for $i \in \{ 0, 1\}$ (which leads to an approach to statistical testing different from what we did in Section~\ref{sec:tests}).
\end{example}


\section{Bayes risk} % (fold)


Given two statistical procedures $\delta, \delta'$ for the same problem of statistical inference, we do not have in general that $R(\theta, \delta) < R(\theta, \delta')$ uniformly for $\theta \in \Theta$.
\todo{give bernoulli or gaussian example, exercice on Stein effect ?}
We we can do instead is to consider an ``averaged risk'': we consider a distribution $\mu$ on $\Theta$ and we use it to integrate the risk over $\Theta$.
This distribution is called the \emph{prior distribution} or simply the \emph{prior}.
\begin{definition}[Bayesian risk]
	The Bayesian risk of a procedure $\delta$ associated to the prior $\mu$ is given by
	\begin{equation*}
		R_B(\mu, \delta) = \int_{\Theta} R(\theta, \delta) \mu(d \theta) = \int_{\Theta} \mu(d \theta) 
		\int_E \ell(\theta, \delta(x)) P_\theta(dx).
	\end{equation*}
\end{definition}
Note that $R_B(\mu, \delta) \leq \max_{\theta \in \Theta} R(\theta, \delta)$ which means that the Bayes risk is always smaller than the minimax risk.
We understand this risk as an average of the risk over $\Theta$ ``weighted'' by the prior $\mu$.
\begin{example}
	For tests, the Bayes risk associated to a prior $\mu$ writes
	\begin{align*}
		R_B(\mu, \delta) &= c_0 \int_{\Theta_0} \P_\theta(\delta(X) = 1) \mu(d \theta) \\
		& \quad + c_1 \int_{\Theta_1} \P_\theta(\delta(X) = 0) \mu(d \theta)
	\end{align*}
	which is a weighted combination of the Type~I and Type~II errors averaged by the prior $\mu$.
\end{example}

But we can understand it differently as well. 
Indeed, we could say instead that the parameter $\theta$ is itself a random variable distributed as $\mu$, that we denote $T$ instead of $\theta$, and that $P_\theta$ is actually the distribution of $X$ conditionally on $T = \theta$. 
In that case we can write
\begin{equation*}
	R_B(\mu, \delta) = \E[ \ell(T, \delta(X))] = \E_Q[ \ell(T, \delta(X))]
\end{equation*}
where the expectation is computed with respect to the joint distribution of $(T, X)$, namely integration with respect to $Q$ defined by
\begin{align*}
	Q(B \times C) &= \P[ T \in B, X \in C] = \int_B \mu(d \theta) \int_C P_\theta(dx) \\
	&= \int_B P_\theta[C] \mu(d \theta).
\end{align*}
We do ``as if'' $\theta$ where random and with distribution $\mu$.
The distribution $P_\theta$ becomes the conditional distribution of $X | T = \theta$, namely
\begin{equation*}
	P_\theta[A] = \E [\ind{A}(X) | T = \theta],
\end{equation*}
or even $P_T[A] = \E [\ind{A}(X) | T]$.

In what follows, we will assume that we have a dominating measure $\lambda$ on $\Theta$ such that $\mu = g \cdot \lambda$ where $g$ is the density of $T$ on $\Theta$ with resoect tp $\lambda$ (often $\lambda$ is the lEbuesgue measure).
We suppose also that thre is a measure $\nu$ on $E$ for which $P_\theta = f(\cdot | \theta) \cdot \nu$ where similarly $\frac{d P_\theta}{d \nu} = f(x | \theta)$. 
We use here the notation $f(\cot | \theta)$ instead of $f_\theta$ to stress that it will correspond to a conditional density.
We need at this point to classify things about conditional distributions and conditional densities.

\section{About conditional distributions and densities} % (fold)
\label{sec:about_conditional_distributions_and_densities}

Let $X$ and $Y$ be random variables on the same probability space and values in sets $\cX$ and $\cY$.
For any integrable function $f(X)$, we can define the conditional expectation $\E [f(X) | Y]$ as the random variable $g(Y)$ (for some measurable function $g$) which is unique almost surely, such that
\begin{equation}
	\E[f(X) h(Y)] = \E[ g(Y) h(Y)]
\end{equation}
for any measurable and bounded function $h$.
The particular value $g(y)$ of this conditional expectation is denoted $\E[f(X) | Y = y]$. 
In particular, we have that
\begin{align*}
	\E[f(X) h(Y) | Y] &= h(Y) \E[ f(X) | Y]
\end{align*}
almost surely and
\begin{align}
	\E [\E[ f(X) | Y]] = \E[f(X)].
\end{align}
Let us suppose now that the joint distribution $\P_{X, Y}$ of $X$ and $Y$ has a density $p(x, y)$ with respect to $\lambda \otimes \nu$ on $\cX \times \cY$.
We can define the marginal distributions of $X$ and $Y$ as
\begin{equation*}
	p_X(x) = \int_{\cY} p(x, y) \mu(dx), \quad
	p_Y(y) = \int_{\cX} p(x, y) \lambda(dy)
\end{equation*}
\todo{bon y'a des $\lambda$ qui est pris avant non ?}
which correpsond to the marginal distributions $\P_X$ and $\P_Y$ of $X$ and $Y$ so that
\begin{align*}
	\E[f(X)] &= \int_{\cX} f(x) P_X(dx) = \int_{\cX} f(x) p_X(x) \lambda(dx) \\ 
	\E[f(Y)] &= \int_{\cY} g(y) P_Y(dx) = \int_{\cY} g(y) p_Y(x) \nu(dy)
\end{align*}
for any $f, g$ such that $f(X)$ and $g(Y)$ are integrabel. \todo{attention $g$ c'est pas deja pris ?}
Let us remark that
\begin{align*}
	P_{X, Y} \big[ \{ y \in \cY : p_Y(y) = 0 \} \big] &= \int_{\cX \times \cY} \ind{p_Y(y) = 0} p(x, y) \lambda(dx) \nu(dy) \\
	&= \int_{y \in \cY : p_Y(y) = 0} \nu(dy) \int_{\cX} p(x, y) \lambda (dx) \\
	&= \int_{y \in \cY : p_Y(y) = 0} p_Y(y) \nu(dy) = 0
\end{align*}
namely $P_{X, Y}[ \{ y \in \cY : p_Y(y) = 0 \}] = 0$.
So, given any density $q$ we can define
\begin{equation*}
	p_{X | Y}(x | y) = \frac{p(x, y)}{p_Y(y)} \ind{p_Y(y) > 0} + k(x) \ind{p_Y(y) = 0},
\end{equation*}
so that any version of $p_{X | Y}$ associated to the choice of $q$ are all equal $P_{X, Y}$-almost surely, since we can check immediatly that $\int_{\cX} p_{X | Y}(x | y) \lambda(dx) = 1$, so that it is a probability density with respect to $\lambda$ on $\cX$.
Moreover, if we define
\begin{equation*}
	g(y) = \int_{\cX} f(x) p_{X | Y}(x | y) \lambda(dx)
\end{equation*}
we have for any measurable bounded function $h$, because of the Fubini theorem and using the fact that $\{y \in \cY : p_Y(y) = 0\}$ is a $P_{X, Y}$ -negligeable set that
\begin{align*}
	\E[ g(Y) h(Y)] &= \int_{\cY} g(y) h(y) p_Y(y) \nu(dy) \\
	&= \int_{\{ y \in \cY : p_Y(y) > 0 \}} h(y) p_Y(y) \nu(dy) \int_{\cX} f(x) \frac{p(x, y)}{p_Y(x)} \\
	&= \int_{\cX \times \cY} f(x) h(y) p(x, y) \lambda(dx) \nu(dy) \lambda (dx)
	&= \E[ f(X) h(Y)]
\end{align*}
warning $g$ is not any function but the function such that $g(Y) = \E[ f(X) | Y]$ almost surely.
which indeed corresponds to the definition ??? of $g(Y) = \E[f(X) | Y)$.

So, we proved that we can compute conditional expectation using the formula
\begin{equation*}
	\E[f(X) | Y] = \int_{\cX} f(x) p_{X | Y}(x | Y) \lambda(dx) = \int_{\cX} f(x) \P_{X | Y} (dx)
\end{equation*}
if we denote $P_{X | Y}$ as the distribution on $\cX$ fo,ction of $Y$ with density $p_{X | Y}(x | Y)$ with respect to $\lambda$. 
We can define in the say way $p_{X | Y}$ and $\P_{X | Y}$ uniquely on the set $\{ y : p_Y(y) > 0 \}$.
As explained above the complement $\{ y : p_Y(y) > 0 \}^\complement$ has mass equal to $0$: this has no iincidence since conditional expectation are themslef uniquely defined up to a negligable set .

We call $\P_{X | Y}$ the conditional distribution of $X$ conditionally on $Y$ and $p_{X | Y}(x | y)$ the density of $X$ ``conditionally on $Y=y$''. We can of couse define $P_{Y | X}$ and $p_{Y | X}$ exacly in the ame xay.
So, we end up with the fact that, by construction of these conditional densities, the density of the joint density $p_{X, Y}$ of $(X, Y)$  satisfies
\begin{equation}
	\label{eq:cond-density-formula}
	p_{X, Y}(x, y) = p_{X | Y}(x | y) p_Y(y) = p_{Y | X}(y | x) p_X(x)
\end{equation}
$P_{X, Y}$-almost surely.

\section{Posterior distribution and Bayes estimator} % (fold)
\label{sec:posterior_distribution_and_bayes_estimator}

We saw in ??? that the joint distribution $Q$ of $(T, X)$ is defined through its marginal distribution $g$ of $T$ and the conditional density $f(x | \theta)$ of $X$ conditionally on $T = \theta$ (hence the notation $f(x | \theta)$), so that
\begin{equation*}
	Q(d \theta, dx) = g(\theta) f(x | \theta) (\lambda \otimes \nu) (d\theta, dx). 	
\end{equation*} 
The marginal density $\bar f$ of $X$ can be obtained by integrating with respect to $\theta$:
\begin{equation*}
	\bar f(x) = \int_{\Theta} f(x | \theta) g(\theta) \lambda(d \theta)
\end{equation*}
and the conditional density of $T | X = x$ can therefore we written as
\begin{equation*}
	g(\theta | x) = \frac{f(x | \theta) g(\theta)}{\bar f(x)} = \frac{f(x | \theta) g(\theta)}{\int_{\Theta} f(x | \theta') g(\theta') \lambda(d \theta')}.
\end{equation*}
If is the density of the conditional distribution denoted $Q_x$ of $T | X = x$.
We simply used here the \emph{Bayes formula} on conditional densities in order to reverse the order of the conditionning: we expressed $T | X$ from $X | T$ since the distribution of $X | T$ is specified by the model we considered

\begin{definition}
	We call $\mu = g \cdot \lambda$ the \emph{prior} distribution we call the conditional distribution $Q_x$of $T | X=x$ the \emph{posterior} distribution or simply \emph{posterior}.
\end{definition}

The Bayesian reasoning is therefore as follows: we choose a prior on $\theta$, and we compute the posterior using the data. 
A nice aspect of this approach is that we can quantify uncertainty right out of the box with such an approach since instead of producing a point estimatpr $\wh \theta_n$ in the \emph{frequentist} approach (what we did in Section???) we produce a full posterior distribution $Q_x[\cdot] = \P[T \in \cdot | X = x]$.

\paragraph{Bayes estimator.} % (fold)

Let us consider the estimation problem where $A = \Theta$ and let us use the Bayes risk as a measure to assess the error of a procedure $\delta : E \go \Theta$.
An optimal Bayesian estimator should minimize the Byaes risk. 
Another beautiful aspect of this Bayesian approach is that the minimizer of the Byaes risk can be made explitic, since using Fubini toghther with thre fact that $f(x | \theta) g(\theta) = g(\theta | x) \bar f(x)$ almost surely, we can rewrite the Bayes risk as follows:
\begin{align*}
	R_B(\mu, \delta) &= \int_{\Theta} \int_E \ell(\theta, \delta(x)) g(\theta | x) \bar f(x) \nu(dx) \lambda(d \theta) \\
	&= \int_E \bar f(x) \nu(dx) \int_{\Theta} \ell(\theta, \delta(x)) g(\theta | x) \lambda(dx) \\
	&=  \int_E \bar f(x) \nu(dx) \int_{\Theta} \ell(\theta, \delta(x)) Q_x(d \theta).
\end{align*}
We is remarkable is that in order to minimize this quantity, we need to minimize for any fixed $x \in E$ the quantity
\begin{equation*}
	\int_{\Theta} \ell(\theta, \delta(x)) Q_x(d \theta) = \E_{Q_x} [\ell(T, \delta(x))] 
	= \E [\ell(T, \delta(X)) | X = x],
\end{equation*}
namely the expectation of the loss with respect to the posterior distribution $Q_x$ of $T | X = x$.
\begin{definition}
	Any estimator $\wh \theta(X)$ (not necessarily unique) defined as 
	\begin{equation*}
		\wh \theta_(x) \in \argmin_{t \in \Theta} \int_{\Theta} \ell(\theta, t) Q_x(d \theta) 
		= \argmin_{t \in \Theta} \E_{Q_x} [\ell(T, t) ],
	\end{equation*}
	namely a minimizer of the loss averaged by the posterior distribution is called a \emph{Bayes} or \emph{Bayesian estimator} associated to the prior $\mu = g \cdot \lambda$ and to the loss $\ell$.
\end{definition}

WHenever $\ell(\theta, \theta') = (\theta - \theta')^2$ then
\begin{equation*}
	\argmin_{t \in \R} \E_{Q_x}[ (T - t)^2] = \E_{Q_x}[T]
\end{equation*}
namely the \emph{Bayes estimator with the quadratic loss is the expectation of the posterior distribution.}
If $\ell(\theta, \theta') = |\theta - \theta'|$ then 
\begin{equation*}
	\argmin_{t \in \R} \E_{Q_x}[ |(T - t| ] = \med(Q_x) = F_{Q_x}^{-1}(1/2),
\end{equation*}
namely the Bayes estimator associated to the $\ell_1$ loss is given by the median of the posterior distribution. \todo{exo proof etc}

\begin{recipe}
	On simple examples, we can compute explicitly the Bayes estimator. Given the data density $f(x | \theta)$ and  the prior density $g$, we simply wrte the joint distribution of $(T, X)$ and use the fact that the posterior density is a density, namely it must integrate with respect to $\theta$ to $1$:
	\begin{equation*}
		g(\theta | x) = \mathrm{constant}(x) f(x | \theta) g(\theta)
	\end{equation*}
	where $\mathrm{constant}(x) =  1 / \int_\Theta f(x | \theta) g(\theta) \lambda(d \theta)$, so that we can identify the posterior distribution with a carefull look, and ideally some coffee at the formula fpr $f(x | \theta) g(\theta)$ and identify a density with respect to $\theta$.
\end{recipe}

Let us give some standard examples of prior and data distribution wuth that the prior can be made epxlicit .


Consider the data distribution $X \sim \bin(n, \theta)$ and with prior $\uni([0, 1])$ on $\theta$. This means that $X | T = \theta$ has density
\begin{equation*}
	f(x | \theta) = \binom{n}{x} \theta^x (1 - \theta^{n-x} \ind{\{ 0, \ldots, n\}}(x)	
\end{equation*}
with respect to the counting measure $\nu$ on $\N$ and that the prior distribution has density $g(\theta) = \ind{[0, 1]}(\theta)$ wuth respect to the Lebuesgye measure $\lambda$ on $\R$, so that the joint distribition of $(T, X)$ has density
\begin{equation*}
	f(\theta, x) = \binom{n}{x} \theta^x (1 - \theta)^{n - x} \ind{[0, 1]}(\theta) 
	\ind{\{ 0, \ldots, n\}}(x)
\end{equation*}
with respect to the product measure $\lambda \otimes \nu$. THe posterior distribution, namely the distriution of $T | X=x$ is therefore proportional to $\mapsto \theta^x (1 - \theta)^{n - x} \ind{[0, 1]}(\theta)$. We recognize the $\bet(a, b)$ distribution ???, that as density
\begin{equation*}
	\frac{1}{\beta(a, b)} t^{a-1} (1 - t)^{b-1} \ind{[0, 1]}(t)
\end{equation*}
where we recall that $\beta(a, b) = \int_0^1 t^{a-1} (1 - t)^{b-1} d t = \Gamma(a) \Gamma(b)  / \Gamma(a + b)$.
Therefore, we have that the posterior distribution is given by
\begin{equation*}
	Q_x = \bet(x + 1, n - x + 1).
\end{equation*}
Also, we have that
\begin{equation*}
	\E[Z^k] = \frac{\beta(a + k, b)}{\beta(a, b)} = \frac{\Gamma(a + k) \Gamma(a + b)}{\Gamma(a + k + b) \Gamma(a)} = \frac{a (a + 1) \cdots (a + k -1)}{(a + b) (a + b + 1) \cdots (a + k + b - 1)}
\end{equation*}
\todo{cjeck formula}. In aprticualr we get whenver $B \sim \bet(a, b)$ that
\begin{equation*}
	\E[Z] = \frac{a}{a + b} \quad \text{and} \quad \var[Z] = \frac{ab}{(a + b)^2 (a + b + 1)}.
\end{equation*}
So, using ???, the Bayes estimator of $\theta$ for the quadratic loss is given by 
\begin{equation*}
	\E_{Q_x}[T | X=x] = \E[ \bet(x+1, n - x + 1)] = \frac{x + 1}{n + 2},
\end{equation*}
\todo{formula c'est ok comem ca?}
namely
\begin{equation*}
	\wh \theta_n^B = \frac{X + 1}{n + 2},
\end{equation*}
which is an estimator different from $\wh \theta_n = X / n$, the one we use in Section~???.
The quadratic risk of $\wh \theta_n^B$ is, using the bias-variance formula from ??? given by
\begin{align*}
	\E_\theta[ (\wh \theta_n^B - \theta)^2] &= \var_\theta[\wh \theta_n^B] + (\E_\theta[\wh \theta_n^B] - \theta)^2 \\
	&= \frac{n \theta(1 - \theta)}{(n + 2)^2} + \Big( \frac{1 - 2 \theta}{n + 2} \Big)^2 = \frac{(1 - 2 \theta)^2 + n \theta (1 - \theta)}{(n+2)^2}
\end{align*}
and the Bayes ris can be computed as 
\begin{equation*}
	\int_0^1 \Big( \frac{1 - 2 \theta}{n + 2} \Big)^2 = \frac{(1 - 2 \theta)^2 + n \theta (1 - \theta)}{(n+2)^2} d \theta = ???
\end{equation*}
\todo{une facton plus direct de le calculer ?}


Followingn Example ? it is easy to see that if the prior is $\beta(a, b)$ and the data distribution
\todo{define clearly data distributin } si $\bin(n, \theta)$ then the posterio distribution is $\bet(a + x - 1, b + n - x + 1)$. Note that for this example the prior and posterior belong to the same family of $\bet$ distributions. In such a case, we say the the $\bin$ and $\bet$ distributions are conjuguate distributions: computations cna be made explicit in such case.
Note that however, this is not often the case and BLALBA
The more general case is the Dirichlet / Multinomial distributions, that we leave as an exercice.

\todo{dire aussi qu'on peut noter abusiement $\theta \sim ???$}
Another classical example is with the Gaussian distribution.
Consider data $X_1, \ldots, X_n$ iid with distribution $\nor(\theta, \sigma^2)$ and prior $\theta \sim \cN(0, \tau^2)$.
Let us find out the posterior distribution in this case
\begin{align*}
	f_{X | T}(x | \theta) g(\theta) &= c(\sigma) \exp\Big( - \frac{1}{2 \sigma^2} \sum_{i=1}^n (x_i - \mu)^2 - \frac{\mu}{\tau^2} \Big) \\
	&= c(\sigma) \exp \Big (  -\Big( \frac{1}{2 \gamma} \mu^2 + \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i + c(x_1, \ldots, x_n) \Big)  \Big ) \\
	&= c(\sigma) \exp \Big( -\Big( \frac{1}{2 \gamma} \Big( \mu -  \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i 
	\Big)^2 + c(x_1, \ldots, x_n) \Big)  \Big )
\end{align*}
where we put $\gamma = 1 / (n / \sigma^2 + 1 / \tau^2) = \sigma^2 / (n + \sigma^2 / \tau^2)$ and where $c$ stands for uninteresing constants which entails that the posterior distribution is
\begin{equation*}
	\nor\Big( \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i, \sigma^2 / (n + \sigma^2 / \tau^2) \Big)
\end{equation*}
so that the Bayes estimator for the quadratic risk is given by 
\begin{equation*}
	\wh \theta_n^B = \frac{1}{n + \sigma^2 / \tau^2} \sum_{i=1}^n X_i.
\end{equation*}
This entails also that the Gussian distribuion is conjuguated to itself.

Another very interesting example is the Gaussian linear regression model we considered in Chapter ??? where
$Y_i = X_i^\top \theta + \eps_i$ with deterministic $X_i \in R^d$ (or everythin is doe conditionally on them) and $\eps_i \sim \nor(0, \sigma^2)$ and iid.
In the Gaussian linear regression setting, we have that $\by \sim \nor(\bX \theta, \sigma^2 \bI)$ where we recall that $\by$ $\bX$ are given by ????.
We consider this in a Bayesian setting by assuming that $\by | T = \theta = \nor(\bX \theta, \sigma^2 \bI)$ and by considering the prior distribution $\theta \sim \nor(0, \frac{1}{\lambda} \bI_d)$.
The joint distribution of $(\by, T)$ is, therefor, given by
\begin{equation*}
	f_{\by | T = \theta}(y | \theta) g(\theta) = \frac{1}{(\sigma \sqrt{2 \pi})^{n}} 
	\exp \Big( -\frac{1}{2 \sigma^2} \norm{\by - \bX \theta}^2 - \frac {\lambda}{2} \norm{\theta}^2 \Big).
\end{equation*}
What is, in this setting, the posterior distribution $\P_{\theta | \by}$ ?
\todo{ en fait c'est chiant, on a vraiment envie de simplifier ces putain de notations de merde}
This is somewhat more complicated then what we did in both previous examples, and we need the following theorem about the multivatiate Gaussian distribution to handle this example.
\begin{theorem}
	Consider two matrices $\bLambda \succ 0$ and $\bL \succ 0$ and consider 
	$X$ such that $\P_X = \nor(\mu, \bLambda^{-1})$ and $Y$ such that $\P_{Y | X = x} = \nor(\bA x + b, \bL^{-1})$. Then, we have the following:
	\begin{equation*}
		\P_Y \sim \nor( \bA \mu + b, \bL^{-1} + \bA \bLambda^{-1} \bA^\top)		
	\end{equation*}
	and
	\begin{equation*}
		\P_{X | Y = y} = \nor(\bSigma (\bA^\top \bL (y - b) + \bLambda \mu), \bSigma)
	\end{equation*}
	where $\bSigma = (\bLambda + \bA^\top \bL \bA)^{-1}$.
\end{theorem}
The proof of this results can be found in ???? Bishop ??? dire endroit exact.
This is a computational proves that makes heavy use of ???
 The proof is given in ???
In the case that interests us we have $\mu = 0$, $\bLambda^{-1} = \bI_d / \lambda$, $\bLambda = \lambda \bI_d$, $\bL^{-1} = \sigma^2 \bI_n$, $\bA = \bX$ and $b = 0$ so that
\begin{equation*}
	\bSigma = \Big( \lambda \bI_d + \frac{1}{\sigma^2} \bX^\top \bX \Big)^{-1} = \sigma^2 \Big( \bX^\top \bX  + \lambda \sigma^2 \Big)
\end{equation*}
so that the posterior is given by
\begin{equation*}
	\P_{\theta | \by} = \nor \Big( (\bX^\top \bX  + \lambda \sigma^2 \bI_d)^{-1} \bX^\top \by,
	\sigma^2 (\bX^\top \bX + \lambda \sigma^2 \bI_d)^{-1} \Big)
	 \Big)
\end{equation*}
and the Bayes estimator for the quaradtic risk writes
\begin{equation*}
	\wh \theta_n^B = (\bX^\top \bX  + \lambda \sigma^2 \bI_d)^{-1} \bX^\top \by.
\end{equation*}
In this example, the Bayes estimator coincides with the so-called MAP estimator (maximum a posterior), which is given by, when it exists, the mode of the posterior, since the Gaussian distribution is symmetrical.
Indeed, the posterior distribugion is propoertional to
\begin{equation*}
	\exp \Big( -\frac{1}{2 \sigma^2} \Big)
\end{equation*}

% paragraph paragraph_name (end)


% section posterior_distribution_and_bayes_estimator (end)


% section bayes_risk (end)
%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

% The bibliography needs to be compiled with biber using your LaTeX editor, or on the command line with 'biber main' from the template directory

% \defbibnote{bibnote}{Here are the references in citation order.\par\bigskip} % Prepend this text to the bibliography
\printbibliography[heading=bibintoc, title=Bibliography] % Add the bibliography heading to the ToC, set the title of the bibliography and output the bibliography note

%----------------------------------------------------------------------------------------
%	NOMENCLATURE
%----------------------------------------------------------------------------------------

% The nomenclature needs to be compiled on the command line with 'makeindex main.nlo -s nomencl.ist -o main.nls' from the template directory

% \nomenclature{$c$}{Speed of light in a vacuum inertial frame}
% \nomenclature{$h$}{Planck constant}

% \renewcommand{\nomname}{Notation} % Rename the default 'Nomenclature'
% \renewcommand{\nompreamble}{The next list describes several symbols that will be later used within the body of the document.} % Prepend this text to the nomenclature

% \printnomenclature % Output the nomenclature

%----------------------------------------------------------------------------------------
%	GREEK ALPHABET
% 	Originally from https://gitlab.com/jim.hefferon/linear-algebra
%----------------------------------------------------------------------------------------

% \vspace{1cm}

% {\usekomafont{chapter}Greek Letters with Pronounciation} \\[2ex]
% \begin{center}
% 	\newcommand{\pronounced}[1]{\hspace*{.2em}\small\textit{#1}}
% 	\begin{tabular}{l l @{\hspace*{3em}} l l}
% 		\toprule
% 		Character & Name & Character & Name \\ 
% 		\midrule
% 		$\alpha$ & alpha \pronounced{AL-fuh} & $\nu$ & nu \pronounced{NEW} \\
% 		$\beta$ & beta \pronounced{BAY-tuh} & $\xi$, $\Xi$ & xi \pronounced{KSIGH} \\ 
% 		$\gamma$, $\Gamma$ & gamma \pronounced{GAM-muh} & o & omicron \pronounced{OM-uh-CRON} \\
% 		$\delta$, $\Delta$ & delta \pronounced{DEL-tuh} & $\pi$, $\Pi$ & pi \pronounced{PIE} \\
% 		$\epsilon$ & epsilon \pronounced{EP-suh-lon} & $\rho$ & rho \pronounced{ROW} \\
% 		$\zeta$ & zeta \pronounced{ZAY-tuh} & $\sigma$, $\Sigma$ & sigma \pronounced{SIG-muh} \\
% 		$\eta$ & eta \pronounced{AY-tuh} & $\tau$ & tau \pronounced{TOW (as in cow)} \\
% 		$\theta$, $\Theta$ & theta \pronounced{THAY-tuh} & $\upsilon$, $\Upsilon$ & upsilon \pronounced{OOP-suh-LON} \\
% 		$\iota$ & iota \pronounced{eye-OH-tuh} & $\phi$, $\Phi$ & phi \pronounced{FEE, or FI (as in hi)} \\
% 		$\kappa$ & kappa \pronounced{KAP-uh} & $\chi$ & chi \pronounced{KI (as in hi)} \\
% 		$\lambda$, $\Lambda$ & lambda \pronounced{LAM-duh} & $\psi$, $\Psi$ & psi \pronounced{SIGH, or PSIGH} \\
% 		$\mu$ & mu \pronounced{MEW} & $\omega$, $\Omega$ & omega \pronounced{oh-MAY-guh} \\
% 		\bottomrule
% 	\end{tabular} \\[1.5ex]
% 	Capitals shown are the ones that differ from Roman capitals.
% \end{center}

%----------------------------------------------------------------------------------------
%	GLOSSARY
%----------------------------------------------------------------------------------------

% The glossary needs to be compiled on the command line with 'makeglossaries main' from the template directory

% \newglossaryentry{computer}{
% 	name=computer,
% 	description={is a programmable machine that receives input, stores and manipulates data, and provides output in a useful format}
% }

% Glossary entries (used in text with e.g. \acrfull{fpsLabel} or \acrshort{fpsLabel})
% \newacronym[longplural={Frames per Second}]{fpsLabel}{FPS}{Frame per Second}
% \newacronym[longplural={Tables of Contents}]{tocLabel}{TOC}{Table of Contents}

% \setglossarystyle{listgroup} % Set the style of the glossary (see https://en.wikibooks.org/wiki/LaTeX/Glossary for a reference)
% \printglossary[title=Special Terms, toctitle=List of Terms] % Output the glossary, 'title' is the chapter heading for the glossary, toctitle is the table of contents heading

%----------------------------------------------------------------------------------------
%	INDEX
%----------------------------------------------------------------------------------------

% The index needs to be compiled on the command line with 'makeindex main' from the template directory

% \printindex % Output the index

%----------------------------------------------------------------------------------------
%	BACK COVER
%----------------------------------------------------------------------------------------

% If you have a PDF/image file that you want to use as a back cover, uncomment the following lines

%\clearpage


\end{document}


