%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% kaobook
% LaTeX Template
% Version 1.2 (4/1/2020)
%
% This template originates from:
% https://www.LaTeXTemplates.com
%
% For the latest template development version and to make contributions:
% https://github.com/fmarotta/kaobook
%
% Authors:
% Federico Marotta (federicomarotta@mail.com)
% Based on the doctoral thesis of Ken Arroyo Ohori (https://3d.bk.tudelft.nl/ken/en)
% and on the Tufte-LaTeX class.
% Modified for LaTeX Templates by Vel (vel@latextemplates.com)
%
% License:
% CC0 1.0 Universal (see included MANIFEST.md file)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
	fontsize=11pt, % Base font size
	twoside=false, % Use different layouts for even and odd pages (in particular, if twoside=true, the margin column will be always on the outside)
	%open=any, % If twoside=true, uncomment this to force new chapters to start on any page, not only on right (odd) pages
	%chapterprefix=true, % Uncomment to use the word "Chapter" before chapter numbers everywhere they appear
	%chapterentrydots=true, % Uncomment to output dots from the chapter name to the page number in the table of contents
	numbers=noenddot, % Comment to output dots after chapter numbers; the most common values for this option are: enddot, noenddot and auto (see the KOMAScript documentation for an in-depth explanation)
	%draft=true, % If uncommented, rulers will be added in the header and footer
	%overfullrule=true, % If uncommented, overly long lines will be marked by a black box; useful for correcting spacing problems
]{kaobook}

% Set the language
\usepackage[english]{babel} % Load characters and hyphenation
\usepackage[english=british]{csquotes} % English quotes

% Load packages for testing
\usepackage{blindtext}
%\usepackage{showframe} % Uncomment to show boxes around the text area, margin, header and footer
%\usepackage{showlabels} % Uncomment to output the content of \label commands to the document where they are used

% Load the bibliography package
\usepackage{styles/kaobiblio}
\addbibresource{main.bib} % Bibliography file

% Load mathematical packages for theorems and related environments. NOTE: choose only one between 'mdftheorems' and 'plaintheorems'.
\usepackage{styles/mdftheorems}
%\usepackage{styles/plaintheorems}


\graphicspath{{images/}} % Paths in which to look for images

% \RequirePackage{times}

\DeclareMathOperator{\cA}{\mathcal A}
\DeclareMathOperator{\cB}{\mathcal B}
\DeclareMathOperator{\cC}{\mathcal C}
\DeclareMathOperator{\cD}{\mathcal D}
\DeclareMathOperator{\cE}{\mathcal E}
\DeclareMathOperator{\cF}{\mathcal F}
\DeclareMathOperator{\cG}{\mathcal G}
\DeclareMathOperator{\cM}{\mathcal M}
\DeclareMathOperator{\cN}{\mathcal N}
\DeclareMathOperator{\cP}{\mathcal P}

\DeclareMathOperator{\bA}{\boldsymbol A}
\DeclareMathOperator{\bB}{\boldsymbol B}
\DeclareMathOperator{\bI}{\boldsymbol I}
\DeclareMathOperator{\bX}{\boldsymbol X}
\DeclareMathOperator{\by}{\boldsymbol y}
\DeclareMathOperator{\bO}{\boldsymbol O}
\DeclareMathOperator{\beps}{\boldsymbol \varepsilon}

\DeclareMathOperator{\bSigma}{\boldsymbol \Sigma}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\nor}{Normal}
\DeclareMathOperator{\ber}{Bernoulli}
\DeclareMathOperator{\bin}{Binomial}
\DeclareMathOperator{\mul}{Multinomial}
\DeclareMathOperator{\expo}{Exponential}
\DeclareMathOperator{\uni}{Uniform}

\DeclareMathOperator{\sigmoid}{sigmoid}

\DeclareMathOperator{\leb}{Lebesgue}

\DeclareMathOperator*{\argmin}{argmin}
\renewcommand{\span}{\mathrm{span}}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\cov}{cov}

\newcommand{\eps}{\varepsilon}

\renewcommand{\P}{\mathbb P}
\newcommand{\E}{\mathbb E}
\newcommand{\R}{\mathbb R}
\newcommand{\var}{\mathbb V}

\newcommand{\wh}{\widehat}

\newcommand{\ind}[1]{\mathbf 1_{#1}}
\newcommand{\grad}{\nabla}


\newcommand{\mgeq}{\succcurlyeq}
\newcommand{\mleq}{\preccurlyeq}
\newcommand{\goes}{\rightarrow}
\newcommand{\go}{\rightarrow}

\newcommand{\norm}[1]{\|#1\|}
\newcommand{\inr}[1]{\langle #1 \rangle}

\newcommand{\gopro}{\overset{\P}{\rightarrow}}
\newcommand{\goas}{\overset{\text{as\ }}{\rightarrow}}
\newcommand{\goqr}{\overset{\text{$L^2$\ }}{\rightarrow}}
\newcommand{\gosto}{\leadsto}


% \newcommand{\lest}{ \underset{\text{st}}{\leq}}
\newcommand{\lest}{\preceq}
% \newcommand{\gest}{\underset{\text{st}}{\qeq}}
\newcommand{\gest}{\succeq}



% \makeindex[columns=3, title=Alphabetical Index, intoc] % Make LaTeX produce the files required to compile the index

% \makeglossaries % Make LaTeX produce the files required to compile the glossary

% \makenomenclature % Make LaTeX produce the files required to compile the nomenclature

% Reset sidenote counter at chapters
%\counterwithin*{sidenote}{chapter}

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	BOOK INFORMATION
%----------------------------------------------------------------------------------------

\titlehead{Some stuff about statistics}
\subject{Lecture notes for the ENS course of Statistics}

\title[Some stuff about Statistics]{Some stuff about Statistics}
% \subtitle{Customise this page according to your needs}

\author[St\'ephane Ga\"iffas"]{St\'ephane Ga\"iffas\thanks{}}

\date{\today}

\publishers{}

%----------------------------------------------------------------------------------------

\frontmatter % Denotes the start of the pre-document content, uses roman numerals

%----------------------------------------------------------------------------------------
%	OPENING PAGE
%----------------------------------------------------------------------------------------

%\makeatletter
%\extratitle{
%	% In the title page, the title is vspaced by 9.5\baselineskip
%	\vspace*{9\baselineskip}
%	\vspace*{\parskip}
%	\begin{center}
%		% In the title page, \huge is set after the komafont for title
%		\usekomafont{title}\huge\@title
%	\end{center}
%}
%\makeatother

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

% \makeatletter
% \uppertitleback{\@titlehead} % Header

% \lowertitleback{
% 	\textbf{Disclaimer}\\
% 	You can edit this page to suit your needs. For instance, here we have a no copyright statement, a colophon and some other information. This page is based on the corresponding page of Ken Arroyo Ohori's thesis, with minimal changes.
	
% 	\medskip
	
% 	\textbf{No copyright}\\
% 	\cczero\ This book is released into the public domain using the CC0 code. To the extent possible under law, I waive all copyright and related or neighbouring rights to this work.
	
% 	To view a copy of the CC0 code, visit: \\\url{http://creativecommons.org/publicdomain/zero/1.0/}
	
% 	\medskip
	
% 	\textbf{Colophon} \\
% 	This document was typeset with the help of \href{https://sourceforge.net/projects/koma-script/}{\KOMAScript} and \href{https://www.latex-project.org/}{\LaTeX} using the \href{https://github.com/fmarotta/kaobook/}{kaobook} class.
	
% 	The source code of this book is available at:\\\url{https://github.com/fmarotta/kaobook}
	
% 	(You are welcome to contribute!)
	
% 	\medskip
	
% 	\textbf{Publisher} \\
% 	First printed in May 2019 by \@publishers
% }
% \makeatother

%----------------------------------------------------------------------------------------
%	DEDICATION
%----------------------------------------------------------------------------------------

% \dedication{
% 	The harmony of the world is made manifest in Form and Number, and the heart and soul and all the poetry of Natural Philosophy are embodied in the concept of mathematical beauty.\\
% 	\flushright -- D'Arcy Wentworth Thompson
% }

%----------------------------------------------------------------------------------------
%	OUTPUT TITLE PAGE AND PREVIOUS
%----------------------------------------------------------------------------------------

% Note that \maketitle outputs the pages before here

% If twoside=false, \uppertitleback and \lowertitleback are not printed
% To overcome this issue, we set twoside=semi just before printing the title pages, and set it back to false just after the title pages
\KOMAoptions{twoside=semi}
\maketitle
\KOMAoptions{twoside=false}

%----------------------------------------------------------------------------------------
%	PREFACE
%----------------------------------------------------------------------------------------

% \chapter*{Preface}
% \addcontentsline{toc}{chapter}{Preface} % Add the preface to the table of contents as a chapter

% I am of the opinion that every \LaTeX\xspace geek, at least once during 
% his life, feels the need to create his or her own class: this is what 
% happened to me and here is the result, which, however, should be seen as 
% a work still in progress. Actually, this class is not completely 
% original, but it is a blend of all the best ideas that I have found in a 
% number of guides, tutorials, blogs and tex.stackexchange.com posts. In 
% particular, the main ideas come from two sources:

% \begin{itemize}
% 	\item \href{https://3d.bk.tudelft.nl/ken/en/}{Ken Arroyo Ohori}'s 
% 	\href{https://3d.bk.tudelft.nl/ken/en/nl/ken/en/2016/04/17/a-1.5-column-layout-in-latex.html}{Doctoral 
% 	Thesis}, which served, with the author's permission, as a backbone 
% 	for the implementation of this class;
% 	\item The 
% 		\href{https://github.com/Tufte-LaTeX/tufte-latex}{Tufte-Latex 
% 			Class}, which was a model for the style.
% \end{itemize}

% The first chapter of this book is introductive and covers the most 
% essential features of the class. Next, there is a bunch of chapters 
% devoted to all the commands and environments that you may use in writing 
% a book; in particular, it will be explained how to add notes, figures 
% and tables, and references. The second part deals with the page layout 
% and design, as well as additional features like coloured boxes and 
% theorem environments.

% I started writing this class as an experiment, and as such it should be 
% regarded. Since it has always been indended for my personal use, it may 
% not be perfect but I find it quite satisfactory for the use I want to 
% make of it. I share this work in the hope that someone might find here 
% the inspiration for writing his or her own class.

% \begin{flushright}
% 	\textit{Federico Marotta}
% \end{flushright}


%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LIST OF FIGURES/TABLES
%----------------------------------------------------------------------------------------

% \begingroup % Local scope for the following commands

% % Define the style for the TOC, LOF, and LOT
% %\setstretch{1} % Uncomment to modify line spacing in the ToC
% %\hypersetup{linkcolor=blue} % Uncomment to set the colour of links in the ToC
% \setlength{\textheight}{23cm} % Manually adjust the height of the ToC pages

% % Turn on compatibility mode for the etoc package
% \etocstandarddisplaystyle % "toc display" as if etoc was not loaded
% \etocstandardlines % toc lines as if etoc was not loaded

% \tableofcontents % Output the table of contents

% \listoffigures % Output the list of figures

% % Comment both of the following lines to have the LOF and the LOT on different pages
% \let\cleardoublepage\bigskip
% \let\clearpage\bigskip

% \listoftables % Output the list of tables

% \endgroup

%----------------------------------------------------------------------------------------
%	MAIN BODY
%----------------------------------------------------------------------------------------

\mainmatter % Denotes the start of the main document content, resets page numbering and uses arabic numbers
\setchapterstyle{kao} % Choose the default chapter heading style


% \input{chap01_introduction}

% \input{chap02_statistical_models}

% \input{chap03_statistical_inference}


\setchapterpreamble[u]{\margintoc}
\chapter{Linear regression}
\label{chap:linear_regression}

We observe iid pairs $(X_1, Y_1), \ldots, (X_n, Y_n)$ where $X_i \in \R^d$ and $Y_i \in \R$.
We want to \emph{predict} $Y_i$ from $X_i$.
Actually, we want to study the conditional distribution $Y_1 | X_1$, namely we want to \emph{regress} $Y_i$ on $X_i$.
We know that the closest $X_1$-measurable function from $Y_1$ is the conditional expectation $\E (Y | X) = f(X)$  in $L^2$, but we do not know the joint distrivbution of $(X_1, Y_1)$
So we want to use the observations $(X_i, Y_i)$ in order to build some approximation of $f$.

On the other hand, wat kind of functions could we consider ?
As a start, we should consider the simplest non-constant function $\R^d \go \R$ we can think of, which is naturally a \emph{linear} function $f(x) = x^\top \theta + c$.

\begin{kaobox}[frametitle=Features engineering]
	Considering only linear function is of course very limiting. But let us stress that, in practice, we can do whatever we want with the data $(X_i, Y_i)$. A linear model is typically \emph{trained} on \emph{mappings} of $X_i$, that can include non-linear mappings such as a polynomial mapping, pairwise products leading to $d(d-1)/2$ extra coordinates, etc. The construction of such a \emph{features mapping} is called \emph{feature engineering} in statistical and machine learning, and is more an art than a science. Note that many industrial large scale problems (such as web-display advertisment) are handled using such linear methods, but on highly tested and engineered features. We won't discuss this in this chapter, and will assume that $X_i$ are well crafted features vectors on which we want to train a linear method.	
	This makes the model linear we respect to $\phi(X_i)$ but certainly not with respect to $X_i$.
	We forget about it and work on $X_i$ assuming that...
\end{kaobox}

Also, to simplify notations we will forget about the \emph{intercept}, also called \emph{population bias} $b \in \R$ since we can, without loss of generality simply replace $\theta$ by $[1 \; \theta^\top]^\top$ and $X_i$ by $[1 \; X_i^\top]^\top$ (by $d$ by $d=1$).

We assume that $\E(Y_i | X_i)$ can reasonably well approximated by $\theta^\top X_i$ for some $\theta \in \R^d$ to be trained and 

We therefore assume that we can write $Y_i = \theta^\top X_i + \eps_i$ where $\eps_i = Y_i - \E(Y_i | X_i)$ and $\E(\eps_i | X_i) = 0$.

\todo{dire que c'est une definition ce modele lineaire et qu'est ce que l'hypothse}


\section{Least squares estimator} % (fold)
\label{sec:least_squares_estimator}

We say that iid real random variables $\eps_1, \ldots, \eps_n$ is the \emph{noise} of the linear model $Y_i = X_i^\top \theta + \eps_i$.
How can we \emph{estimate} or \emph{train} $\theta \in \R^d$ ?
A natural idea is to find $\wh \theta_n \in \R^d$ such that $X_i^\top \wh \theta_n$ is close to $Y_i$ for each $i=1, \ldots, n$. The simplest way to measure this closeness is to use the Euclidean distance on $\R^n$.
Let us first introduce the following notations:
\begin{equation*}
	\by = \begin{bmatrix}
		Y_1 \\
		\vdots \\
		Y_n
	\end{bmatrix}
	\quad \text{and} \quad
	\bX = 
	\begin{bmatrix}
		X_{1, 1} & \cdots & X_{1, d} \\
		\vdots & \ddots & \vdots \\
		X_{n, 1} & \cdots & X_{n, d}
	\end{bmatrix}	
	=
	\begin{bmatrix}
		X_1^\top \\
		\vdots \\
		X_n^\top 
	\end{bmatrix}
	= 
	\begin{bmatrix}
		X^1 & \cdots & X^d
	\end{bmatrix}	
	\in \R^{n \times d}
\end{equation*}
and we will denote also $\eps = [\eps_1 \cdots \eps_n]^\top \in \R^n$ the noise vector 

There notation $X^j$ for the $j$-th columns of $\bX$ and $X_i$ is the $i$-th row of $\bX$.

\sidenote{All vectors are written as column matrices BLABLA The norm $\norm{\cdot}$ stands for the Euclidean norm}

A \emph{least squares} estimator or \emph{ordinary least squares} is defined as
\begin{equation}
	\label{eq:least-squares-estimator}
	\wh \theta_n \in \argmin_{\theta \in \R^d} \norm{\by - \bX \theta}^2 = \argmin_{\theta \in \R^d} \sum_{i=1}^n (Y_i - X_i^\top \theta)^2
\end{equation}
How can we compute $\wh \theta_n$ ?
The definition of $\wh \theta_n$ given by Equation~\eqref{eq:least-squares-estimator} entails that
\begin{equation*}
	\bX \wh \theta_n = \proj_V (Y)
\end{equation*}
where $\proj_V$ is the orthogonal projection operator onto $V = \{ \bX u : u \in \R^d \}Â = \span(\bX) = \span(X^1, \ldots, X^d)$, the linear space in $\R^n$ spanned by the columns of $\bX$.
\todo{insert picture}
We need to have that $Y - \bX \wh \theta_n \perp V$, namely $\inr{\bX u, Y - \bX \wh \theta_n} = 0$ for any $u \in \R^d$, namely $u^\top \bX^\top (Y - \bX \wh \theta_n) = 0$ for any $u$ which leads to the so-called \emph{normal equation}
\begin{equation}
	\label{eq:least-squares-normal-equation}
	\bX^\top \bX \wh \theta_n = \bX^\top \by.
\end{equation}
This means that $\wh \theta_n$ is a solution to this normal equation, which in turn is simply a linear system.
Another approach that leads to the same result is to put $F(\theta) = \norm{Y - \bX \theta}^2$, which is a \emph{convex} and differentiable function on $\R^d$, so that $\wh \theta_n$ is an $\argmin$ if and only if it is a critical point, namely $\grad F(\wh \theta_n) = 0$, but $\grad F(\theta) = 2 \bX^\top (\bX \theta - \theta)$ so that we end up with the same normal Equation~\eqref{eq:least-squares-normal-equation}.

A least squares estimator is therefore solution tothe linear system???

From now on, let us assume that $\E \norm{X}^2 < +\infty$ and $\E(Y^2) < +\infty$.
This allows to define the $d \times d$ matrix
\begin{equation}
	\E(X X^\top) = (\E(X_j X_k))_{1 \leq j, k \leq d}
\end{equation}
and let us recall that the covariance matrix between two random vectors $U$ and $V$ is, whenever it makes sense, given by
\begin{equation*}
	\cov(U, V) = \E [(U - \E U)(V - \E V)^\top]
\end{equation*}
\todo{les matrices avec du gras !!!}
and we will denote $\var (U) = \cov(U, U)$ the covariance matrix of $U$.
Let us also remark that whenever $V = A U + b$ for some deterministic matrix $A$ and vector $b$ we have that $\E [V] = A \E [U] + b$ and $\var[V] = A \var(U) A^\top$.
Also, we will write $\bA \succ 0$ to say that a matrix $\bA$ is positive definite.
\begin{theorem}
	\label{thm:least-squares-existence}
	Let $n \geq d$. The following points about $\P_X$ are all equivalent whenever $X_1, \ldots, X_n$ are independent.
	\begin{enumerate}
		\item $\E(X X^\top) \succ 0$
		\item For any hyperplane $H \subset \R^d$ we have $\P(X \in H) = 0$, namely $\P(X^\top \theta = 0) = 0$ for any $\theta \in S^{d-1}$
		\item $\bX^\top \bX = \sum_{i=1}^n X_i X_i^\top \succ 0$ almost surely
		\item The least squares estimator is uniquely defined and given by
		\begin{equation*}
			\wh \theta_n = (\bX \bX)^{-1} \bX^\top \by
		\end{equation*}
		almost surely.
	\end{enumerate}
	Whenever $\P_X$ satisfies either of this points, we say that $\P_X$ is \emph{non-degenerate}.
\end{theorem}
Theorem~\ref{thm:least-squares-existence} explains exaclty when the least squares estimatpr is 

\begin{proof}
	Point~(3) $\Leftrightarrow$ Point~(4) is obvious since $\bX^\top \bX \succ 0$ entails that $\bX^\top \bX$ is invertible. Point~(1) $\leftrightarrow$ Point~(2) is obvious as well, since $0 = \E(X X^\top) u = 0$ entails $0 = u^\top \E(X X^\top) u = \E[ (X^\top u)^2] = 0$ which entails $X^\top u = 0$ almost surely. Point~(3) $\Rightarrow$ Point~(2) comes from a proof by contradiction. If $0 < p = \P(X^\top u = 0)$ then $X_i^\top u = 0$ for all $i=1, \ldots, n$ with a probability $p^n > 0$ since $X_1, \ldots, X_n$ are iid, so that $\bX^\top \bX \theta = \sum_{i=1}^n (X_i^\top \theta)^2 X_i = 0$ and $\bX^\top \bX$ cannot be invertible almost surely.
	The proof of Point~(2) $\Rightarrow$ Point~(3) can be done by recurrence. We first remark that $\bX^\top \bX$ is invertible if and only if $\span(X_1, \ldots X_n) = \R^d$ (indeed $\ker(\bX^\top \bX = \ker(\bX)$ so that $\bX^\top \bX u = 0 \Leftrightarrow \bX u = 0 \Leftrightarrow X_i^\top \theta = 0$ for all $i=1, \ldots, n$.) We will show that $\span(X_1, \ldots, X_d) \R^d$ almost surely by recurrence. We put $V_k = \span(X_1, \ldots, X_k)$ so that $\dim(V_k) \leq k \leq d$. For $k=1$ we do have $\dim V_1 = 1$ so it is OK. Assume that $\dim(V_{k-1}) = k-1$. We have that $X_k$ is indendent from $V_{k-1} = \span(X_1, \ldots, X_{k-1})$ and $\dim(V_{k-1}) = k-1 < d$ so that $V_{k-1} \subset H$ where $H \subset \R^d$ is an hyperplane. So, we have again by indepedence that $\P(X_k \in V_{k-1}) = \P(X_k \in V_{k-1} | X_1, \ldots, X_{k-1}) \leq \P(X_k \in H) = 0$ using Point~(2). So, $X_k \notin V_{k-1}$ almost surely, and $\dim(V_k) = k$ almost surely.
\end{proof}

\section{Some properties of the least squares estimator} % (fold)
\label{sec:some_properties_of_the_least_squares_estimator}

In this section, we will simplify further the problem, and assume that everything will be computed conditionally on $X_1, \ldots, X_n$, so that we will threat them as if these were deterministic random vectors.
We assume also that $\eps_1, \ldots, \eps_n$ are iid and such that $\E[\eps_i] = 0$ and $\var[\eps_i] = \sigma^2 < +\infty$, which means that $\E[\beps] = 0$ and $\var[\beps] = \sigma^2 \bI_n$, so that
\begin{equation*}
	Y_i = X_i^\top \theta + \eps_i.
\end{equation*}
We assume also that $\bX^\top \bX \succ 0$ (namely $\bX$ is full rank, we saw in Theorem~\ref{thm:least-squares-existence} what this means whenever $X_i$ are random).
In this case we can write $\wh \theta_n = (\bX^\top \bX)^{-1} \bX^\top \by$ so that
\begin{equation*}
	\E_\theta[\wh \theta_n] = (\bX^\top \bX)^{-1} \bX^\top \E_\theta[\by] = (\bX^\top \bX)^{-1} \bX^\top \bX \theta = \theta
\end{equation*}
which means that $\wh \theta_n$ is an \emph{unbiased} estimator. We can write also
\begin{equation*}
	\var_\theta[\wh \theta_n] = \var_\theta[ (\bX^\top \bX)^{-1} \bX^\top \by] = \bA \var_\theta(\by) \bA^\top = \sigma^2 \bA \bA^\top = \sigma^2 (\bX^\top \bX)^{-1}
\end{equation*}
where we put $\bA = (\bX^\top \bX)^{-1} \bX^\top$.
In particular, this proves that the quadratic risk of $\wh \theta_n$ is given by
\begin{equation*}
	\E_\theta[(\wh \theta_j - \theta_j)^2] = \var_\theta[\wh \theta_j] = \sigma^2 ((\bX^\top \bX)^{-1})_{j, j}
\end{equation*}
or equivalently that
\begin{equation*}
	\E_\theta \norm{\wh \theta - \theta}^2 = \sigma^2 \tr [(\bX^\top \bX)^{-1}].
\end{equation*}
We don't know either the variance $\sigma^2$ of the noise, so we want to build an estimator for it as well.


% section some_properties_of_the_least_squares_estimator (end)

% section least_squares_estimator (end)

% section proofs (end)

% paragraph paragraph_name (end)


% section tests (end)

% section confidence_intervals (end)



\end{document}
