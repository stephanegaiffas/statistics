%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% kaobook
% LaTeX Template
% Version 1.2 (4/1/2020)
%
% This template originates from:
% https://www.LaTeXTemplates.com
%
% For the latest template development version and to make contributions:
% https://github.com/fmarotta/kaobook
%
% Authors:
% Federico Marotta (federicomarotta@mail.com)
% Based on the doctoral thesis of Ken Arroyo Ohori (https://3d.bk.tudelft.nl/ken/en)
% and on the Tufte-LaTeX class.
% Modified for LaTeX Templates by Vel (vel@latextemplates.com)
%
% License:
% CC0 1.0 Universal (see included MANIFEST.md file)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
	fontsize=11pt, % Base font size
	twoside=false, % Use different layouts for even and odd pages (in particular, if twoside=true, the margin column will be always on the outside)
	%open=any, % If twoside=true, uncomment this to force new chapters to start on any page, not only on right (odd) pages
	%chapterprefix=true, % Uncomment to use the word "Chapter" before chapter numbers everywhere they appear
	%chapterentrydots=true, % Uncomment to output dots from the chapter name to the page number in the table of contents
	numbers=noenddot, % Comment to output dots after chapter numbers; the most common values for this option are: enddot, noenddot and auto (see the KOMAScript documentation for an in-depth explanation)
	%draft=true, % If uncommented, rulers will be added in the header and footer
	%overfullrule=true, % If uncommented, overly long lines will be marked by a black box; useful for correcting spacing problems
]{kaobook}

% Set the language
\usepackage[english]{babel} % Load characters and hyphenation
\usepackage[english=british]{csquotes} % English quotes

% Load packages for testing
\usepackage{blindtext}
%\usepackage{showframe} % Uncomment to show boxes around the text area, margin, header and footer
%\usepackage{showlabels} % Uncomment to output the content of \label commands to the document where they are used

% Load the bibliography package
\usepackage{styles/kaobiblio}
\addbibresource{main.bib} % Bibliography file

% Load mathematical packages for theorems and related environments. NOTE: choose only one between 'mdftheorems' and 'plaintheorems'.
\usepackage{styles/mdftheorems}
%\usepackage{styles/plaintheorems}


\graphicspath{{images/}} % Paths in which to look for images

% \RequirePackage{times}

\DeclareMathOperator{\cA}{\mathcal A}
\DeclareMathOperator{\cB}{\mathcal B}
\DeclareMathOperator{\cC}{\mathcal C}
\DeclareMathOperator{\cD}{\mathcal D}
\DeclareMathOperator{\cE}{\mathcal E}
\DeclareMathOperator{\cF}{\mathcal F}
\DeclareMathOperator{\cG}{\mathcal G}
\DeclareMathOperator{\cM}{\mathcal M}
\DeclareMathOperator{\cN}{\mathcal N}
\DeclareMathOperator{\cP}{\mathcal P}

\DeclareMathOperator{\bA}{\boldsymbol A}
\DeclareMathOperator{\bB}{\boldsymbol B}
\DeclareMathOperator{\bI}{\boldsymbol I}

\DeclareMathOperator{\bSigma}{\boldsymbol \Sigma}


\DeclareMathOperator{\nor}{Normal}
\DeclareMathOperator{\ber}{Bernoulli}
\DeclareMathOperator{\bin}{Binomial}
\DeclareMathOperator{\mul}{Multinomial}
\DeclareMathOperator{\expo}{Exponential}
\DeclareMathOperator{\uni}{Uniform}

\DeclareMathOperator{\sigmoid}{sigmoid}

\DeclareMathOperator{\leb}{Lebesgue}

\newcommand{\eps}{\varepsilon}

\renewcommand{\P}{\mathbb P}
\newcommand{\E}{\mathbb E}
\newcommand{\R}{\mathbb R}
\newcommand{\var}{\mathbb V}

\newcommand{\wh}{\widehat}

\newcommand{\ind}[1]{\mathbf 1_{#1}}
\newcommand{\grad}{\nabla}


\newcommand{\mgeq}{\succcurlyeq}
\newcommand{\mleq}{\preccurlyeq}
\newcommand{\goes}{\rightarrow}
\newcommand{\go}{\rightarrow}

\newcommand{\norm}[1]{\|#1\|}

\newcommand{\gopro}{\overset{\P}{\rightarrow}}
\newcommand{\goas}{\overset{\text{as\ }}{\rightarrow}}
\newcommand{\goqr}{\overset{\text{$L^2$\ }}{\rightarrow}}
\newcommand{\gosto}{\leadsto}


% \newcommand{\lest}{ \underset{\text{st}}{\leq}}
\newcommand{\lest}{\preceq}
% \newcommand{\gest}{\underset{\text{st}}{\qeq}}
\newcommand{\gest}{\succeq}



% \makeindex[columns=3, title=Alphabetical Index, intoc] % Make LaTeX produce the files required to compile the index

% \makeglossaries % Make LaTeX produce the files required to compile the glossary

% \makenomenclature % Make LaTeX produce the files required to compile the nomenclature

% Reset sidenote counter at chapters
%\counterwithin*{sidenote}{chapter}

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	BOOK INFORMATION
%----------------------------------------------------------------------------------------

\titlehead{Some stuff about statistics}
\subject{Lecture notes for the ENS course of Statistics}

\title[Some stuff about Statistics]{Some stuff about Statistics}
% \subtitle{Customise this page according to your needs}

\author[St\'ephane Ga\"iffas"]{St\'ephane Ga\"iffas\thanks{}}

\date{\today}

\publishers{}

%----------------------------------------------------------------------------------------

\frontmatter % Denotes the start of the pre-document content, uses roman numerals

%----------------------------------------------------------------------------------------
%	OPENING PAGE
%----------------------------------------------------------------------------------------

%\makeatletter
%\extratitle{
%	% In the title page, the title is vspaced by 9.5\baselineskip
%	\vspace*{9\baselineskip}
%	\vspace*{\parskip}
%	\begin{center}
%		% In the title page, \huge is set after the komafont for title
%		\usekomafont{title}\huge\@title
%	\end{center}
%}
%\makeatother

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

% \makeatletter
% \uppertitleback{\@titlehead} % Header

% \lowertitleback{
% 	\textbf{Disclaimer}\\
% 	You can edit this page to suit your needs. For instance, here we have a no copyright statement, a colophon and some other information. This page is based on the corresponding page of Ken Arroyo Ohori's thesis, with minimal changes.
	
% 	\medskip
	
% 	\textbf{No copyright}\\
% 	\cczero\ This book is released into the public domain using the CC0 code. To the extent possible under law, I waive all copyright and related or neighbouring rights to this work.
	
% 	To view a copy of the CC0 code, visit: \\\url{http://creativecommons.org/publicdomain/zero/1.0/}
	
% 	\medskip
	
% 	\textbf{Colophon} \\
% 	This document was typeset with the help of \href{https://sourceforge.net/projects/koma-script/}{\KOMAScript} and \href{https://www.latex-project.org/}{\LaTeX} using the \href{https://github.com/fmarotta/kaobook/}{kaobook} class.
	
% 	The source code of this book is available at:\\\url{https://github.com/fmarotta/kaobook}
	
% 	(You are welcome to contribute!)
	
% 	\medskip
	
% 	\textbf{Publisher} \\
% 	First printed in May 2019 by \@publishers
% }
% \makeatother

%----------------------------------------------------------------------------------------
%	DEDICATION
%----------------------------------------------------------------------------------------

% \dedication{
% 	The harmony of the world is made manifest in Form and Number, and the heart and soul and all the poetry of Natural Philosophy are embodied in the concept of mathematical beauty.\\
% 	\flushright -- D'Arcy Wentworth Thompson
% }

%----------------------------------------------------------------------------------------
%	OUTPUT TITLE PAGE AND PREVIOUS
%----------------------------------------------------------------------------------------

% Note that \maketitle outputs the pages before here

% If twoside=false, \uppertitleback and \lowertitleback are not printed
% To overcome this issue, we set twoside=semi just before printing the title pages, and set it back to false just after the title pages
\KOMAoptions{twoside=semi}
\maketitle
\KOMAoptions{twoside=false}

%----------------------------------------------------------------------------------------
%	PREFACE
%----------------------------------------------------------------------------------------

% \chapter*{Preface}
% \addcontentsline{toc}{chapter}{Preface} % Add the preface to the table of contents as a chapter

% I am of the opinion that every \LaTeX\xspace geek, at least once during 
% his life, feels the need to create his or her own class: this is what 
% happened to me and here is the result, which, however, should be seen as 
% a work still in progress. Actually, this class is not completely 
% original, but it is a blend of all the best ideas that I have found in a 
% number of guides, tutorials, blogs and tex.stackexchange.com posts. In 
% particular, the main ideas come from two sources:

% \begin{itemize}
% 	\item \href{https://3d.bk.tudelft.nl/ken/en/}{Ken Arroyo Ohori}'s 
% 	\href{https://3d.bk.tudelft.nl/ken/en/nl/ken/en/2016/04/17/a-1.5-column-layout-in-latex.html}{Doctoral 
% 	Thesis}, which served, with the author's permission, as a backbone 
% 	for the implementation of this class;
% 	\item The 
% 		\href{https://github.com/Tufte-LaTeX/tufte-latex}{Tufte-Latex 
% 			Class}, which was a model for the style.
% \end{itemize}

% The first chapter of this book is introductive and covers the most 
% essential features of the class. Next, there is a bunch of chapters 
% devoted to all the commands and environments that you may use in writing 
% a book; in particular, it will be explained how to add notes, figures 
% and tables, and references. The second part deals with the page layout 
% and design, as well as additional features like coloured boxes and 
% theorem environments.

% I started writing this class as an experiment, and as such it should be 
% regarded. Since it has always been indended for my personal use, it may 
% not be perfect but I find it quite satisfactory for the use I want to 
% make of it. I share this work in the hope that someone might find here 
% the inspiration for writing his or her own class.

% \begin{flushright}
% 	\textit{Federico Marotta}
% \end{flushright}


%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LIST OF FIGURES/TABLES
%----------------------------------------------------------------------------------------

% \begingroup % Local scope for the following commands

% % Define the style for the TOC, LOF, and LOT
% %\setstretch{1} % Uncomment to modify line spacing in the ToC
% %\hypersetup{linkcolor=blue} % Uncomment to set the colour of links in the ToC
% \setlength{\textheight}{23cm} % Manually adjust the height of the ToC pages

% % Turn on compatibility mode for the etoc package
% \etocstandarddisplaystyle % "toc display" as if etoc was not loaded
% \etocstandardlines % toc lines as if etoc was not loaded

% \tableofcontents % Output the table of contents

% \listoffigures % Output the list of figures

% % Comment both of the following lines to have the LOF and the LOT on different pages
% \let\cleardoublepage\bigskip
% \let\clearpage\bigskip

% \listoftables % Output the list of tables

% \endgroup

%----------------------------------------------------------------------------------------
%	MAIN BODY
%----------------------------------------------------------------------------------------

\mainmatter % Denotes the start of the main document content, resets page numbering and uses arabic numbers
\setchapterstyle{kao} % Choose the default chapter heading style


\input{chap01_introduction}

\input{chap02_statistical_models}



\setchapterpreamble[u]{\margintoc}
\chapter{Statistical inference}
\label{chap:statistical_inference}

In this Chapter, we will consider all along the simple Bernoulli model, where we have iid samples $X_1, \ldots, X_n$ distributed as $\ber(\theta)$ with $\theta \in (0, 1)$.
Let us start with the first \emph{inference} problem: the \emph{estimation} problem.

\section{Estimation} % (fold)
\label{sec:estimation}

We want to \emph{infer} $\theta$, or \emph{estimate} it by finding a statistic which is a measurable function of $(X_1, \ldots, X_n)$%
\sidenote{Once again, since we are doing statistics, the only thing we are allowed to use is the data.}%
or a measurable function of $S_n = \sum_{i=1}^n X_i$ thereof, since $S_n$ is sufficient, see Section~\ref{sec:statistics}.
We will denote such a statistic as
\begin{equation*}
 	\wh \theta_n = \wh \theta(X_1, \ldots, X_n).
\end{equation*}
This function \emph{does not depend} on $\theta$, but its distribution does.
Ideally, we want $\wh \theta_n$ to be ``close'' to $\theta$, since we want a good estimator, so that the first thing we need to do is to quantify ``closeness''.
For instance, we could want $|\wh \theta_n - \theta|$ to be close to $0$ with a large probability, since we do not forget that $\wh \theta_n$ is a random variable, as a function of the data $(X_1, \ldots, X_n)$.
The most natural distance is arguably the Euclidean one, in this context the $L^2$ distance, 
which leads to the \emph{quadratic risk}.%
\sidenote{Although the quadratic risk corresponds to a \emph{squared} $L^2$ norm.}
\begin{definition}[Quadratic risk]
	\label{def:quadratic_risk}
	Consider a statistical model with data $X$ and set of parameters $\Theta \subset \R$ and an estimator $\wh \theta(X)$. 
	The quadratic risk of $\wh \theta$ is given by
	\begin{equation*}
		R(\wh \theta, \theta) = \E_\theta[ (\wh \theta - \theta)^2 ] = \int_E (\wh \theta(x) - \theta)^2 P_\theta(dx).
	\end{equation*}
	We consider the quadratic risk as a function $\Theta \goes \R^+$ of the parameter given by $\theta \mapsto R(\wh \theta, \theta)$.
\end{definition}
At this point, it's useful to recall some classical inequalities on the queues of random variables.
The Markov inequality tells us that if $Y$ is a real random variable such that $\E |Y|^p < +\infty$ for some $p > 0$ then
\begin{equation*}
	\P(|Y| > t) \leq \frac{\E |Y|^p}{t^p}
\end{equation*}
for any $t > 0$.
This tells us that the more $Y$ has moments%
\sidenote{We say that $Y$ as moments up to order $p$ if $\E |Y|^p < +\infty$. 
Note that this entails $\E |Y|^q < +\infty$ for any $q < p$ since $\E |Y|^p = \E (|Y|^q)^{p/q} \geq (\E |Y|^q)^{p/q}$ using Jensen's inequality.}%
the more the queue of $Y$ is tight (it goes faster to $0$ with $t \goes +\infty$).
Markov's inequality with $p=2$ entails 
\begin{equation}
	\label{eq:l2_entrails_proba}
	\P(|\wh \theta - \theta| > t) \leq \frac{R(\wh \theta, \theta)}{t^2}
\end{equation}
which tells us that whenever the quadratic risk is small, then $\wh \theta$ is close to $\theta$ with a large probability.

Whenever $R(\wh \theta_n, \theta) \rightarrow 0$ with $n \rightarrow +\infty$, we will write $\wh \theta_n \goqr \theta$, which stands for convergence in $L^2$ norm, which entails, because of Inequality~\eqref{eq:l2_entrails_proba}, that $\wh \theta_n \gopro \theta$, which stands for convergence in probability.\sidenote{More precisely, in $\P_\theta$-probability, namely $\P_\theta[|\wh \theta_n - \theta| > \eps] \rightarrow 0$ as $n \rightarrow +\infty$ for any $\eps > 0$, but we will write $\wh \theta_n \gopro \theta$ in order to keep the notations as simple as possible.}%
\begin{definition}
	\label{def:consistent}
	We say that $\wh \theta_n$ is \emph{consistent} whenever $\P_\theta[|\wh \theta_n - \theta| > \eps] \rightarrow 0$ as $n \rightarrow +\infty$ for any $\eps > 0$ and any $\theta \in \Theta$.
	We say that it is strongly consistent whenever $\P_\theta[\wh \theta_n \rightarrow \theta] = 1$ for any $\theta \in \Theta$.
\end{definition}
In Definitions~\ref{def:quadratic_risk} and~\ref{def:consistent} above, if $\Theta \subset \R^d$, it suffices to replace $|\cdot|$ by the Euclidean norm $\norm{\cdot}_2$.%
\sidenote{We can use any norm on $\R^d$ in Definition~\ref{def:consistent}, and let us recall that $\norm{x}_2 = \sqrt{x^\top x} = (\sum_{j=1}^d x_j^2)^{1/2}$.}%

\paragraph{Bias variance decomposition.} % (fold)

The \emph{bias-variance decomposition} is the following decomposition of the quadratic risk between two terms: a bias term denoted $b(\wh \theta_n, \theta)$ (squared in the formula) and a variance term:
\begin{equation}
	\label{eq:bias-variance-decomposition}
	\begin{split}
	R(\wh \theta_n, \theta) &= \E_\theta[(\wh \theta_n - \theta)^2] = (\E_\theta \wh \theta_n - \theta)^2 + \var_\theta(\wh \theta_n) \\
	&= b(\wh \theta_n, \theta)^2 + \var_\theta(\wh \theta_n).		
	\end{split}
\end{equation}
When $b(\wh \theta_n, \theta) = 0$ for all $\theta \in \Theta$ we say that the estimator $\wh \theta_n$ is \emph{unbiased}.
This means that this estimator will not tend to over or under-estimate $\theta$. 

\paragraph{Back to Bernoulli.} % (fold)

% paragraph back_to_bernoulli (end)Back to Bernoulli
Going back to the $\ber(\theta)$ model, we consider the estimator $\wh \theta_n = S / n = \sum_{i=1}^n X_i / n$.
We already know many things about this estimator:
\begin{enumerate}
	\item We have $\E_\theta [\wh \theta_n] = \theta$ which means that $\wh \theta_n$ is unbiased;
	\item The bias-variance decomposition gives
	\begin{equation}
		\label{eq:bernoulli-quadratic-risk}
	 	R(\wh \theta_n, \theta) = \var(\wh \theta_n) = \frac{\theta (1 - \theta)}{n} \leq 
	 	\frac{1}{4 n} \rightarrow 0
	 \end{equation}
	 which means that $\wh \theta_n \goqr \theta$ and which entails that $\wh \theta_n$ it is consistent;
	\item The law of large number tells us that $\wh \theta_n \goas \theta$, hence $\wh \theta_n$ is strongly consistent;
	\item The central limit theorem tells us that
	\begin{equation}
	\label{eq:tcl-bernoulli}
	\sqrt n (\wh \theta_n - \theta) \leadsto \nor(0, \theta(1 - \theta)).
	\end{equation}
\end{enumerate}
The points 2--4 from above are all different ways of saying that when $n$ is large enough, then $\wh \theta_n$ is close to $\theta$.
In practice, an estimator leads to a value: for the Bernoulli model with $n=100$ and $42$ ones you end up with a single estimated value $0.42$.
But what if we want to include uncertainty in this estimation ?
Namely how confident are we about this $0.42$ value ?
Moreover, what do we mean by ``when $n$ large enough'', can we quantify this better ?
All these questions can be answered by considering another inference problem: confidence intervals.

\section{Confidence intervals} % (fold)
\label{sec:confidence_intervals}

Here, we don't only want to build an estimator $\wh \theta_n$ but also to quantify the uncertainty of this estimation.
Combining Inequalities~\eqref{eq:l2_entrails_proba} and~\eqref{eq:bernoulli-quadratic-risk} leads to
\begin{equation*}
	\P_\theta[ |\wh \theta_n - \theta| > t] \leq \frac{1}{4 n t^2}
\end{equation*}
so that for $\alpha \in (0, 1)$ and the choice $t_\alpha = 1 / (2 \sqrt{n \alpha})$ we have 
\begin{equation*}
	\P_\theta \{ \theta \in [ \wh \theta^L, \wh \theta^R ] \} \geq 1 - \alpha,
\end{equation*}
where
\begin{equation*}
	\wh \theta^L =  \wh \theta_n - \frac{1}{2 \sqrt{n \alpha}} \quad \text{ and } \quad \wh \theta^R =  \wh \theta_n + \frac{1}{2 \sqrt{n \alpha}}.
\end{equation*}
Therefore, if we choose $\alpha = 0.05 = 5\%$, we know that $\theta \in [\wh \theta^L, \wh \theta^R]$ with a probability larger than $95\%$.
We say in this case that the interval $[\wh \theta^L, \wh \theta^R]$ is a \emph{confidence interval} with coverage $95\%$.%
\sidenote{If we toss the coin $1000$ times and get $420$ heads, the realization of this confidence interval at $95\%$ is $[0.35, 0.49]$.}

Note that if $\alpha = 0$ then we have no other choice than using the whole $\R$ as a confidence interval: $\alpha$ allows to give some slack, so that we can build a non-absurdly  large confidence interval. 
We typically have that $|\wh \theta^R - \wh \theta^L|$ increases as $\alpha$ decreases, since a smaller $\alpha$ means more confidence, hence a larger interval.
On the contrary, $|\wh \theta^R - \wh \theta^L|$ should decrease with the sample size $n$.
\begin{definition}[Confidence interval]
	Consider a statistical model with data $X$ and set of parameters $\Theta \subset \R$. 
	Fix a \emph{confidence level} $\alpha \in (0, 1)$ and consider two statistics $\wh \theta^L(X)$ and $\wh \theta^R(X)$. Whenever 
	\begin{equation}
		\label{eq:coverage-property}
		\P_\theta\{ \theta \in [\wh \theta^L(X), \wh \theta^R(X)] \} \geq 1 - \alpha
	\end{equation}
	for any $\theta \in \Theta$, we say that $[\wh \theta^L(X), \wh \theta^R(X)]$ is a \emph{confidence interval} at level $1 - \alpha$.
\end{definition}
Inequality~\eqref{eq:coverage-property} is called the \emph{coverage} property of the confidence interval.
More generally, when $\Theta \subset \R^d$, we will say that $S(X)$ is a \emph{confidence set} if it is a statistic satisfying the coverage property $\P_\theta\{ \theta \in S(X) \} \geq 1 - \alpha$ for any $\theta \in \Theta$.
\begin{remark}
	Whenever we need only an upper or lower bound on $\theta$ (for instance, when we need to check statistically that some toxicity level is below some threshold), we build a \emph{unilateral} or \emph{one-sided} confidence interval, where we choose either $\wh \theta^L = -\infty$ ($0$ for the Bernoulli model) or $\wh \theta^R = +\infty$ ($1$ for the Bernoulli model).
	Indeed, at a fixed level $1 - \alpha$, the bound of a one-sided confidence interval is tighter than the same sided bound corresponding to a two-sided interval. 
\end{remark}
But, we can do better for the Bernoulli model (or any model where $X$ is bounded) thanks to the following Hoeffding inequality.
\begin{theorem}
	\label{thm:hoeffding}
	Let $X_1, \ldots, X_n$ be independent random variables such that $X_i \in [a_i, b_i]$ almost surely and let $S = \sum_{i=1}^n X_i$. Then,
	\begin{equation*}
		\P[ S \geq \E S + t] \leq \exp\Big( - \frac{2 t^2}{\sum_{i=1}^n (b_i - a_i)^2} \Big)
	\end{equation*}
	holds for any $t >0$.
\end{theorem}
Theorem~\ref{thm:hoeffding} is something called a deviation inequality: it provides a control on the probability of deviation of $S$ with respect to its mean.
It shows that bounded random variables are \emph{sub-Gaussian}, since it shows that the queue of $S - \E S$ is bounded by $\exp(-c t^2)$ for some constant $c$ (that depends on $n$).
The proof of this inequality is provided in Chapter~??\todo{insert reference} below.
\todo{Provide the proof of Hoeffding}

\paragraph{Back to Bernoulli.} % (fold)

% paragraph back_to_bernoulli (end)
Let's apply Theorem~\ref{thm:hoeffding} to the Bernoulli model $X_i \sim \ber(\theta)$ so that $a_i = 0$, $b_i = 1$ and therefore $\P[ S \geq \E S + t] \leq e^{-2 t^2 / n}$.
Using again Theorem~\ref{thm:hoeffding} with $X_i$ replaced by $-X_i$ together with an union bound%
\sidenote{}%
leads to $\P[ | S - \E S | \geq t] \leq 2 e^{-2 t^2 / n}$.
So, for some $\alpha \in (0, 1)$, we obtain another confidence interval, since the following coverage property holds:
\begin{equation*}
	\P \bigg[ \wh \theta_n - \sqrt{\frac{\log(2 / \alpha)}{2n}} \leq \theta \leq \wh \theta_n 
	+ \sqrt{\frac{\log(2 / \alpha)}{2n}} \bigg] \geq 1 - \alpha.
\end{equation*}
This proves that $[\wh \theta \pm \sqrt{\log(2 / \alpha) / (2n)}]$ is a confidence interval at level $1 - \alpha$.%
\sidenote{For $1000$ tosses and $420$ heads, the realization of this interval at level $95\%$ is $[0.37, 0.46]$. It's a bit more precise than the previous one, which was based on Markov's inequality.}
Let's compare the two confidence intervals we obtained so far for the Bernoulli model.
We have of course
\begin{equation*}
	\frac{1}{2 \sqrt{n \alpha}} > \sqrt{\frac{\log(2 / \alpha)}{2n}} 
\end{equation*}
for $\alpha$ small enough\todo{etre plus precis ici}, although both sides are $O(1 / \sqrt n)$.
Only the dependence on the level $\alpha$ is improved with the confidence interval obtained through Hoeffding's inequality, since it exploits the sub-Gaussianity of the Bernoulli distribution, while the first confidence interval only used the upper bound~\eqref{eq:l2_entrails_proba} on the variance.%
\sidenote{There is yet another way to build a (non-asymptotic) confidence interval, using a more computational approach, see for instance ????}

\paragraph{Asymptotic coverage.}

For both the previous confidence intervals, we adopted a \emph{non-asymtotic} approach: the coverage properties hold for any value of $n \geq 1$.
This is possible since we know things about the distribution of $S$, which is a simple $\bin(n, \theta)$ distribution.
However, in general, the \emph{exact} distribution of an estimator $\wh \theta_n$ cannot always be made explicit, and in such cases, we often use Gaussian approximations, thanks to the central limit theorem. 
Let's do this for the Bernoulli model.
Indeed, we know from~\eqref{eq:tcl-bernoulli} that
\begin{equation}
	\label{eq:portemanteau-bernoulli}
	\P_\theta \bigg[ \sqrt{\frac{n}{\theta (1 - \theta)}} (\wh \theta_n - \theta) \in I
	\bigg] \rightarrow \P[Z \in I]
\end{equation}
where $Z \sim \nor(0, 1)$ for any closed interval $I \subset \R$.%
\sidenote{This uses the porte-manteau theorem, which says that $X_n \gosto X$ if and only if $\P(X_n \in A) \goes \P(X \in A)$ for any Borelian set $A$ such that $\P(X \in \partial A) = 0$, where $\partial A$ stands for the boundary of $A$.}%
Using $I = [-q_\alpha, q_\alpha]$ with $q_\alpha = \Phi^{-1}(1 - \alpha / 2)$ we end up%
\sidenote{We recall that $\Phi^{-1}$ is the inverse of the distribution function $\Phi(x) = \P( Z \leq x)$ of the $\nor(0, 1)$ distribution, that we call the \emph{quantile} function of $\nor(0, 1)$.}%
with the fact that
\begin{equation}
	\label{eq:not-and-ic}
	\P_\theta\bigg\{ \theta \in \Big[ \wh \theta_n \pm q_\alpha \sqrt{\frac{\theta(1 - \theta)}{n}} \Big] \bigg\} \rightarrow 1 - \alpha.
\end{equation}
This is interesting, but not enough to build a confidence interval, since the interval in Equation~\eqref{eq:not-and-ic} depends on $\theta$ through the variance term $\theta(1 - \theta)$.
Indeed, a confidence interval must be something that does \emph{not} depend on $\theta$.
We need to work a little bit more in order to remove the dependence on $\theta$ from this interval. 
We do the same as before: we use the fact that $\theta (1 - \theta) \leq 1 / 4$ for any $\theta \in [0, 1]$, so that
\begin{equation*}
	\liminf_n \P_\theta \bigg\{ \theta \in \Big [\wh \theta_n \pm \frac{q_\alpha}{2 \sqrt n} \Big] \bigg\} \geq 1 - \alpha.
\end{equation*}
This is what we call a confidence interval \emph{asymptotically of level} $1 - \alpha$ constructed \emph{by excess}.

In the above construction of an asymptotic confidence interval, we used the central limit theorem to approximate the $\bin(n, \theta)$ distribution (the distribution of $S = \sum_{i=1}^n X_i$) by a Gaussian distribution.
This requires $n$ to be ``large enough'', but the central limit theorem does not tell us how large.
We can quantify this better by trying to quantify how close the distribution function of $S$ is to the one of $\nor(0, 1)$, using the following theorem.
\begin{theorem}[Berry-Esseen]
	Let $X_1, \ldots, X_n$ be i.i.d random variables such that $\E X_i = 0$ and $\var(X_i) = \sigma^2$ and introduce the distribution function%
	\marginnote{BLABLABLA}%
	\begin{equation*}
		F_n(x) = \P \bigg[ \frac{\sum_{i=1}^n X_i}{\sqrt{n \sigma^2}} \leq x \bigg]
	\end{equation*}
	for any $x \in \R$. Then, the following inequality holds:
	\begin{equation*}
		\sup_{x \in \R} |F_n(x) - \Phi(x)| \leq \frac{c \kappa}{\sigma^3 \sqrt n},
	\end{equation*}
	where $\kappa = \E |X_1|^3$ (assumed finite) and where $c$ is a purely numerical constant (the best known one is $c = 0.4748$).
\end{theorem}
We provide a proof with a worse constant $c$ in Section~??? below.\todo{insert proof}
Note that there is also a lower bound with $c \geq 0.4097$, as explained in ?? and that the best constant $c = 0.4748$ for the upper bound is from ???.
Note also that a similar result holds if the $X_i$ are independent but not identically distributed.

For Bernoulli with have $\E|X_1|^3 = \theta$ and $\sigma^3 = (\theta(1 - \theta))^{3/2}$ so that 
\begin{equation*}
	|F_n(x) - \Phi(x)| \leq \frac{3}{\sqrt{n \theta (1 - \theta)^3}}
\end{equation*}
which shows that the approximation by the Gaussian distribution deteriorates whenever $\theta$ is close to $0$ or $1$.

\paragraph{Reparametrization.} % (fold)

Another asymptotic approach allowing to get rid of the dependence on $\theta$ in the confidence interval~\eqref{eq:not-and-ic} is based on the idea of reparametrization.
Indeed, given a statistical model $\{ P_\theta : \theta \in \Theta \}$ and a bijective function $g : \Theta \goes \Lambda$ we can use instead the ``reparametrized''model $\{ Q_\lambda : \lambda \in \Lambda \}$ where $Q_\lambda = P_{g^{-1}(\lambda)}$.
Let us also remark that if $[\wh \theta^L, \wh \theta^R]$ is a confidence interval for $\theta$ at level $1 - \alpha$ and if $g$ is an increasing function, then 
$[g(\wh \theta^L), g(\wh \theta^R]$ is a confidence interval for $g(\theta)$ at level $1 - \alpha$.
Now, a natural question is to understand if the convergences we used above (in particular, the convergence in distribution involved in the central limit theorem) is stable under such a reparametrization.
\begin{example}
	\label{ex:expo}
	Consider a iid dataset $X_1, \ldots, X_n$ with distribution $\expo(\theta)$ with scale parameter $\theta > 0$, namely the distribution $P_\theta(dx) = \theta e^{-\theta x} \ind{x \geq 0} dx$. 
	We have $\E(X_1) = 1 / \theta$ and $\var(X_1) = 1 / \theta^2$, so that using the law of large numbers and the central limit theorem we have
	\begin{equation*}
		\bar X_n \goas \theta^{-1} \quad \text{ and } \quad \sqrt n (\bar X_n - \theta^{-1}) \leadsto \nor(0, \theta^{-2})
	\end{equation*}
	when $n \rightarrow +\infty$.
	Since $x \mapsto 1 / x$ is a continuous function on $(0, +\infty)$, we know that $(\bar X_n)^{-1} \goas \theta$ so that a strongly consistent estimator is given by $\wh \theta = (\bar X_n)^{-1}$.
	But what can we say about the convergence in distribution of 
	$\sqrt n (\wh \theta_n - \theta)$ ?
\end{example}
This is answered by so-called $\Delta$-method, described in the next theorem.
\begin{theorem}[$\Delta$-method]
	\label{thm:delta-method}
 	Let $(Z_n)_{n \geq 1}$ be a sequence of real random variables and assume that 
	$a_n(Z_n - z) \leadsto Z$, where $(a_n)_{n \geq 1}$ is a positive sequence such that $a_n \goes +\infty$, where $z \in \R$ and where $Z$ is a real random variable.
 	If $g$ is a function defined on a neighborhood of $z$ and differentiable at $z$, we have 
 	\begin{equation}
 		a_n (g(Z_n) - g(z)) \leadsto g'(z) Z
 	\end{equation}
 	as $n \goes +\infty$.
\end{theorem}
The proof easily follows from a first-order Taylor expansion of $g$ around~$z$, as explained in Section~??? below. \todo{insert proof}
A particularly useful case for statistics is when $Z$ is Gaussian.
For instance, if $\sqrt n (\wh \theta_n - \theta) \leadsto \nor(0, \sigma(\theta)^2)$, we have
\begin{equation*}
	\sqrt n (g(\wh \theta_n) - g(\theta)) \leadsto 
	\nor(0, \sigma(\theta)^2 (g'(\theta))^2)
\end{equation*}
whenever $g$ satisfies the conditions of Theorem~\ref{thm:delta-method}.
Going back to the $\expo(\theta)$ example from Example~\ref{ex:expo}, we obtain with $g(x) = 1 / x$ and since $\wh \theta = g(\bar X_n)$ that $\sqrt n (\wh \theta - \theta) \leadsto \nor(0, \theta^2)$.
Another result which provides stability for the convergence in distribution under a smooth mapping is the so-called Slutsky theorem.
\begin{theorem}[Slutsky]
	\label{thm:slutsky}
	Let $(X_n)_{n \geq 1}$ and $(Y_n)_{n \geq 1}$ be sequences of real random variables such that $X_n \leadsto X$ and $Y_n \leadsto y$ where $X$ is some real random variable and $y \in \R$.
	Then, we have that $Y_n \gopro y$ and $(X_n, Y_n) \leadsto (X, y)$ as $n \goes +\infty$. In particular, we have $f(X_n, Y_n) \leadsto f(X, y)$ for any continuous function $f$.
\end{theorem}
The proof of Theorem~\ref{thm:slutsky} is given in Section~? below.\todo{insert proof}
The $\Delta$-method provides stability for the convergence in distribution when a differentiable function is applied to a sequence, while Slutsky theorem provides ``algebraic'' stability when combining two sequences converging respectively in distribution and probability.
\sidenote{Be careful with convergence in distribution. Please keep in mind that this mode of convergence is about the convergence of the distributions and not the convergence of the random variables (hence its name). The notation $X_n \gosto X$ is rather misleading but convenient. In particular, nothing can be said in general about $f(X_n, Y_n)$ when we know that $X_n \gosto X$ and $Y_n \gosto Y$ (unless $X_n$ and $X_n$ are independent sequences).}

\paragraph{Back again to Bernoulli.}

We have $\wh \theta_n \gopro \theta$, so that $(\wh \theta_n (1 - \wh \theta_n)^{1/2} \gopro (\theta (1 -  \theta))^{1/2}$ since $x \mapsto (x(1-x))^{1/2}$ is continuous on $[0, 1]$ and let us write
\begin{equation*}
	\frac{\sqrt n (\wh \theta_n - \theta)}{\sqrt{\wh \theta_n (1 - \wh \theta_n)}} 
	= \frac{\sqrt n (\wh \theta_n - \theta)}{\sqrt{ \theta (1 -  \theta)}} \times 
	\sqrt{\frac{ \theta (1 -  \theta)}{\wh \theta_n (1 - \wh \theta_n)}} =: A_n \times B_n.
\end{equation*}
We know that $A_n \gosto \nor(0, 1)$ and that $B_n \gopro 1$.
Therefore, using Theorem~\ref{thm:slutsky} leads%
\sidenote{With $f(x, y) = xy$.}%
to
\begin{equation*}
	\sqrt{\frac{n}{\wh \theta_n (1 - \wh \theta_n)}} (\wh \theta_n - \theta) \gosto \nor(0, 1).
\end{equation*}
Yes, we just replaced $\theta$ by $\wh \theta_n$ in the variance term $\theta(1 - \theta)$ of the limit~\eqref{eq:portemanteau-bernoulli}, but doing so required Slutsky's theorem to prove this rigorously, and this provides us another confidence interval with asymptotic coverage given by
\begin{equation*}
	\P_\theta \bigg\{ \theta \in \Big[ \wh \theta_n \pm q_\alpha \sqrt{\frac{\wh \theta_n (1 - \wh \theta_n)}{n}} \Big] \bigg\} \goes 1 - \alpha
\end{equation*}
as $n \goes +\infty$.%
\sidenote{With $1000$ tosses and $420$ heads, the realization of this confidence interval at level $95\%$ is $[0.38, 0.45]$.}%


\section{Tests} % (fold)
\label{sec:tests}

Let us consider, again, in this section, a statistical experiment with data $X$ and model $\{ P_\theta : \theta \in \Theta \}$.
Here, we want to decide between to hypothesis $H_0$ and $H_1$, where
\begin{equation*}
	H_i \quad \text{means that} \quad \theta \in \Theta_i
\end{equation*}
for $i \in \{ 0, 1 \}$, where $\{ \Theta_0, \Theta_1 \}$ is a partition of the set of parameters~$\Theta$.
In order to understand the concept of statistical testing, let us consider the following unsettling example: imagine that you need to decide if a patient has cancer or not.
The patient has cancer if some parameter $\theta \in (0, 1)$ about him satisfies
$\theta \geq 0.42$.
We \emph{choose} $\Theta_0 = [0.42, 1]$ and $\Theta_1 = [0, 0.42)$, namely we decide that $H_0$ means that the patient has cancer, while $H_1$ means that the patient has not.
We need to construct a testing function $\varphi : E \goes \{ 0, 1 \}$ that maps $X \mapsto \varphi(X)$, our decision being given by the value of $\varphi(X)$. 
We decide that $H_0$ is true whenever $\varphi(X) = 0$, in this case we say that we \emph{accept} $H_0$ and we \emph{reject} $H_0$ whenever $\varphi(X) = 1$.
This is a convention, the $1$ in $\varphi(X) = 1$ and $H_1$ will always mean that we \emph{reject} the \emph{null hypothesis} $H_0$.

\paragraph{Type I and Type II errors.}

When $\theta \in \Theta_i$, we are correct if $\varphi(X) = i$ and incorrect if $\varphi(X) = 1 - i$.
We have two types of errors: the \emph{Type-I error}, also called the \emph{first-order error}, given by 
\begin{equation}
	\label{eq:type-1-error}
	\P_\theta[ \varphi(X) = 1] = \E_\theta [\varphi(X)] \quad \text{for} \quad
	\theta \in \Theta_0
\end{equation}
and the \emph{Type-II error}, also called \emph{second-order error}, given by 
\begin{equation}
	\label{eq:type-2-error}
	\P_\theta[ \varphi(X) = 0] = 1 - \E_\theta [\varphi(X)] \quad \text{for} \quad 
	\theta \in \Theta_1.
\end{equation}
For the cancer detection problem, the Type~I error corresponds to the \emph{probability of saying to the patient that he has not cancer while he has.}
The Type~II error corresponds to the \emph{probability of saying to the patient that he has cancer while he has not}.
Note that these two types of errors are not symmetrical: we consider that the first one is more serious than the second (although this can be debated, the patient could do a depression, or start an invasive treatment for nothing).%
\sidenote{Of course this morbid example is highly unrealistic, and is used only to stress the asymmetry of errors in a statistical testing problem.}
The important point here is that $H_0$ and $H_1$ must be \emph{chosen} depending on the practical application considered. 
They are not \emph{given} and they correspond to an important modeling choice.
We will see below that $H_0$ and $H_1$ must be chosen, in practice, so that the corresponding Type I error is \emph{more serious}, for the considered application, than the Type II error.
\begin{definition}
	\label{def:power-function}
	The function $\beta : \Theta \goes [0, 1]$ that maps $\theta \mapsto \beta(\theta) = \E_\theta [\varphi(X)]$ is called the \emph{power function} of the test $\varphi$.
\end{definition}
Ideally, we would like both the Type~I and Type~II errors to be small, namely $\beta(\theta) \approx 0$ for $\theta \in \Theta_0$ and $\beta(\theta) \approx 1$ for $\theta \in \Theta_1$.
But this is impossible: if $\Theta$ is a connected set then $\Theta_0$ and $\Theta_1$ share a common frontier, so that $\beta$ must be discontinuous on it, while $\beta$ is in general a continuous function.\todo{donner sa valeur dans le cas bernoulli}
Therefore, it is hard to make both the Type~I and Type~II errors small at the same time.

\todo{en fait faudrait donner l'exemple du cancer ici... ?}

\paragraph{Desymmetrization of statistical tests.} % (fold)

The way a statistical test is performed is through the \emph{Neyman-Pearson approach}, where we  \emph{desymmetrize} the problem: choose the hypothesis $H_0$ using common sense, so that the hypothesis leading to a Type~I error that is more serious than the Type~II error.
The Type~I error is always the rejection of $H_0$ when it is true, while the Type~II error is always the acceptation of $H_0$ when it is false.
The only thing that we choose is what are $H_0$ and $H_1$.
Let us wrap up all what we said before, and introduce some extra things.
\begin{definition}
	\label{def:test-definitions}
	Consider a statistical testing problem with hypothesis
	\begin{equation*}
		H_0 : \theta \in \Theta_0 \quad \text{versus} \quad H_1 : \theta \in \Theta_1
	\end{equation*}
	and a testing function $\varphi : E \goes \{ 0, 1 \}$.
	We call $H_0$ the \emph{null} hypothesis and $H_1$ the \emph{alternative} hypothesis.
	When $\varphi(X) = 0$ we say that the test \emph{accepts} $H_0$ or simply that it \emph{accepts}. When $\varphi(X) = 1$ the test \emph{rejects}.
	\begin{enumerate}
		\item The set $R = \{ x \in E : \varphi(x) = 1 \}$ is called the \emph{rejection set} or the \emph{rejection region} of the test $\varphi$, and we call $R^\complement$ the \emph{acceptation region};
		% \item The function $\beta : \Theta \goes [0, 1]$ given by $\beta(\theta) = \E_\theta [\varphi(X)]$ is called the \emph{power function} of the test;
		\item The restriction of the power function $\beta$ from Definition~\ref{def:power-function} $\beta : \Theta_0 \goes [0, 1]$ is called the \emph{Type~I error}, while the restriction $\beta : \Theta_1 \goes [0, 1]$ is called the \emph{power} of the test. The function $1 - \beta : \Theta_1 \goes [0, 1]$ is called the \emph{Type~II error} or \emph{second order error}.
		\item Whenever $\sup_{\theta \in \Theta_0} \beta(\theta) \leq \alpha$ for some fixed $\alpha \in (0, 1)$, we say that the test \emph{has level} $\alpha$.
	\end{enumerate}
\end{definition}
The idea of desymmetrization is as follows: given a level $\alpha \in (0, 1)$ (something like $1\%$, $5\%$ or $10\%$) we build a test so that it has, \emph{by construction}, level $\alpha$.
Namelu, a test is \emph{built so that the Type~I error is controlled}, while nothing is done directly about the Type~II error.
Given two statistical tests with level $\alpha$ (namely Type~I error $\leq \alpha$), we can simply compare their Type~II error and choose the one that maximizes it.

\paragraph{Back to Bernoulli.} % (fold)

Let us go back to the Bernoulli model where $X_1, \ldots, X_n$ are iid and distributed as $\ber(\theta)$.
We consider the problem of statistical testing with hypothesis:
\begin{equation}
	\label{eq:chap03-tests-hypothesis}
	H_0 : \theta \leq \theta_0 \quad \text{ against } \quad H_1 : \theta > \theta_0
\end{equation}
so that $\Theta = (0, 1)$ and $\Theta_0 = (0, \theta_0]$ and $\Theta_1 = (\theta_0, 1)$.
We studied in Sections~\ref{sec:estimation} and~\ref{sec:confidence_intervals} the estimator $\wh \theta_n = \bar X_n$, and know that it is a good estimator.
A natural idea is therefore to reject $H_0$ if $\wh \theta_n$ is too large.
\begin{recipe}
	We build a test by defining its rejection set $R$. The shape of the rejection set can be easily guessed by looking at the alternative hypothesis $H_1$.
\end{recipe}
Since we want to reject when $\theta > \theta_0$, we want to consider a rejection set $R = \{ \wh \theta_n > c \}$ for some constant $c$ chosen so that the Type~I error is controlled by $\alpha$.
Note that choosing $c = \theta_0$ is a bad idea: using the central limit theorem, we see that $\P_{\theta_0}[\wh \theta_n > \theta_0] \goes 1/2$.
We need to increase $c$ by some amount, so that the Type~I error can be indeed smaller than $\alpha$.%
\sidenote{It is very easy to build a statistical test with $\alpha = 0$, namely zero Type~I error. For the cancer example from above, we just need to tell to all the patient that they have cancer. By doing so, we never miss any cancer diagnostic, but on the other hand this test has zero power. Arguably, this is not a good idea, so we need to give some slack in the construction of the test by considering a small but non-zero $\alpha$, giving an opportunity to get a better power.}

We understand at this point that $c$ will depend on $\alpha, \theta_0$ and the sample size $n$, among other things, and that in view of Definition~\ref{def:test-definitions} it needs to be such that $\sup_{\theta \leq \theta_0} \P_\theta[S > n c] \leq \alpha$.
But, we know that $n \wh \theta_n = S \sim \bin(n, \theta)$ under $\P_\theta$%
\sidenote{We write ``under $\P_\theta$'' here since Type~I error control must be performed under the null assumption $\theta \leq \theta_0$, so that we must specify under which distribution (which parameter $\theta$) we are working at this point.}%
, so that
\begin{equation}
	\label{eq:power-control-binomial1}
	\beta(\theta) = \P_\theta[\wh \theta_n > c] = \P_\theta[S > n c] = \P [\bin(n, \theta) > nc]
\end{equation}
for any $\theta \in (0, 1)$.%
\todo{est ce que ca vaut le coup d'expliciter $\beta$ pour montrer que ce n'est pas si simple ?}
%%%
\sidenote{The notation $\P [\bin(n, \theta) > nc]$ is just a convenient way of writing $\P [ B > nc]$ where $B$ is a random variable distributed as $\bin(n, \theta)$. Note also that we replaced $\P_\theta$ simply by $\P$ herein, the notation $\P_\theta$ is required when we need to stress that the computation is performed under $\P_\theta$, while in the last equality we consider a generic probability space with probability $\P$ on which $B$ lives, the dependency on $\theta$ is now only through the distribution of it. These semantics are important and very convenient for statistical computations.}%
%%%
What we need to do now is to study the variations of $\theta \mapsto \beta(\theta)$.
Intuitively, $\beta(\theta)$ should be increasing with $\theta$, since when $\theta$ increases, we get more ones, so that $S$ increases.
This can be nicely formalized using \emph{stochastic ordering}.
\begin{proposition}
	\label{prop:stochastic-ordering}
	Let $P$ and $Q$ be two probability measures on the same probability space. We say that \emph{$Q$ stochastically dominates $P$}, that we denote $P \lest Q$, whenever one of the following equivalent points is granted:
	\begin{enumerate}
		\item There are two real random variables $X \sim P$ and $Y \sim Q$ (on the same probability space) such that $\P[X \leq Y] = 1$;
		\item We have $F_P(x) \geq F_Q(x)$ for any $x \in \R$, where $F_P$ and $F_Q$ are the distribution functions of $P$ and $Q$, or equivalently, $P[(x, +\infty)] \leq Q[(x, +\infty)]$ for any $x \in \R$;%
		\marginnote{We recall that the distribution function of $P$ is 
		$F_P(x) = P[ (-\infty, x]]$.}%
		\item We have $F_P^-(p) \leq F_Q^-(p)$ for any $p \in [0, 1]$ where $F_P^-(p) = \inf \{ x \in \R : F_P(x) \geq p \}$ is the \emph{generalized inverse} of $F_P$ or \emph{quantile function} of $P$;%
		\marginnote{Since a distribution function is non-decreasing and c\`adl\`ag, its generalized inverse is well-defined and unique. See the proof of Proposition~\ref{prop:stochastic-ordering} for more details about it.}%
		\item For any non-decreasing and bounded function $f$ we have $\int f dP \leq \int f dQ$.
	\end{enumerate}
\end{proposition}
The proof of Proposition~\ref{prop:stochastic-ordering} is given in Section~\ref{sec:chap03-proofs} below and follows rather standard arguments.
The proof of $(2) \Rightarrow (1)$ however deserves to be discussed here, since it uses a beautiful yet simple \emph{coupling} argument, which is a very powerful technique often used in probability theory.\todo{INSERT REFERENCE}
More precisely, we use something called a ``quantile coupling'': consider a random variable $U \sim \uni([0, 1])$%
\sidenote{We say that $X \sim \uni([a, b])$ for $a < b$ if it has density $x \mapsto (b - a)^{-1} \ind{[a, b]}(x)$ with respect to the Lebesgue measure, namely $\P_X(dx) = (b - a)^{-1} \ind{[a, b]}(x) dx$.}%
on some probability space and define $X = F_P^-(U)$ and $Y = F_P^-(U)$.
We have by construction%
\sidenote{This comes from the fact that $\P(F_P^-(U) \leq x) = \P(U \leq F_P(x)) = F_P(x)$ since $U \sim \uni([0, 1])$ and since, by construction of the generalized inverse, we have that $F_P^-(u) \leq x$ is equivalent to $u \leq F_P(x)$ for any $u \in [0, 1]$ and $x \in \R$.}% 
that $X \sim P$ and $Y \sim Q$, and that
\begin{equation*}
	\P(X \leq Y) = \P(F_P^-(U) \leq F_P^-(U)) = 1
\end{equation*}
since Point~2 $\Rightarrow$ Point~3 and since Point~3 tells us that $F_P^-(p) \leq F_P^-(p)$ for any $p \in [0, 1]$. This proves Point~2 $\Rightarrow$ Point~1.

The really nice feature of Proposition~\ref{prop:stochastic-ordering} is that it allows to reformulate $P \lest Q$, which is a property regarding the \emph{distributions} $P$ and $Q$, to a property about \emph{random variables} $X$ and $Y$ with distributions $X$ and $Y$.
Let us provide two examples.
\begin{example}
	Whenever $\lambda_1 \leq \lambda_2$, we have $\expo(\lambda_2) \lest \expo(\lambda_1)$. This follows very easily from Point~2 of Proposition~\ref{prop:stochastic-ordering}.
\end{example}
\begin{example}
	\label{ex:coupling-binomial}
	Whenever $\theta_1 \leq \theta_2$, we have $\ber(n, \theta_1) \lest \ber(n, \theta_2)$. This is obtained through Point~1 (namely a coupling argument).
	Consider $U_1, \ldots, U_n$ iid $\uni([0, 1])$ and define $S_i = \# \{ k : U_k \leq \theta_i \}$ for $i \in \{ 1, 2 \}$. By construction we have $S_i \sim \bin(n, \theta_i)$, and obviously $\P(S_1 \leq S_2) = 1$ since $\theta_1 \leq \theta_2$.
\end{example}
Thanks to Example~\ref{ex:coupling-binomial} together with Proposition~\ref{prop:stochastic-ordering}, we know now that $F_{\bin(n, \theta_2)} \leq F_{\bin(n, \theta_1)}$ whenever $\theta_1 \leq \theta_2$, so that combined with Inequality~\eqref{eq:power-control-binomial1} this provides the following control of the Type~I error:
\begin{align*}
	\sup_{\theta \leq \theta_0} \P_\theta[ \wh \theta > c] &= \sup_{\theta \leq \theta_0} (1 - F_{\bin(n, \theta)}(n c))\\
	& \leq (1 - F_{\bin(n, \theta_0)}(n c)).
\end{align*}
We can find out, given $\theta_0$, $\alpha$ and $n$, a constant $c$ as small as possible that satisfies $F_{\bin(n, \theta_0)}(n c) \geq 1 - \alpha$.
This can be done numerically, since we don't have in general an explicit solution for such a constant~$c$.
Otherwise, we can use (but it leads to a slightly less powerful test) Bernstein's deviation inequality to obtain
\begin{equation*}
	\P_{\theta_0} [\wh \theta_n > c] = \P_{\theta_0}[S - n \theta_0 > c'] \leq e^{-2 c'^2 / n} = \alpha,
\end{equation*}
so that choosing $c' = \sqrt{n \log(1 / \alpha) / 2}$ gives $\sup_{\theta \leq \theta_0} \beta(\theta) \leq \alpha$, and proves that the test with rejection set 
\begin{equation*}
	R = \bigg\{ \wh \theta_n \geq \theta_0 + \sqrt{ \frac{\log(1 / \alpha)}{2n}} \bigg\}
\end{equation*}
is a test of level $\alpha$ for the hypothesis~\eqref{eq:chap03-tests-hypothesis}.
Note that we managed to quantify exactly by how much we need to increase $\theta_0$ in order to tune the test so that its Type~I error is smaller than $\alpha$.

\paragraph{Asymptotic approach.}

We can use also an asymptotic approach by considering the test with rejection set
\begin{equation*}
	R = \big\{ \wh \theta_n > \theta_0  + \delta_n \big\} \quad \text{where} \quad \delta_n	:= \sqrt{\frac{\theta_0 (1 - \theta_0)}{n}} \Phi^{-1}(1 - \alpha).
\end{equation*}
Indeed, we know by combining Example~\ref{ex:coupling-binomial} together with~\eqref{eq:portemanteau-bernoulli} that for any $\theta \leq \theta_0$ we have
\begin{equation*}
	\P_\theta[ \wh \theta_n > \theta_0  + \delta_n ] \leq \P_{\theta_0}[ \wh \theta_n > \theta_0  + \delta_n ] \goes \alpha
\end{equation*}
\todo{detailler dans la marge ?}
as $n \goes +\infty$, so that $\limsup_n \sup_{\theta \leq \theta_0} \P_\theta[ \wh \theta_n > \theta_0  + \delta_n ] \leq \alpha$, which provides an asymptotic control of the Type~I error of this test: we say that it is \emph{asymptotically of level $\alpha$}.
But what can be said about the \emph{power} of the test ?
We know that $\wh \theta_n \goas \theta$ under $\P_\theta$ and that $\delta_n \go 0$, so, \emph{under $H_1$}, namely whenever $\theta > \theta_0$, we have
\begin{equation*}
	\beta(\theta) = \P_\theta[ \wh \theta_n > \theta_0 + \delta_n] \go 1
\end{equation*}
as $n \go +\infty$, which claims that the power of the test goes to $1$.
In this case, we say that the test is \emph{consistent} or \emph{convergent}.
\begin{remark}
 	The convergence of $\beta(\theta)$ is not uniform in $\theta$ since its limit is discontinuous while $\beta(\theta)$ is continuous.
\end{remark}

\paragraph{Ancillary statistics.}

Note that an interesting pattern emerges from what we did for confidence intervals and tests.
In both cases, for the Bernoulli case, we constructed a statistic 
$\sqrt n (\wh \theta_n - \theta) / \sqrt{\theta (1 - \theta)}$ whose asymptotic distribution is $\nor(0, 1)$, namely a distribution that \emph{does not} depend on the parameter $\theta$.
This is called an asymptotically \emph{ancillary} statistic.
\begin{definition}
	Whenever $X \sim P_\theta$ and the distribution of $f_\theta(X)$ does not depend on $\theta$, we say that $f_\theta(X)$ is an \emph{ancillary} statistic.
\end{definition}
The construction of confidence intervals and tests requires such an ancillary or asymptotically ancillary statistic.
Indeed, we need to remove the dependence on $\theta$ from the distribution to be able to compute quantiles allowing to tune the coverage property of a confidence interval, or the level of a test.

\paragraph{Confidence intervals and tests.}

There is of course a strong connection between confidence intervals and tests, as explained in the following proposition.
\begin{proposition}
	If $S(X)$ is a confidence set of level $1 - \alpha$, namely $\P_\theta[ \theta \in S(X)] \geq 1 - \alpha$ for any $\theta \in \Theta$, then the test with rejection set $\{ x : D(x) \cap \Theta_0  = \emptyset\}$ is of level $\alpha$.
\end{proposition}
This proposition easily follows from the fact that $\P_\theta[ S(X) \cap \Theta_0 = \emptyset] \leq \P_\theta[ \theta \notin S(X) ] \leq \alpha$ for any $\theta \in \Theta_0$.
Confidence intervals and tests are therefore deeply intertwined notions in the sense that when you have built one of the two, you can build easily the other.

\paragraph{Types of hypothesis.} % (fold)

For $\Theta \subset \R$, we often consider one of the null hypothesis listed in Table~\ref{tab:standard-null-hypothesis}, where we provide some vocabulary.
\begin{table}[htbp]
	\centering
	\small
	\begin{tabular}{|l|l|l|}\hline
		$\Theta_0 = \{ \theta_0 \}$ & Simple hypothesis & \\ \hline
		$\Theta_0 = [\theta_0, +\infty)$ & \multirow{3}{*}{Multiple hypothesis} & \multirow{2}{*}{One-sided hypothesis} \\
		$\Theta_0 = (-\infty, \Theta_0]$ & & \\  \cline{3-3}
		$\Theta_0 = [\theta_0 - \delta, \theta_0 + \delta]$ & & Two-sided hypothesis \\ \hline
	\end{tabular}
	\caption{Some examples of standard null hypothesis.}
	\label{tab:standard-null-hypothesis}
\end{table}


A test with a one-sided null hypothesis can be obtained using a one-sided confidence interval in the opposite direction of $\Theta_0$. A test with a two-sided null hypothesis can be obtained using a (two-sided) confidence interval.
For hypothesis $H_0 : \theta = \theta_0$ versus $H_1 : \theta > \theta_0$ we use $R = \{ \wh \theta_n > \theta_0 + c \}$ while for $H_0 : \theta = \theta_0$ versus $H_1 : \theta \neq \theta_0$ we use $R = \{ | \wh \theta_n - \theta_0 | > c \}$, where $\wh \theta_n$ is some estimator of $\theta$ and where $c$ is a constant to be tuned so that the test has level $\alpha$.
Note that this is a generic recipe, that holds for any statistical model.

In Chapter~? below, we provide systematic rules to build \emph{optimal} tests in a fairly general setting, but this will require some extra concepts that we will be developed later.

\paragraph{About $p$-values.}

Consider a statistical model and a test at level $\alpha$, and keep everything 
fixed but $\alpha$.
If $\alpha$ is very small, the test has no choice but to accept $H_0$, since it has almost no slack to eventually be wrong about it.
Once again, the only way to build a test with $\alpha = 0$ is to never reject (tell all the patients that they have cancer).
With everything fixed but $\alpha$, we can expect that for some value $\alpha(X)$ (that depends on the data $X$), we have that whenever $\alpha < \alpha(X)$ then the test \emph{accepts} $H_0$ while when $\alpha > \alpha(X)$ the test \emph{rejects} $H_0$.
Such a value $\alpha(X)$ is called the $p$-value of the test.

Let $R_\alpha$ be the rejection set of some test at level $\alpha$, so that it satisfies $\sup_{\theta \in \Theta_0} \P_\theta(R_\alpha) \leq \alpha < \alpha'$ for any $\alpha' > \alpha$, which means that $R_\alpha$ also is a rejection set at level $\alpha'$.
Generally (but not always), the family $\{ R_\alpha \}_{\alpha \in [0, 1]}$ of rejection sets of a test is \emph{increasing} with respect to $\alpha$, namely $R_\alpha \subset R_\alpha'$ for any $\alpha < \alpha'$.
In this case, we can define the $p$-value as follows.
\begin{definition}
	Consider a statistical experiment with data $X$ and a statistical test with an increasing family $\{ R_\alpha \}_{\alpha \in [0, 1]}$ of rejection sets.
	The $p$-value of such a test the $X$-measurable random variable given by
	\begin{equation*}
		\alpha(X) = \inf \{ \alpha \in [0, 1] : X \in R_\alpha \}.	
	\end{equation*}
\end{definition}
Let us compute the $p$-value of one of the tests we built previously for the $\ber(\theta)$ model and the hypothesis~\eqref{eq:chap03-tests-hypothesis}.
The rejection set is given by
\begin{equation*}
	R_\alpha = \bigg\{ \wh \theta_n > \theta_0 + \Phi^{-1}(1 - \alpha) \sqrt{\frac{\theta_0 
	(1 - \theta_0)}{n}} \bigg \}
\end{equation*}
so that the $p$-value can be computed as follows:
\begin{align*}
	\alpha(X) &= \inf \bigg\{ \alpha \in [0, 1] : \wh \theta_n > \theta_0 + \theta_0 + \Phi^{-1}(1 - \alpha) \sqrt{\frac{\theta_0 (1 - \theta_0)}{n}}  \bigg\} \\
	&= \inf \bigg \{ \alpha \in [0, 1] : \alpha > 1 - \Phi\Big( \sqrt{\frac{n}{\theta_0 (1 - \theta_0)}} (\wh \theta_n - \theta_0) \Big) \bigg \} \\
	&= 1 - \Phi\Big(  \sqrt{\frac{n}{\theta_0 (1 - \theta_0)}} (\wh \theta_n - \theta_0) \Big)
\end{align*}
In practice, when using a statistical testing procedure, we \emph{do not} choose the level $\alpha$, but we compute the $p$-value using the definition of the test and the data.
A statistical software will never ask you $\alpha$ but will rather give you the value of the $p$-value.
This value quantifies, somehow, \emph{how much we are willing to believe in $H_0$.}
\todo{Exemple mise sur le marche de medicaments, les p values, attention c'est aussi critique, etc...}
For instance, if $\alpha(X) \leq 10^{-3}$ then we are strongly rejecting $H_0$, since it would require a level $\alpha < 10^{-3}$ to accept $H_0$, which is very small. 
If $\alpha(X) = 3\%$, the result of the test is rather ambiguous while $\alpha(X) = 30\%$ is a strong acceptation of $H_0$.

In many sciences, in order to publish conclusions based on experimental observations, researchers must exhibit the $p$-values of the considered statistical tests in order to justify that some effect is indeed observed.
\todo{Donner exemples en sante et en physique nucleaire ?}


\pagelayout{wide}

\section{Proofs} % (fold)
\label{sec:chap03-proofs}

This Section gathers the proofs of some results provided in this chapter.
This Section gathers the proofs of some results provided in this chapter.
This Section gathers the proofs of some results provided in this chapter.
This Section gathers the proofs of some results provided in this chapter.
This Section gathers the proofs of some results provided in this chapter.
This Section gathers the proofs of some results provided in this chapter.

\pagelayout{margin}

% section proofs (end)

% paragraph paragraph_name (end)


% section tests (end)

% section confidence_intervals (end)



\end{document}
