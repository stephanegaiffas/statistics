%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% kaobook
% LaTeX Template
% Version 1.2 (4/1/2020)
%
% This template originates from:
% https://www.LaTeXTemplates.com
%
% For the latest template development version and to make contributions:
% https://github.com/fmarotta/kaobook
%
% Authors:
% Federico Marotta (federicomarotta@mail.com)
% Based on the doctoral thesis of Ken Arroyo Ohori (https://3d.bk.tudelft.nl/ken/en)
% and on the Tufte-LaTeX class.
% Modified for LaTeX Templates by Vel (vel@latextemplates.com)
%
% License:
% CC0 1.0 Universal (see included MANIFEST.md file)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
	fontsize=11pt, % Base font size
	twoside=false, % Use different layouts for even and odd pages (in particular, if twoside=true, the margin column will be always on the outside)
	%open=any, % If twoside=true, uncomment this to force new chapters to start on any page, not only on right (odd) pages
	%chapterprefix=true, % Uncomment to use the word "Chapter" before chapter numbers everywhere they appear
	%chapterentrydots=true, % Uncomment to output dots from the chapter name to the page number in the table of contents
	numbers=noenddot, % Comment to output dots after chapter numbers; the most common values for this option are: enddot, noenddot and auto (see the KOMAScript documentation for an in-depth explanation)
	%draft=true, % If uncommented, rulers will be added in the header and footer
	%overfullrule=true, % If uncommented, overly long lines will be marked by a black box; useful for correcting spacing problems
]{kaobook}

% Set the language
\usepackage[english]{babel} % Load characters and hyphenation
\usepackage[english=british]{csquotes} % English quotes

% Load packages for testing
\usepackage{blindtext}
%\usepackage{showframe} % Uncomment to show boxes around the text area, margin, header and footer
%\usepackage{showlabels} % Uncomment to output the content of \label commands to the document where they are used

% Load the bibliography package
\usepackage{styles/kaobiblio}
\addbibresource{main.bib} % Bibliography file

% Load mathematical packages for theorems and related environments. NOTE: choose only one between 'mdftheorems' and 'plaintheorems'.
\usepackage{styles/mdftheorems}
%\usepackage{styles/plaintheorems}


\graphicspath{{images/}} % Paths in which to look for images


\DeclareMathOperator{\cA}{\mathcal A}
\DeclareMathOperator{\cB}{\mathcal B}
\DeclareMathOperator{\cC}{\mathcal C}
\DeclareMathOperator{\cD}{\mathcal D}
\DeclareMathOperator{\cE}{\mathcal E}
\DeclareMathOperator{\cF}{\mathcal F}
\DeclareMathOperator{\cG}{\mathcal G}
\DeclareMathOperator{\cM}{\mathcal M}
\DeclareMathOperator{\cN}{\mathcal N}
\DeclareMathOperator{\cP}{\mathcal P}

\DeclareMathOperator{\nor}{Normal}
\DeclareMathOperator{\ber}{Bernoulli}
\DeclareMathOperator{\bin}{Binomial}
\DeclareMathOperator{\mul}{Multinomial}

\DeclareMathOperator{\sigmoid}{sigmoid}

\newcommand{\eps}{\varepsilon}

\renewcommand{\P}{\mathbb P}
\newcommand{\E}{\mathbb E}
\newcommand{\R}{\mathbb R}
\newcommand{\var}{\mathbb V}

\newcommand{\wh}{\widehat}

\newcommand{\ind}[1]{\mathbf 1_{#1}}
\newcommand{\grad}{\nabla}


\newcommand{\mgeq}{\succcurlyeq}
\newcommand{\mleq}{\preccurlyeq}
\newcommand{\goes}{\rightarrow}

\newcommand{\norm}[1]{\|#1\|}

% \newcommand{\cB}{\Bernouilli}



\makeindex[columns=3, title=Alphabetical Index, intoc] % Make LaTeX produce the files required to compile the index

\makeglossaries % Make LaTeX produce the files required to compile the glossary

\makenomenclature % Make LaTeX produce the files required to compile the nomenclature

% Reset sidenote counter at chapters
%\counterwithin*{sidenote}{chapter}

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	BOOK INFORMATION
%----------------------------------------------------------------------------------------

\titlehead{Some stuff about statistics}
\subject{Lecture notes for the ENS course of Statistics}

\title[Some stuff about Statistics]{Some stuff about Statistics}
% \subtitle{Customise this page according to your needs}

\author[St\'ephane Ga\"iffas"]{St\'ephane Ga\"iffas\thanks{}}

\date{\today}

\publishers{}

%----------------------------------------------------------------------------------------

\frontmatter % Denotes the start of the pre-document content, uses roman numerals

%----------------------------------------------------------------------------------------
%	OPENING PAGE
%----------------------------------------------------------------------------------------

%\makeatletter
%\extratitle{
%	% In the title page, the title is vspaced by 9.5\baselineskip
%	\vspace*{9\baselineskip}
%	\vspace*{\parskip}
%	\begin{center}
%		% In the title page, \huge is set after the komafont for title
%		\usekomafont{title}\huge\@title
%	\end{center}
%}
%\makeatother

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

% \makeatletter
% \uppertitleback{\@titlehead} % Header

% \lowertitleback{
% 	\textbf{Disclaimer}\\
% 	You can edit this page to suit your needs. For instance, here we have a no copyright statement, a colophon and some other information. This page is based on the corresponding page of Ken Arroyo Ohori's thesis, with minimal changes.
	
% 	\medskip
	
% 	\textbf{No copyright}\\
% 	\cczero\ This book is released into the public domain using the CC0 code. To the extent possible under law, I waive all copyright and related or neighbouring rights to this work.
	
% 	To view a copy of the CC0 code, visit: \\\url{http://creativecommons.org/publicdomain/zero/1.0/}
	
% 	\medskip
	
% 	\textbf{Colophon} \\
% 	This document was typeset with the help of \href{https://sourceforge.net/projects/koma-script/}{\KOMAScript} and \href{https://www.latex-project.org/}{\LaTeX} using the \href{https://github.com/fmarotta/kaobook/}{kaobook} class.
	
% 	The source code of this book is available at:\\\url{https://github.com/fmarotta/kaobook}
	
% 	(You are welcome to contribute!)
	
% 	\medskip
	
% 	\textbf{Publisher} \\
% 	First printed in May 2019 by \@publishers
% }
% \makeatother

%----------------------------------------------------------------------------------------
%	DEDICATION
%----------------------------------------------------------------------------------------

% \dedication{
% 	The harmony of the world is made manifest in Form and Number, and the heart and soul and all the poetry of Natural Philosophy are embodied in the concept of mathematical beauty.\\
% 	\flushright -- D'Arcy Wentworth Thompson
% }

%----------------------------------------------------------------------------------------
%	OUTPUT TITLE PAGE AND PREVIOUS
%----------------------------------------------------------------------------------------

% Note that \maketitle outputs the pages before here

% If twoside=false, \uppertitleback and \lowertitleback are not printed
% To overcome this issue, we set twoside=semi just before printing the title pages, and set it back to false just after the title pages
\KOMAoptions{twoside=semi}
\maketitle
\KOMAoptions{twoside=false}

%----------------------------------------------------------------------------------------
%	PREFACE
%----------------------------------------------------------------------------------------

% \chapter*{Preface}
% \addcontentsline{toc}{chapter}{Preface} % Add the preface to the table of contents as a chapter

% I am of the opinion that every \LaTeX\xspace geek, at least once during 
% his life, feels the need to create his or her own class: this is what 
% happened to me and here is the result, which, however, should be seen as 
% a work still in progress. Actually, this class is not completely 
% original, but it is a blend of all the best ideas that I have found in a 
% number of guides, tutorials, blogs and tex.stackexchange.com posts. In 
% particular, the main ideas come from two sources:

% \begin{itemize}
% 	\item \href{https://3d.bk.tudelft.nl/ken/en/}{Ken Arroyo Ohori}'s 
% 	\href{https://3d.bk.tudelft.nl/ken/en/nl/ken/en/2016/04/17/a-1.5-column-layout-in-latex.html}{Doctoral 
% 	Thesis}, which served, with the author's permission, as a backbone 
% 	for the implementation of this class;
% 	\item The 
% 		\href{https://github.com/Tufte-LaTeX/tufte-latex}{Tufte-Latex 
% 			Class}, which was a model for the style.
% \end{itemize}

% The first chapter of this book is introductive and covers the most 
% essential features of the class. Next, there is a bunch of chapters 
% devoted to all the commands and environments that you may use in writing 
% a book; in particular, it will be explained how to add notes, figures 
% and tables, and references. The second part deals with the page layout 
% and design, as well as additional features like coloured boxes and 
% theorem environments.

% I started writing this class as an experiment, and as such it should be 
% regarded. Since it has always been indended for my personal use, it may 
% not be perfect but I find it quite satisfactory for the use I want to 
% make of it. I share this work in the hope that someone might find here 
% the inspiration for writing his or her own class.

% \begin{flushright}
% 	\textit{Federico Marotta}
% \end{flushright}


%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LIST OF FIGURES/TABLES
%----------------------------------------------------------------------------------------

% \begingroup % Local scope for the following commands

% % Define the style for the TOC, LOF, and LOT
% %\setstretch{1} % Uncomment to modify line spacing in the ToC
% %\hypersetup{linkcolor=blue} % Uncomment to set the colour of links in the ToC
% \setlength{\textheight}{23cm} % Manually adjust the height of the ToC pages

% % Turn on compatibility mode for the etoc package
% \etocstandarddisplaystyle % "toc display" as if etoc was not loaded
% \etocstandardlines % toc lines as if etoc was not loaded

% \tableofcontents % Output the table of contents

% \listoffigures % Output the list of figures

% % Comment both of the following lines to have the LOF and the LOT on different pages
% \let\cleardoublepage\bigskip
% \let\clearpage\bigskip

% \listoftables % Output the list of tables

% \endgroup

%----------------------------------------------------------------------------------------
%	MAIN BODY
%----------------------------------------------------------------------------------------

\mainmatter % Denotes the start of the main document content, resets page numbering and uses arabic numbers
\setchapterstyle{kao} % Choose the default chapter heading style


\setchapterpreamble[u]{\margintoc}
\chapter{Introduction}
\labch{intro}

\section{Aim of the course}

The aim of this course is, as the title indicated, to learn some stuff about statistics, and to try to exhibit some good looking mathematics from this field of applied mathematics, beyond convincing you that statistics are useful\sidenote{We won't list here, exhaustively, the numerous fields that make a regular use of mathematical statistics: marketing, medicine and more broadly health, finance, insurance, banking, etc.}

We will try to provide, all along the course, at material featuring 60\% of classical and unavoidable material from a course about statistics, and 40\% of more recent research results and some open questions.

The tentative agenda for the course is as follows:

\begin{itemize}
 	\item Modelization and the main statistical inference problems (estimation, confidence regions and tests)
 	\item Gaussian vectors and the Gaussian linear model
 	\item Theoretical guarantees and the optimality of least-squares
 	\item Estimation methods: methods of moments, maximum likelihood and other things
 	\item Exponential models and generalized linear models, logistic regression (optimal rates and some open questions)
 	\item Tests and multiple tests
 \end{itemize} 



\setchapterpreamble[u]{\margintoc}
\chapter{Statistical models}
\label{chap:statistical_models}



\section{Probability versus statistics} % (fold)
\label{sec:probability_versus_statistics}


Let us start with the most classical and simplest statistical experiment: the coin flip experience.
We launch $n$ times a coin, that we model using $n$ random variables $X_1, \ldots, X_n$ that are i.i.d\sidenote{i.i.d stands for independent and identically distributed. More about this fundamental assumption will follow later}\todo{where?}with distribution $\ber(1/2)$\sidenote{The notation $\ber(p)$ for $p \in (0, 1)$ stands for the \emph{Bernoulli distribution}: we will denote $X \sim \ber(p)$ to say that $X \in \{ 0, 1\}$ and that $\P(X = 1) = p = 1 - \P(X = 0)$.}
or even $X_i = \ind{Y_i \in A}$ where $Y_1, \ldots, Y_n$ are i.i.d random variables valued in $(E, \cE)$ and $A \in \cE$ so that $X_i \sim \ber(p)$ with $p = \P(Y_i \in A)$.


\paragraph{Probability.} % (fold)
\label{par:probability}

% paragraph probability (end)
In the field of probabilities, we suppose that $p \in (0, 1)$ is known, and we study the properties of the sequence $(X_i)_{i \geq 1}$, for instance the distribution of $S_n = \sum_{i=1}^n X_i$ that is distributed as $\bin(n, p)$,  
We know that $\P(S_n = k) = \binom{n}{k} p^k (1 - p)^{n-k}$ for $k \in \{0, \ldots, n\}$ (once again we denote $S_n \sim \bin(n, p)$.)
It is easy to see that $\E(S_n) = np$ and $\var(S_n) = np (1 - p)$.
\todo{Recall expectation and variance}
We can study its asymptotic properties, such as 
\begin{equation}
	\label{eq:lln-binomial}
	\frac{S_n}{n} \rightarrow p	\quad \text{almost surely}
\end{equation}
and
\begin{equation}
	\label{eq:tcl-binomial}
	\sqrt n \Big(\frac{S_n}{n} - p\Big) \leadsto \nor(0, p(1-p))
\end{equation}
as $n \rightarrow +\infty$\sidenote{$X_n \leadsto X$ means that $X_n$ converges to $X$ in distribution.}.
Note that statement~\eqref{eq:lln-binomial} comes from the law of large numbers, while~\eqref{eq:tcl-binomial} comes from the central limit theorem.
In the field of probability, the object of interest is the \emph{random variable}.

\paragraph{Statistics.} % (fold)
\label{par:statistics}

% paragraph statistics (end)
In statistics, the reasoning is somewhat inverse. We do not know $p$ but we observe \emph{realizations} (also called \emph{data}, \emph{samples} or \emph{observations})
\begin{equation*}
	x_i = X_i(\omega).
\end{equation*}
We can do whatever we want with the $x_i$ in order to say things about $p$.
We build measurable functions of $(X_1, \ldots, X_n)$ (that we call \emph{statistics}) in order to tell things about their common distribution (still working under the i.i.d assumption).
The object of interest in the field of statistics is the \emph{distribution of the observations}, namely $\P_{X_1}$.


\section{Statistical models and experiences} % (fold)
\label{sec:statistical_models_and_experiences}

Let us consider another very classical problem: the election poll problem, where people can vote in a set $\{A, B\}$ of candidates in a population of size $N$.
There are $N_A$ people voting for $A$ while $N - N_A$ vote for $B$, and we wish to know about $\theta_0 = N_A / N$.

We perform of poll including $n \ll N$ voters and obtain observations $x_1, \ldots, x_n \in \{ 0, 1 \}$, where $x_i = 0$ (resp. $x_i = 1$) means that voter $i$ votes for $B$ (resp. $A$).
The problem here is that $N_A$ and $N$ are so large that we can suppose that
\begin{equation*}
	(x_1, \ldots, x_n) = (X_1(\omega), \ldots, X_n(\omega))
\end{equation*}
where $X_i \sim \ber(\theta_0)$ are random variables $X_i : (\Omega, \cA, \P) \rightarrow (\{0, 1\}, \cP(\{ 0, 1\})$.

Let's look a little bit at all these mathematical objects. 
In statistics, we are mainly only interested by the fact that the observations are valued in $(\{0, 1\}, \cP(\{ 0, 1\})$ and the distribution of $\P_{X_i}$, which is in this fully carried by the value of $p \in (0, 1)$.
\todo{dire qu'on s'en fout de $\Omega, \cA, \P$ etc.}

\paragraph{Statistical model.}

The \emph{statistical model} for $X = (X_1, \ldots, X_n) \in \{0, 1\}^n$ is a family of distributions
\begin{equation}
	\{ \P_\theta^{\otimes n} : \theta \in (0, 1) \}	= \{ \ber(\theta)^{\otimes n} : \theta \in (0, 1) \}
\end{equation}
indexed here by $\theta \in (0, 1)$.

Once again, let us insist on the following: we do whatever we want with $X_1, \ldots, X_n$ but never to $\theta_0$, which is the unknown parameter.

\todo{faudrait definir $\otimes$}

\paragraph{Another (naive) example.}

The length of screws coming out of a production chain.
We want to study the production quality by selecting at random a set of $n$ screws, and we measure their length leading to random variables $X_1, \ldots, X_n \in \R$.
We can choose to model them as Gaussian variable $\nor(\mu, \sigma^2)$, where $\mu$ corresponds to the theoretical average length of the screws, while $\sigma^2$ corresponds to a variance coming from small production errors and measurement errors.
\emph{A model is a simplification of the reality.} For this screws example, we made some modeling assumptions:

\begin{description}
	\item[Distribution choice] If we choose a $\nor(\mu, \sigma^2)$, then we implicitely assume that the true underlying distribution of the screws lengths is, among other things, symmetrical\sidenote{DEFINE SYMMETRICAL} and highly concentrated around $\mu$.
	\item[The i.i.d assumption] Assuming that measurments are i.i.d means that weve been careful in the way we selected the screws produced (not the same days, not from the same producing machine, in order to avoid time and machine biases.)
\end{description}

A statistical model is \emph{always a simplification and an approximation of the truth}.
By truth we mean the true distribution $\P_{(X_1, \ldots, X_n})$ of $(X_1, \ldots, X_n)$.

Let us look at the $\nor(0, 1)$ distribution with density $\phi(x) = e^{-x^2/2} / \sqrt{2 \pi}$ supported on $\R$, while observations are usually bounded.
We can prove that if $Z \sim \nor(0, 1)$, then
\begin{equation*}
	\Big( \frac 1t - \frac{1}{t^3} \Big) \frac{1}{\sqrt{2\pi}} e^{-t^2 / 2} \leq \P(Z > t) \leq \frac{1}{t \sqrt{2 \pi}} e^{-t^2 / 2}
\end{equation*}
which means that the queues of the $\nor$ distribution are very tight.
For $t=6$ this entails that $\P(Z > t) \leq 10^{-9}$: we cannot distinguish between a variable supported in $[-6, 6]$ and a $\nor(0, 1)$.\todo{bof bof attention}

\begin{definition}
	A statistical experiment consists o
	\begin{itemize}
		\item A random object $X$ valued in a probability space $(E, \cE)$
		\item A family of distributions $\cP = \{ P_\theta : \theta \in \Theta\}$ on $(E, \cE)$.
	\end{itemize}
	We suppose that $\P_X = \P \circ X^{-1} \in \cP$. We say that $\cP$ is a \emph{statistical model} for $X$.
\end{definition}
The random variable $X : (\Omega, \cA, \P) \rightarrow (E, \cE)$ has distribution  $\P_X = \P \circ X^{-1}$ which is the probability image of $\P$ by $X$ on $(\Omega, \cA)$.

We will always suppose that there is a family $\{ \P_\theta : \theta \in \Theta\}$ on $(\Omega, \cA)$ that induce $\{ P_\theta : \theta \in \Theta\}$ on $(E, \cE)$ and we will use the notations
\begin{equation*}
	P_\theta(A) = \P_\theta(X \in A) = \P_\theta[ \{ \omega \in \Omega : X(\omega) \in A \}] \quad \forall A \in \cE.
\end{equation*}

Let us insist on the fact that $(\Omega, \cA, \P)$ is  purely mathematical build and has no interest in statistics.
We can alway assume that $X$ is the identity function and that $(\Omega, \cA) = (E, \cE)$, and let us recall the transfer formula
\begin{equation*}
	\int f(X(\omega)) \P_\theta(d \omega) = \int f(x) P_\theta(dx).
\end{equation*}
Because of this formula, we can always work only with $P_\theta$ and forget about $\P_\theta$ and the space $(\Omega, \cA)$.
We will often work with a space of parameters $\Theta \subset \R^d$, but this space can be much more complicated (such as infinite dimensional sets of functions with some smoothness properties, in this case we talk about nonparametric statistics.)

We will use the notation
\begin{equation*}
	\E_Q f(Y) := \int f(y) Q(dy)
\end{equation*}
where we implicitly assume, when computing this expectation, that $Y \sim Q$, and we wil shorten as following:
\begin{equation*}
	\E_\theta f(X) = \E_\theta f(X) = \int f(x) P_\theta(dx).
\end{equation*}
We will never work on $(\Omega, \cA, \P)$ but rather on $(E, \cE)$ and with the family $\cP$ (the statistical model).
\begin{definition}[Sampled experiment]
We have observations $X = (X_1, \ldots, X_n)$ with $X_i$ i.i.d and $\cP = \{ \P_\theta^{\otimes n} : \theta \in \Theta\}$, namely for $A = \prod_{i=1}^n A_i$ we have
\begin{align*}
	P_\theta^{\otimes n}(A) &= \P_\theta^{\otimes n}( (X_1, \ldots, X_n) \in A_1 \times \cdots A_n) = \prod_{i=1}^n \P_\theta(X_i \in A_i) \\
	&= \prod_{i=1}^n P_\theta(A_i).
\end{align*}
\end{definition}

Warning: we will quickly forget to distinguish between $\P_\theta$ and $P_\theta$ and $\P_\theta^{\otimes n}$ and $P_\theta^{\otimes n}$. 


Back to Bernoulli. We have $(E, \cE) = ({0, 1}^n, \cP(\{0, 1\}^n))$ and $P_\theta^{\otimes n} = \ber(\theta)^{\otimes n}$, $X = (X_1, \ldots, X_n)$ and $\Theta = (0, 1)$.
A ``roughly equivalent'' model is $S_n = \sum_{i=1}^n$ and $(E, \cE) = (\{ 0, \ldots, n\}, \cP(\{0, \ldots, n\}))$ with the same $\Theta = (0, 1)$.
Indeed, we have
\begin{equation*}
	\P_\theta^{\otimes n} (X_1 = x_1, \ldots, X_n = x_n | S_n=k) =
	\begin{cases}
	 & 0 \text{ if } k \neq \sum_{i=1}^n x_i \\
	 & \frac{\theta^k (1 - \theta)^{n-k}}{\binom{n}{k} 
	 \theta^k (1 - \theta)^{n-k}} = \frac{1}{\binom{n}{k} } \text{ otherwise.}
	\end{cases}
\end{equation*}
We observe that the conditional distribution of $(X_1, \ldots, X_n) | S$ \emph{does not depend} on $\theta$.
If we know $S$, then we can, without knowing $\theta$, build a sample $(X_1', \ldots, X_n')$ with the same distribution as $(X_1, \ldots, X_n)$, simply by choosing the $k$ among $n$ uniformly at random.
This means that $(X_1, \ldots, X_n)$ brings to more information about $\theta$ than $S_n$ alone.
In such a case, we say that $S_n$ is an \emph{exhaustive statistic}.

\begin{definition}
	Given a statistical experiment $X$, $\{ P_\theta : \theta \in \Theta \}$, we call \emph{statistic} any $X$-measurable function (a measurable function of $X$) which does not depend on $\theta$).
\end{definition}
\todo{doob lemma}
A statistic is therefore a quantity that we can compute using the data only.

\todo{Why $\theta_0$, we should say that there is a true parameter, well-specified model ?}

We want to say things about $\theta_0$ using $X$, but through $X$ we only have access to $\P_{\theta_0} = \P \circ X^{-1} = \P_X$.

\begin{definition}
	We say that a model $\cP = \{ P_\theta : \theta \in \Theta \}$ is \emph{identifiable} whenever the function
	\begin{align*}
		\Theta &\rightarrow \cP \\
		\theta &\mapsto P_\theta
	\end{align*}
	is injective.
\end{definition}


Identifiability is a necassary requirement when one want to perform statistical inference, namely whenever we can to find what the parameter $\theta$ is, only from data, namely realizations of $\P_\theta$.
If $\theta \mapsto \P_\theta$ is not injective, then there is no way to find back $\theta$ from $X \sim \P_\theta$.

Let us give simple examples: $\theta \mapsto \ber(\theta)$ is injective on $(0, 1)$ and $x \mapsto \ber(\sigmoid(x))$ is injective on $\R$.\sidenote{The sigmoid function is given by $\sigmoid(x) = \frac{1}{1 + e^{-x}}$. We will see later that this distribution function is of importance in statistics and machine learning.}

A stupid example of non-identifiable model is for instance $\mu \mapsto \nor(\mu^2, 1)$.
Identifiability is generally an interesting property, that you can ensure by choosing and parametrizing your model cleverly.
A more interesting example of non-identifiable model than the previous one is 
mixture models, such as the Gaussian mixture model where we consider a distribution with density
\begin{equation*}
	f_\theta(x) = \sum_{k=1}^K \pi_k \phi_{\mu_k, \Sigma_k}(x)
\end{equation*}
for any $x \in \R^d$, where $\theta = (\pi_k, \mu_k, \Sigma_k)_{k=1, \ldots, K}$ and $K \geq 1$ is an integer corresponding to the number of ``clusters'' in this mixture density.%
\sidenote{In the paramatrization of the Gaussian mixture we have $\mu_k \in \R^d$, $\Sigma_k \in \R^{d \times d}$ and $\Sigma_k \succ 0$ (meaning that $\Sigma_k$ is a symmetric positive matrix and $\pi_k \geq 0$ with $\sum_{k=1}^K \pi_k = 1$).}

A mixture density is not identifiable, since $f_{\theta} = f_{\sigma(\theta)}$
for any permutation $\sigma$ of $\{1, \ldots, K \}$, where we put $\sigma(\theta) = (\pi_{\sigma(k)}, \mu_{\sigma(k)}, \Sigma_{\sigma(k)})_{k=1, \ldots, K}$, namely we just relabel the numbering of the clusters.
Such mixture models are not identifiable, but are used often for clustering for instance (and instance of unsupervised learning, more on that later).
The situation is even worse for neural networks, in which an infinitely large number of parametrizations leads to the same cost function.

\section{Some types of models} % (fold)
\label{sec:some_types_of_models}

Whenever $\cP = \{ P_\theta : \theta \in \Theta \}$ with $\Theta \subset \R^d$ we say that $\cP$ is a \emph{parametric} model, since it is parametrized by a finite-dimensional parameter, trivial instances being $\{ \ber(\theta) : \theta \in (0, 1) \}$ for which $d=1$ and $\{ \nor(\mu, v) : (\mu, v) \in \R \times (0, +\infty) \}$ for which $d=2$.
We say that both models are \emph{dominated}, the first being dominated by the counting measure $\nu = \delta_0 + \delta_1$ on $\{ 0, 1\}$ and the second by the Lebesgue measure on $\R$.\sidenote{We recall that if $P$ and $Q$ are two measures on the same probability space, $P \ll Q$ means that the measure $P$ is \emph{absolutely continuous} with respect to $Q$, namely that $Q(A) = 0 \Rightarrow P(A)= 0$ for any measurable set $A$.}
\begin{definition}
	\label{def:dominated-model}
	We say that a model $\cP = \{ P_\theta : \theta \in \Theta \}$ is dominated if there is a $\sigma$-finite measure $\mu$ such that $P_\theta \ll \mu$ for all $\theta \in \Theta$.
\end{definition}
In Definition~\ref{def:dominated-model}, we require that the dominating measure is $\sigma$-finite, so that we can apply the Radon-Nikodym theorem: since $P_\theta \ll \mu$ for all $\theta \in \Theta$, then there is a density 
\begin{equation*}
	f_\theta = \frac{dP_\theta}{d\mu}
\end{equation*}
for all $\theta \in \Theta$.
The domination property allows to work with densities instead of distribution: a model can be therefore defined by a family of densities $\{ f_\theta : \theta \in \Theta \}$ together with a dominating measure (which is in most cases the Lebesgue measure, a counting measure, or a combination of them.)

Non-dominated models are usually pathological and uninteresting examples in statistics, such as the model
\begin{equation*}
	P_\lambda = \frac 1e \sum_{n \geq 0} \frac{1}{n!} \delta_{\lambda n}
\end{equation*}
for $\lambda \in (0, +\infty)$, which cannot be dominated by a $\sigma$-finite measure.

Let us finish this chapter with an example of non-parametric model: we observe $X_1, \ldots, X_n$ with a density $f \in F$ with respect to the Lebesgue measure, where $F$ is the set of probability densities on $[0, 1]^d$ that are $C^2$ and such that $\grad^2 f(x) \mleq L \mathbf I_d$.\todo{define identity and the fact that bold letters are for matrices only} This is the set of gradient-Lipschitz probability densities.
In this setting, we work with an infinite dimensional set of parameters $F$, and need to build a \emph{non-parametric} estimator of the unknown density $f$.


\setchapterpreamble[u]{\margintoc}
\chapter{Statistical inference}
\label{chap:statistical_inference}

In this Chapter, we will consider all along the most simple Bernoulli model, where we have i.i.d samples $X_1, \ldots, X_n$ distributed as $\ber(\theta)$ with $\theta \in (0, 1)$, hence the model $\cP = \{ \ber(\theta) : \theta \in (0, 1) \}$.
Let us start with the first inference problem: \emph{estimation}.
\todo{citer birge et son approche?}

\section{Estimation} % (fold)
\label{sec:estimation}

We want to infer $\theta$, or estimate it by finding a (measurable) function of $(X_1, \ldots, X_n)$\sidenote{Since we are doing statistics, the only thing we are allowed to use is the data...} or a function of $S_n = \sum_{i=1}^n X_i$ thereof, since $S_n$ is sufficient (see ??), so that there is no information loss if we look for a function of $S_n$ only, that we will denote
\begin{equation*}
 	\wh \theta_n = \wh \theta(X_1, \ldots, X_n).
\end{equation*}
Of course, this function \emph{cannot depend} on $\theta$, since once again, we are doing statistics.
Ideally, we want $\wh \theta_n$ to be "close" to $\theta$, since we want a good estimator, so we need to quantify closeness.
For instance, we could want $|\wh \theta_n - \theta|$ to be close to $0$ with a large probability, let us not forget that $\wh \theta_n$ is random, since it depends on the samples $(X_1, \ldots, X_n)$, so that quantifying the closeness of $\wh \theta_n$ to $\theta$ means controlling a stochastic quantity such as $|\wh \theta_n - \theta|$.
The most classical way to achieve this is by introducing the \emph{quadratic risk}.
\begin{definition}[Quadratic risk]
	Consider a statistical model with data $X$ and model with $\Theta \subset \R$ and an estimator $\wh \theta(X)$. The quadratic risk of $\wh \theta$ is given by
	\begin{equation*}
		R(\wh \theta, \theta) = \E_\theta[ (\wh \theta - \theta)^2 ] = \int_E (\wh \theta(x) - \theta)^2 P_\theta(dx).
	\end{equation*}
	We consider the quadratic risk as a function of the parameter, namely $\Theta \mapsto \R^+$ given by $\theta \mapsto R(\wh \theta, \theta)$.
\end{definition}
At this point, it's useful to recall some classical inequalities on the queues of random variables.
The Markov inequality tells us that if $Y$ is real random variable such that $\E |Y|^p < +\infty$ for some $p > 0$ then
\begin{equation*}
	\P(|Y| > t) \leq \frac{\E |Y|^p}{t^p}
\end{equation*}
for any $t > 0$.
This tells us that the more $Y$ has moments (an order $p$ moment means that $\E |Y|^p < +\infty$), with a large $p$, the more the queue of $Y$ is tight (it goes faster to $0$).
Markov's inequality entails easily the Bienaym\'e Tchebychev inequality
\begin{equation*}
	\P(|Y - \E Y| > t) \leq \frac{\var(Y)}{t^2}
\end{equation*}
whenever $\E Y^2 < +\infty$.
The Markov inequality entails, for $p=2$, that
\begin{equation}
	\label{eq:l2_entrails_proba}
	\P(|\wh \theta_n - \theta| > t) \leq \frac{R(\wh \theta, \theta)}{t^2}
\end{equation}
which entails that whenever the quadratic risk is small, then $\wh \theta$ is close to $\theta$ with a large probability.

Whenever $R(\wh \theta_n, \theta) \rightarrow 0$ with $n \rightarrow +\infty$, we say that $\wh \theta_n \rightarrow \theta$ in $L^2$\sidenote{More precisely, in $L^2(\P_\theta$}, which entails because of~\eqref{eq:l2_entrails_proba} that $\wh \theta_n \rightarrow \theta$ in probability.\sidenote{More precisely, in $\P_\theta$-probability. Let us recall at this point that $\wh \theta_n \rightarrow \theta$ in $\P_\theta$ probability whenever $\P_\theta[|\wh \theta_n - \theta| > \eps] \rightarrow 0$ as $n \rightarrow +\infty$ for any $\eps > 0$.}
When $\wh \theta_n \rightarrow \theta$ in probability, we say that $\wh \theta_n$ is a \emph{consistent} or \emph{convergent} estimator.
\begin{definition}
	We say that $\wh \theta_n$ is \emph{consistent} whenever $\P_\theta[|\wh \theta_n - \theta| > \eps] \rightarrow 0$ as $n \rightarrow +\infty$ for any $\eps > 0$ and any $\theta \in \Theta$.
	We say that it is strongly consistent whenever $\P_\theta[\wh \theta_n \rightarrow \theta] = 1$ and any $\theta \in \Theta$.
\end{definition}
In all these definitions, if $\Theta \subset \R^d$, it suffices to replace $|\cdot|$ by $\norm{\cdot}_2$ (or any other norm on $\R^d$)\sidenote{The norm $\norm{x}_2 = \sqrt{x^\top x} = (\sum_{j=1}^d x_j^2)^{1/2}$}


The bias-variance decomposition is the following decomposition of the quadratic risk between two terms: a bias term denoted $b(\wh \theta_n, \theta)$ (squared in the formula) and a variance term:
\begin{equation}
	\label{eq:bias-variance-decomposition}
	\begin{split}
	R(\wh \theta_n, \theta) &= \E_\theta[(\wh \theta_n - \theta)^2] = (\E_\theta \wh \theta_n - \theta)^2 + \var_\theta(\wh \theta_n) \\
	&= b(\wh \theta_n, \theta)^2 + \var_\theta(\wh \theta_n).		
	\end{split}
\end{equation}
When $b(\wh \theta_n, \theta) = 0$ for all $\theta \in \Theta$ we say that the estimator $\wh \theta_n$ is \emph{unbiased}.
This means that this estimator will not tend to over or under-estimate $\theta$. 

Going back to Bernoulli, we consider $\wh \theta_n = S_n / n = \sum_{i=1}^n X_i / n$, so that we have the following:
\begin{enumerate}
	\item We have $\E_\theta [\wh \theta_n] = \theta$ which means that $\wh \theta_n$ is unbiased;
	\item The bias-variance decomposition gives $R(\wh \theta_n, \theta) = \var(\wh \theta_n) = \theta (1 - \theta) / n \leq 1 / (4n) \rightarrow 0$ which means that $\wh \theta_n$ converges in $L^2$ (and that it is consistent)
	\item The law of large number tells us that $\wh \theta_n \goes \theta$ almost surely.
	\item The central limit theorem tells us that $\sqrt n (\wh \theta_n - \theta) \leadsto \nor(0, \theta(1 - \theta))$.
\end{enumerate}
The points 2--4 from above show that when $n$ is large enough, then $\wh \theta_n$ is close to $\theta$.
In practice, an estimator leads to a value: for the Bernoulli model with $n=100$ and $42$ ones you end up with a single estimated value $0.42$.
What if we want to include uncertainty in this estimation ?
Namely how confident are we about this $0.42$ value ?
This can be achieved using another inference problem, described in the next section.

\section{Confidence intervals} % (fold)
\label{sec:confidence_intervals}

Here, we 
% section confidence_intervals (end)



\end{document}
