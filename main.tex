
\documentclass[
	fontsize=11pt, % Base font size
	twoside=false, % Use different layouts for even and odd pages (in particular, if twoside=true, the margin column will be always on the outside)
	%open=any, % If twoside=true, uncomment this to force new chapters to start on any page, not only on right (odd) pages
	%chapterprefix=true, % Uncomment to use the word "Chapter" before chapter numbers everywhere they appear
	%chapterentrydots=true, % Uncomment to output dots from the chapter name to the page number in the table of contents
	numbers=noenddot, % Comment to output dots after chapter numbers; the most common values for this option are: enddot, noenddot and auto (see the KOMAScript documentation for an in-depth explanation)
	%draft=true, % If uncommented, rulers will be added in the header and footer
	%overfullrule=true, % If uncommented, overly long lines will be marked by a black box; useful for correcting spacing problems
]{kaobook}

% Set the language
\usepackage[english]{babel} % Load characters and hyphenation
\usepackage[english=british]{csquotes} % English quotes

\usepackage{bm}


% txfonts/pxfonts

% Load packages for testing
\usepackage{blindtext}
%\usepackage{showframe} % Uncomment to show boxes around the text area, margin, header and footer
%\usepackage{showlabels} % Uncomment to output the content of \label commands to the document where they are used

% Load the bibliography package
\usepackage{styles/kaobiblio}
\addbibresource{biblio.bib} % Bibliography file

% Load mathematical packages for theorems and related environments. NOTE: choose only one between 'mdftheorems' and 'plaintheorems'.
\usepackage{styles/mdftheorems}
%\usepackage{styles/plaintheorems}


\graphicspath{{assets/}} % Paths in which to look for images

% \RequirePackage{times}

\DeclareMathOperator{\cA}{{\mathcal A}}
\DeclareMathOperator{\cB}{{\mathcal B}}
\DeclareMathOperator{\cC}{{\mathcal C}}
\DeclareMathOperator{\cD}{{\mathcal D}}
\DeclareMathOperator{\cE}{{\mathcal E}}
\DeclareMathOperator{\cF}{{\mathcal F}}
\DeclareMathOperator{\cG}{{\mathcal G}}
\DeclareMathOperator{\cM}{{\mathcal M}}
\DeclareMathOperator{\cN}{{\mathcal N}}
\DeclareMathOperator{\cP}{{\mathcal P}}
\DeclareMathOperator{\cX}{{\mathcal X}}
\DeclareMathOperator{\cY}{{\mathcal Y}}
\DeclareMathOperator{\cZ}{{\mathcal Z}}

\DeclareMathOperator{\bA}{{\boldsymbol A}}
\DeclareMathOperator{\bB}{{\boldsymbol B}}
\DeclareMathOperator{\bC}{{\boldsymbol C}}
\DeclareMathOperator{\bD}{{\boldsymbol D}}
\DeclareMathOperator{\bE}{{\boldsymbol E}}
\DeclareMathOperator{\bF}{{\boldsymbol F}}
\DeclareMathOperator{\bG}{{\boldsymbol G}}
\DeclareMathOperator{\bH}{{\boldsymbol H}}
\DeclareMathOperator{\bI}{{\boldsymbol I}}
\DeclareMathOperator{\bJ}{{\boldsymbol J}}
\DeclareMathOperator{\bK}{{\boldsymbol K}}
\DeclareMathOperator{\bL}{{\boldsymbol L}}
\DeclareMathOperator{\bM}{{\boldsymbol M}}
\DeclareMathOperator{\bN}{{\boldsymbol N}}
\DeclareMathOperator{\bO}{{\boldsymbol O}}
\DeclareMathOperator{\bP}{{\boldsymbol P}}
\DeclareMathOperator{\bQ}{{\boldsymbol Q}}
\DeclareMathOperator{\bR}{{\boldsymbol R}}
\DeclareMathOperator{\bS}{{\boldsymbol S}}
\DeclareMathOperator{\bT}{{\boldsymbol T}}
\DeclareMathOperator{\bU}{{\boldsymbol U}}
\DeclareMathOperator{\bV}{{\boldsymbol V}}
\DeclareMathOperator{\bW}{{\boldsymbol W}}
\DeclareMathOperator{\bX}{{\boldsymbol X}}
\DeclareMathOperator{\bY}{{\boldsymbol Y}}
\DeclareMathOperator{\bZ}{{\boldsymbol Z}}

\DeclareMathOperator{\ba}{{\boldsymbol a}}
\DeclareMathOperator{\bb}{{\boldsymbol b}}
\DeclareMathOperator{\bc}{{\boldsymbol c}}
\DeclareMathOperator{\bd}{{\boldsymbol d}}
\DeclareMathOperator{\be}{{\boldsymbol e}}
\renewcommand{\bf}{{\boldsymbol f}}
\DeclareMathOperator{\bg}{{\boldsymbol g}}
\DeclareMathOperator{\bh}{{\boldsymbol h}}
\DeclareMathOperator{\bi}{{\boldsymbol i}}
\DeclareMathOperator{\bj}{{\boldsymbol j}}
\DeclareMathOperator{\bk}{{\boldsymbol k}}
\DeclareMathOperator{\bl}{{\boldsymbol l}}
% \DeclareMathOperator{\bm}{{\boldsymbol m}}
\DeclareMathOperator{\bn}{{\boldsymbol n}}
\DeclareMathOperator{\bo}{{\boldsymbol o}}
\DeclareMathOperator{\bp}{{\boldsymbol p}}
\DeclareMathOperator{\bq}{{\boldsymbol q}}
\DeclareMathOperator{\br}{{\boldsymbol r}}
\DeclareMathOperator{\bs}{{\boldsymbol s}}
\DeclareMathOperator{\bt}{{\boldsymbol t}}
\DeclareMathOperator{\bu}{{\boldsymbol u}}
\DeclareMathOperator{\bv}{{\boldsymbol v}}
\DeclareMathOperator{\bw}{{\boldsymbol w}}
\DeclareMathOperator{\bx}{{\boldsymbol x}}
\DeclareMathOperator{\by}{{\boldsymbol y}}
\DeclareMathOperator{\bz}{{\boldsymbol z}}

\DeclareMathOperator{\bLambda}{{\boldsymbol \Lambda}}

\DeclareMathOperator{\bone}{\boldsymbol 1}

\DeclareMathOperator{\beps}{\boldsymbol \varepsilon}
\DeclareMathOperator{\bSigma}{\boldsymbol \Sigma}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\mode}{mode}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\med}{med}
\DeclareMathOperator{\pen}{pen}
\DeclareMathOperator{\inte}{int}
\DeclareMathOperator{\dom}{dom}


\DeclareMathOperator{\ber}{Bernoulli}
\DeclareMathOperator{\bet}{Beta}
\DeclareMathOperator{\bin}{Binomial}
\DeclareMathOperator{\chisq}{ChiSq}
\DeclareMathOperator{\expo}{Exponential}
\DeclareMathOperator{\fis}{Fisher}
\DeclareMathOperator{\gam}{Gamma}
\DeclareMathOperator{\mul}{Multinomial}
\DeclareMathOperator{\nor}{Normal}
\DeclareMathOperator{\stu}{Student}
\DeclareMathOperator{\uni}{Uniform}

\DeclareMathOperator{\new}{new}

\DeclareMathOperator{\remain}{remainder}



\DeclareMathOperator{\sigmoid}{sigmoid}
\DeclareMathOperator{\leb}{Lebesgue}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\DeclareMathOperator*{\spa}{span}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\cov}{cov}

\newcommand{\eps}{\varepsilon}

\renewcommand{\P}{\mathbb P}
\newcommand{\E}{\mathbb E}
\newcommand{\R}{\mathbb R}
\newcommand{\N}{\mathbb N}
\newcommand{\var}{\mathbb V}

\newcommand{\wh}{\widehat}
\newcommand{\wt}{\widetilde}

\newcommand{\ind}[1]{\mathbf 1_{#1}}
\newcommand{\grad}{\nabla}


\newcommand{\mgeq}{\succcurlyeq}
\newcommand{\mleq}{\preccurlyeq}
\newcommand{\goes}{\rightarrow}
\newcommand{\go}{\rightarrow}

\newcommand{\norm}[1]{\| #1 \|}
\newcommand{\inr}[1]{\langle #1 \rangle}

\newcommand{\gopro}{\overset{\P}{\rightarrow}}
\newcommand{\goas}{\overset{\text{as\ }}{\rightarrow}}
\newcommand{\goqr}{\overset{\text{$L^2$\ }}{\rightarrow}}
\newcommand{\gosto}{\leadsto}


\DeclareMathOperator{\conv}{conv}

% \newcommand{\lest}{ \underset{\text{st}}{\leq}}
\newcommand{\lest}{\preceq}
% \newcommand{\gest}{\underset{\text{st}}{\qeq}}
\newcommand{\gest}{\succeq}


\DeclareMathOperator{\sign}{sign}

% \makeindex[columns=3, title=Alphabetical Index, intoc] % Make LaTeX produce the files required to compile the index

% \makeglossaries % Make LaTeX produce the files required to compile the glossary

% \makenomenclature % Make LaTeX produce the files required to compile the nomenclature

% Reset sidenote counter at chapters
%\counterwithin*{sidenote}{chapter}

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	BOOK INFORMATION
%----------------------------------------------------------------------------------------

\titlehead{Some stuff about statistics}
\subject{Lecture notes for the ENS course of Statistics}

\title[Some stuff about Statistics]{Some stuff about Statistics}
% \subtitle{Customise this page according to your needs}

\author[St\'ephane Ga\"iffas"]{St\'ephane Ga\"iffas\thanks{}}

\date{\today}

\publishers{}

%----------------------------------------------------------------------------------------

\frontmatter % Denotes the start of the pre-document content, uses roman numerals

%----------------------------------------------------------------------------------------
%	OPENING PAGE
%----------------------------------------------------------------------------------------

%\makeatletter
%\extratitle{
%	% In the title page, the title is vspaced by 9.5\baselineskip
%	\vspace*{9\baselineskip}
%	\vspace*{\parskip}
%	\begin{center}
%		% In the title page, \huge is set after the komafont for title
%		\usekomafont{title}\huge\@title
%	\end{center}
%}
%\makeatother

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

% \makeatletter
% \uppertitleback{\@titlehead} % Header

% \lowertitleback{
% 	\textbf{Disclaimer}\\
% 	You can edit this page to suit your needs. For instance, here we have a no copyright statement, a colophon and some other information. This page is based on the corresponding page of Ken Arroyo Ohori's thesis, with minimal changes.
	
% 	\medskip
	
% 	\textbf{No copyright}\\
% 	\cczero\ This book is released into the public domain using the CC0 code. To the extent possible under law, I waive all copyright and related or neighbouring rights to this work.
	
% 	To view a copy of the CC0 code, visit: \\\url{http://creativecommons.org/publicdomain/zero/1.0/}
	
% 	\medskip
	
% 	\textbf{Colophon} \\
% 	This document was typeset with the help of \href{https://sourceforge.net/projects/koma-script/}{\KOMAScript} and \href{https://www.latex-project.org/}{\LaTeX} using the \href{https://github.com/fmarotta/kaobook/}{kaobook} class.
	
% 	The source code of this book is available at:\\\url{https://github.com/fmarotta/kaobook}
	
% 	(You are welcome to contribute!)
	
% 	\medskip
	
% 	\textbf{Publisher} \\
% 	First printed in May 2019 by \@publishers
% }
% \makeatother

%----------------------------------------------------------------------------------------
%	DEDICATION
%----------------------------------------------------------------------------------------

% \dedication{
% 	The harmony of the world is made manifest in Form and Number, and the heart and soul and all the poetry of Natural Philosophy are embodied in the concept of mathematical beauty.\\
% 	\flushright -- D'Arcy Wentworth Thompson
% }

%----------------------------------------------------------------------------------------
%	OUTPUT TITLE PAGE AND PREVIOUS
%----------------------------------------------------------------------------------------

% Note that \maketitle outputs the pages before here

% If twoside=false, \uppertitleback and \lowertitleback are not printed
% To overcome this issue, we set twoside=semi just before printing the title pages, and set it back to false just after the title pages
\KOMAoptions{twoside=semi}
\maketitle
\KOMAoptions{twoside=false}

%----------------------------------------------------------------------------------------
%	PREFACE
%----------------------------------------------------------------------------------------

\chapter*{Preface}
\addcontentsline{toc}{chapter}{Preface} % Add the preface to the table of contents as a chapter


The aim of this course is, as the title indicated, to learn some stuff about statistics, and to try to exhibit some good looking mathematics from this field of applied mathematics, beyond convincing you that statistics are useful\sidenote{We won't list here, exhaustively, the numerous fields that make a regular use of mathematical statistics: marketing, medicine and more broadly health, finance, insurance, banking, etc.}

We will try to provide, all along the course, at material featuring 60\% of classical and unavoidable material from a course about statistics, and 40\% of more recent research results and some open questions.

The tentative agenda for the course is as follows:

\begin{itemize}
 	\item Modelization and the main statistical inference problems (estimation, confidence regions and tests)
 	\item Gaussian vectors and the Gaussian linear model
 	\item Theoretical guarantees and the optimality of least-squares
 	\item Estimation methods: methods of moments, maximum likelihood and other things
 	\item Exponential models and generalized linear models, logistic regression (optimal rates and some open questions)
 	\item Tests and multiple tests
 \end{itemize} 

% I am of the opinion that every \LaTeX\xspace geek, at least once during 
% his life, feels the need to create his or her own class: this is what 
% happened to me and here is the result, which, however, should be seen as 
% a work still in progress. Actually, this class is not completely 
% original, but it is a blend of all the best ideas that I have found in a 
% number of guides, tutorials, blogs and tex.stackexchange.com posts. In 
% particular, the main ideas come from two sources:

% \begin{itemize}
% 	\item \href{https://3d.bk.tudelft.nl/ken/en/}{Ken Arroyo Ohori}'s 
% 	\href{https://3d.bk.tudelft.nl/ken/en/nl/ken/en/2016/04/17/a-1.5-column-layout-in-latex.html}{Doctoral 
% 	Thesis}, which served, with the author's permission, as a backbone 
% 	for the implementation of this class;
% 	\item The 
% 		\href{https://github.com/Tufte-LaTeX/tufte-latex}{Tufte-Latex 
% 			Class}, which was a model for the style.
% \end{itemize}

% The first chapter of this book is introductive and covers the most 
% essential features of the class. Next, there is a bunch of chapters 
% devoted to all the commands and environments that you may use in writing 
% a book; in particular, it will be explained how to add notes, figures 
% and tables, and references. The second part deals with the page layout 
% and design, as well as additional features like coloured boxes and 
% theorem environments.

% I started writing this class as an experiment, and as such it should be 
% regarded. Since it has always been indended for my personal use, it may 
% not be perfect but I find it quite satisfactory for the use I want to 
% make of it. I share this work in the hope that someone might find here 
% the inspiration for writing his or her own class.

\begin{flushright}
	\textit{St\'ephane Ga\"iffas}
\end{flushright}


%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LIST OF FIGURES/TABLES
%----------------------------------------------------------------------------------------

\begingroup % Local scope for the following commands

% Define the style for the TOC, LOF, and LOT
%\setstretch{1} % Uncomment to modify line spacing in the ToC
%\hypersetup{linkcolor=blue} % Uncomment to set the colour of links in the ToC
\setlength{\textheight}{23cm} % Manually adjust the height of the ToC pages

% Turn on compatibility mode for the etoc package
\etocstandarddisplaystyle % "toc display" as if etoc was not loaded
\etocstandardlines % toc lines as if etoc was not loaded

\tableofcontents % Output the table of contents

% \listoffigures % Output the list of figures

% Comment both of the following lines to have the LOF and the LOT on different pages
% \let\cleardoublepage\bigskip
% \let\clearpage\bigskip

% \listoftables % Output the list of tables

\endgroup

%----------------------------------------------------------------------------------------
%	MAIN BODY
%----------------------------------------------------------------------------------------

\mainmatter % Denotes the start of the main document content, resets page numbering and uses arabic numbers
\setchapterstyle{kao} % Choose the default chapter heading style


\input{chap02_statistical_models}

\input{chap03_statistical_inference}

% \input{chap04_linear_regression}

% \input{chap05_bayesian_statistics}

% \input{chap06_lasso}

% \input{chap07_mle_exponential_models}


\setchapterpreamble[u]{\margintoc}
\chapter{Maximum likelihood estimation, application to exponential models}
\label{chap:maximum_likelihood_estimation}


\emph{Maximum likelihood estimation} is a fundamental and very general approach for statistical inference of model parameters.
In this chapter, we consider a statistical experiment with data $X : \Omega \go \cX$ and model $\{ P_\theta : \theta \in \Theta \}$ with $\Theta \subset \R^d$ dominated by a $\sigma$-finite measure $\mu$ on $\cX$, so that we can define the family of densities 
\begin{equation*}
	\marginnote{This uses the Radon-Nikodym theorem}
	f_\theta(x) = \frac{d P_\theta}{d \mu (x)} 
\end{equation*}
on $\cX$, see Definition~\ref{def:dominated-model} from Chapter~\ref{chap:statistical_models}.
\begin{definition}
	The \emph{likelihood} function $L : \Theta \go \R^+$ is defined as
	\begin{equation*}
		L(\theta) := L(\theta; X) = f_\theta(X).
	\end{equation*}
	We also introduce the \emph{log-likelihood} $\ell : \Theta \go \R$ given by
	\begin{equation*}
		\ell(\theta) := \ell(\theta; X) := \log f_\theta(X)
	\end{equation*}
	whenever $f_\theta(X) > 0$ almost surely for all $\theta \in \Theta$.
\end{definition}
The likelihood and log-likelihood are random functions since they depend on the data $X$.


\paragraph{Maximum likelihood estimation.} % (fold)

We want to infer $\theta$, namely we want to find $\theta_0 \in \Theta$ such that $X \sim P_{\theta_0}$.
Given the data $X$, the likelihood $L(\theta; X)$ is the ``probability'' to observe $X$ whenever the parameter is $\theta$. 
So, in order to find $\theta_0$, it is natural to look for $\theta$ that maximizes $\theta \mapsto L(\theta)$ (or equivalently $\theta \mapsto \ell(\theta)$) on $\Theta$, since such a $\theta$ would \emph{maximize the probability of observing} $X$ (since we do observe it).
This simple principle is fundamental and motivates the following definition.
\begin{definition}[Maximum Likelihood Estimator]
	We say that $\wh \theta \in \Theta$ is a \emph{maximum likelihood estimator} (MLE) if
	\begin{equation*}
		L(\wh \theta; X) = \sup_{\theta \in \Theta} L(\theta; X),
	\end{equation*}
	or equivalently that
	\begin{equation*}
		\wh \theta \in \argmax_{\theta \in \Theta} L(\theta; X).
	\end{equation*}
\end{definition}
Whenever it exists, a MLE $\wh \theta$ depends on $X$ and on the choice of the model $\{ f_\theta : \theta \in \Theta \}$.
If $X = (X_1, \ldots, X_n)$ with $X_i$ iid and density $f_\theta$ then
\begin{equation*}
	L_n(\theta) = L(\theta; X) = L(\theta; X_1, \ldots, X_n) = \prod_{i=1}^n f_\theta(X_i)
\end{equation*}
and
\begin{equation*}
	\ell_n(\theta) = \ell(\theta; X) = \ell(\theta; X_1, \ldots, X_n) = \sum_{i=1}^n \log f_\theta(X_i).
\end{equation*}
In this case we say that $L$ and $\ell$ are the likelihood and log-likelihood functions of the \emph{$n$-sampled experiment}.
The existence and uniqueness of the maximum likelihood estimator is not granted in general (even on non-pathological examples).%
\marginnote{In the next Chapter, we will see an example where the maximum likelihood estimator of logistic regression (a widely used model for classification) does not exist when data is linearly separable}

\section{A theoretical motivation} % (fold)
\label{sec:a_theoretical_motivation}

Let $X_1, \ldots, X_n$ be iid with distribution $P_{\theta_0} = f_{\theta_0} \cdot \mu$.
Assuming $\E_{\theta_0} | \log f_{\theta}(X_1) | < +\infty$ for any $\theta \in \Theta$,
we have that
\begin{equation}
	\label{eq:likelihood-kullback-limit}
	\begin{split}
	\frac 1n \ell_n(\theta_0) - \frac 1n \ell_n(\theta) &= \frac 1n \sum_{i=1}^n \big(
	 \log f_{\theta_0}(X_i) - \log f_{\theta}(X_i) \big) \\
	\marginnote{This convergence holds in $P_{\theta_0}$-probability using the law of large numbers}
	& \gopro \E_{\theta_0} \big[ \log f_{\theta_0}(X_1) - \log f_{\theta}(X_1) \big] \\
	&= h(P_{\theta_0}, P_\theta),		
	\end{split}
\end{equation}
where we introduce the quantity
\begin{equation*}
	h(P_{\theta_0}, P_\theta) = \int_{\cX} \log \Big( 
	\frac{f_{\theta_0}(x)}{f_{\theta}(x)} \Big) f_{\theta_0}(x) \mu(dx)
\end{equation*}
which is called the \emph{relative entropy} between $P_{\theta_0}$ and $P_\theta$.
This is a fundamental quantity which deserves a definition.
\begin{definition}
	\label{def:relative-entropy}
	Let $P$ and $Q$ be two probability measures on a measurable space $(\Omega, \cA)$. 
	The quantity given by
	\begin{equation*}
		h(P, Q) = \E_P \Big[ \log \Big( \frac{dP}{dQ} \Big) \Big] = \int \log \Big( \frac{dP}{dQ}(\omega) \Big)  P(d \omega)
	\end{equation*}
	when $P \ll Q$ and $h(P, Q) = +\infty$ otherwise is called the \emph{relative entropy} between $P$ and $Q$.
	It is also called the \emph{Kullback-Liebler divergence} or the \emph{information divergence}.
\end{definition}
Let us give some properties about $h(P, Q)$.
First of all, if $P \ll Q$ then
\begin{equation*}
	h(P, Q) = \E_P \Big[\log \frac{dP}{dQ} \Big] = \E_Q \Big[\frac{dP}{dQ} 
	\log \frac{dP}{dQ} \Big].
\end{equation*}
It is always well-defined, eventually it is $+\infty$ since $x \log \geq -e^{-1}$ for any $x \in (0, +\infty)$.
Also, if $P \ll Q$ then
\begin{equation*}
	\marginnote{using Jensen's inequality}
	h(P, Q) = \E_Q\Big[ \frac{dP}{dQ} \log \frac{dP}{dQ} \Big] 
	\geq \E_Q\Big[ \frac{dP}{dQ} \Big] \log \E_Q\Big[ \frac{dP}{dQ} \Big] 
	= 0,
\end{equation*}
and note also that $h(P, Q) = 0 \Leftrightarrow P = Q$ since $\phi(x) = x \log x$ is strictly convex.

Using~\eqref{eq:likelihood-kullback-limit}, we have that 
$(\ell_n(\theta_0) - \ell_n(\theta)) / n \approx h(P_{\theta_0}, P_\theta)$ for $n$ large and as explained above 
$h(P_{\theta_0}, P_\theta) = 0$ iff $P_{\theta_0} = P_{\theta}$, namely iff $\theta = \theta_0$ whenever the model is identifiable (see Definition~\ref{def:identifiable-model} from Chapter~\ref{chap:statistical_models}).
This motivates the use of the maximum likelihood estimator, since maximizing $\ell_n(\theta)$ means minimizing $(\ell_n(\theta_0) - \ell_n(\theta)) / n$, that we expect to be minimal at $\theta \approx \theta_0$ when the model is identifiable.

The MLE is a very general principle that can be used for virtually any statistical model.
Its theoretical study requires smoothness assumptions on the family of densities $f_\theta$ regarded as functions of the parameter $\theta$.
In this Chapter, we study the MLE in the specific family of \emph{exponential models} for two reasons: exponential models contain most parametric models that are of interest in practice, and  their study is interesting by itself, since they are the basis of generalized linear models that we will study in the next Chapter.
We will see that when $\{ f_\theta : \theta \in \Theta \}$ is an exponential model, the MLE is easy to study, since it corresponds to another estimation approach called \emph{method of moments}.
But, keep in mind that MLE goes way beyond the setting considered here.
\begin{example}
	Let us consider $X \sim \gam(a, \lambda)$, namely the likelihood function
	\begin{equation*}
		L(a, \lambda) = \frac{\lambda^a}{\Gamma(a)} x^{a - 1} e^{-\lambda x} = \exp( \theta^\top T(x) - g(\theta))
	\end{equation*}
	with $\theta = [a - 1 \; \lambda]^\top$, $T(x) = [\log x \; x]^\top$ and $g(\theta) = \log \Gamma(a) - a \log \lambda$.
	This is an example of a so-called \emph{exponential model}. Most parametric distributions can be written in this way (Poisson, Exponential, Binomial, Gaussian, etc.), using if necessary a reparametrization (we defined $\theta = [a - 1 \; \lambda]^\top$ in this example).
\end{example}

\section{Exponential models} % (fold)
\label{sec:exponential_models}

Let us first describe the so-called \emph{canonical exponotial model}.

\begin{definition}[Canonical exponential model]
	\label{def:canonical-exponential-model}
	Let $\mu$ be a $\sigma$-finite measure on a measurable space $\cX$ and let $T : \cX \go \R^d$ be a measurable function. 
	We define
	\begin{equation*}
		\Theta_{\dom} = \Big \{ \theta \in \R^d : Z(\theta) := \int_{\cX} e^{\theta^\top T(x)} \mu(dx) < +\infty \Big\}
	\end{equation*}
	and $\Theta = \inte(\Theta_{\dom})$, the interior of $\Theta_{\dom}$.
	We introduce the density
	\begin{equation*}
		f_\theta(x) = \exp\big(\theta^\top T(x) - \log Z(\theta) \big)
	\end{equation*}
	with respect to $\mu$ for $\theta \in \Theta$ and define $P_\theta = f_\theta \cdot \mu$. 
	The family $\{ P_\theta : \theta \in \Theta \}$ is called a \emph{canonical exponential model} and the function $\theta \mapsto Z(\theta)$ is called the \emph{partition function} of the model.
	\marginnote{We discussed sufficient statistics in Section~\ref{sec:statistics} of Chapter~\ref{chap:statistical_models}}
	Also, we call $T$ the \emph{sufficient statistic} of the model.
\end{definition}
We consider $\{ P_\theta \in \Theta \}$ instead of $\{ P_\theta \in \Theta_{\dom} \}$ since we will perform differential calculus and use the inversion theorem on this open domain.
\begin{proposition}
		The set $\Theta_{\dom}$ is convex (if it is not empty) and the function $\Theta_{\dom} \go \R$ defined by $\Theta \mapsto \log Z(\theta)$ is convex.
\end{proposition}
\begin{proof}
	Note that if $\theta_1, \theta_2 \in \Theta_{\dom}$ and $\alpha \in [0, 1]$ we have
	\begin{align*}
		Z(\alpha \theta_1 + (1 - \alpha) \theta_2) &= \int_{\cX} 
		\big( e^{\theta_1^\top T(x)} \big)^\alpha 
		\big( e^{\theta_2^\top T(x)} \big)^{1 - \alpha} \mu(dx) \\
		\marginnote{Using H\"older's inequality}
		&\leq \Big( \int_{\cX} e^{\theta_1^\top T(x)} \mu(dx) \Big)^{\alpha} \\
		& \quad \times \Big(\int_{\cX} e^{\theta_2^\top T(x)} \mu(dx) \Big)^{1 - \alpha} < +\infty
	\end{align*}
	which proves that $\alpha \theta_1 + (1 - \alpha) \theta_2 \in \Theta_{\dom}$ and also that $\log Z$ is convex since we have
	\begin{equation*}
		\log Z(\alpha \theta_1 + (1 - \alpha) \theta_2) \leq \alpha \log Z(\theta_1) + (1 - \alpha) \log Z(\theta_2). \qedhere
	\end{equation*}
\end{proof}

\begin{definition}[Canonical and minimal exponential model]
	\label{def:canonical-minimal-exponential-model}
	We say that a canonical exponential model is minimal if $T(x)$ does not belong, $\mu$-almost surely, to any hyperplane $H \subset \R^d$, namely if 
	\begin{equation*}
		\mu[ \{ x \in \cX : T(x) \in H \}] < 1
	\end{equation*}
	for any hyperplane $H$.
\end{definition}

\begin{proposition}
	If a canonical exponential model is minimal, then it is identifiable.
\end{proposition}

\begin{proof}
Let us consider $\theta_1, \theta_2 \in \Theta$ such that $\theta_1 \neq \theta_2$ and such that $P_{\theta_1} = P_{\theta_2}$.
This entails $f_{\theta_1} = f_{\theta_2}$ and
\begin{equation*}
	(\theta_1 - \theta_2)^\top T(x) - \log (Z(\theta_1) / Z(\theta_2)) = 0
\end{equation*}
 $\mu$-almost surely, which contradicts the fact that the model is minimal according to Definition~\ref{def:canonical-minimal-exponential-model}. \qedhere
\end{proof}

From now on, we suppose that the model is \emph{minimal}.
It is a natural assumption: it means that the coordinates of the sufficient statistic $T(x)$ are not almost-surely linearly redundant.
Let us recall also that $\Theta = \inte(\Theta_{\dom}) \neq \emptyset$.

\begin{theorem}
	\label{thm:canonical-exponential-model-moments}
	Consider a canonical exponential model. Its partition function $\theta \mapsto \log Z(\theta)$ is $C^\infty$ on $\Theta$ and we have
	\begin{equation*}
		\E_\theta[ |T_j(X)|^k ] < +\infty
	\end{equation*}
	 for any $j=1, \ldots, d$, any $k \in \N$ (all the moments of $T(X)$ with $X \sim P_\theta$ are finite) and any $\theta \in \Theta$. 
	 Furthermore, the following relations
	\begin{equation*}
	 	\marginnote{$\grad F(\theta)$ is the gradient of $F$ at $\theta$ while $\grad^2 F(\theta)$ is the Hessian matrix of $F$ at $\theta$}	
		\grad \log Z(\theta) = \E_\theta[T(X)] \quad \text{and} \quad \grad^2 \log Z(\theta) = \var_\theta[T(X)]
	\end{equation*}
	 hold for any $\theta \in \Theta$.
\end{theorem}
The proof of Theorem~\ref{thm:canonical-exponential-model-moments} is left as an exercise, where we just need to use dominated convergence to inverse differentiation and expectation.
Let us just do the following computation, which explains why the first moment of the sufficient statistic is equal to the gradient of the partition function:
\begin{align*}
	\grad \log Z(\theta) &= \grad \log \E_{X \sim \mu} \big[ \exp(\theta^\top T(X)) \big] \\
	&= \frac{\E_{X \sim \mu} \big[ T(X) \exp(\theta^\top T(X)) \big]}{\E_{X \sim \mu} \big[ \exp(\theta^\top T(X)) \big]} \\
	&= \E_{\theta} [T(X)],
\end{align*}
where we just used the definition of $P_\theta$ in the last equality.
\begin{corollary}
	We have $\grad^2 \log Z(\theta) \succ 0$ for all $\theta \in \Theta$ iff the model is minimal.
\end{corollary}
\begin{proof}
	For any $u \in \R^d$ we have $u^\top \grad^2 \log Z(\theta)  u = u^\top \var_\theta[T(X)] u = \var_\theta[u^\top T(X)]$ so that $\grad^2 \log Z(\theta)$ is not positive definite iff $u^\top T(x) $ is constant $\mu$ almost-surely.
\end{proof}
Note that we recover here the fact that $\theta \mapsto \log Z(\theta)$ is strictly convex when the model is minimal, since $\grad^2 \log Z(\theta) \succ 0$ for any $\theta \in \Theta$.
A consequence of this is that the differential of $S(\theta) = \grad \log Z(\theta)$, which is the Hessian matrix $\grad^2 \log Z(\theta)$, is \emph{invertible} for any $\theta \in \Theta$.

The following computation is insightful:
\begin{align*}
	h(P_\theta, P_{\theta'}) &= \E_{\theta} \Big[ \log \frac{d P_\theta}{d P_{\theta'}}(X) \Big] \\
	&= \E_\theta \Big[ (\theta - \theta')^\top T(X) 
	- \log \frac{Z(\theta)}{Z(\theta')} \Big] \\
	&= \log Z(\theta') - \log Z(\theta) -  (\theta' - \theta)^\top \grad \log Z(\theta)
\end{align*}
which means that $h(P_\theta, P_{\theta'})$ is equivalent to a local ``linearization'' of $\log Z(\theta)$ and therefore approximately equal to
\begin{equation*}
	h(P_\theta, P_{\theta'}) \approx \frac 12 (\theta' - \theta)^\top \grad^2 \log Z(\theta) (\theta' - \theta)
\end{equation*}
for $\theta \approx \theta'$. This makes a connection between the \emph{local curvature} of the model $\theta$ and its identifiability.
\todo{Natural gradient, Optimization, Fisher versus likelihood Hessian}
Another proposition goes as follows.
\begin{proposition}
	The function $\theta \mapsto \log Z(\theta)$ is injective on $\Theta$ if and only if the model is identifiable.
\end{proposition}
\begin{proof}
	We have that 
	\begin{equation*}
		h(P_\theta, P_{\theta'}) + h(P_{\theta'}, P_{\theta}) = \inr{ \grad \log Z(\theta)  - \grad \log Z(\theta'), \theta - \theta'}	
	\end{equation*}
	which is $\geq 0$ by convexity.
	If $ \log Z(\theta') =  \log Z(\theta)$ and $\theta \neq \theta'$ then $h(P_\theta, P_{\theta'}) = h(P_{\theta'}, P_\theta) = 0$ so that $P_\theta = P_{\theta'}$. Now, if $P_\theta = P_{\theta'}$ for $\theta \neq \theta'$, we have $\E_\theta[T(X)] = \E_{\theta'}[T(X)]$ and therefore $\log Z(\theta') =  \log Z(\theta)$ using Theorem~\ref{thm:canonical-exponential-model-moments}.
\end{proof}

\todo{ICI ICI ICI ICI}

We proved several properties about the function $S : \Theta \go \R^d$ given by $S(\theta) = \grad \log Z(\theta)$.
\begin{itemize}
	\item $S$ is injective on $\Theta$;
	\item $S$ is $C^\infty$ on $\Theta$;
	\item The differential of $S$ is invertible on $\Theta$.
\end{itemize}

We can therefore apply the theorem of global inversion to say that the function
\begin{align*}
	S : \Theta &\rightarrow S(\Theta)
	\theta \mapsto \grad \log Z(\theta)
\end{align*}
is a \textbf{diffeomorphism} and that $S(\Theta)$ is an open subset of $\R^d$ and that its inverse
$S^{-1}$ is also $C^\infty$.
We therefore proved the follwoing theorem.
\begin{theorem}
	In an exponential and canonical model we have that $S : \Theta \rightarrow S(\Theta)$ given by $S(\theta) = \grad \log Z(\theta)$ is a diffeomorphism, that $S(\Theta)$ is open and $S^{-1}$ is also $C^\infty$.
\end{theorem} 

\section{Maximum likelihood estimation in an exponential model} % (fold)
\label{sec:maximum_likelihood_estimation_in_an_exponential_model}

Let is consider a sample $X_1, \ldots, X_n$ iid from a canonical and minimal exponential model.
The likelihood write
\begin{equation*}
	L(\theta; x_1, \ldots, x_n) = \exp \Big( \langle \theta, \sum_{i=1}^n T(x_i) - n \log Z(\theta \rangle \Big)
\end{equation*}
namely 
\begin{equation*}
	\frac 1n \ell(\theta; x_1, \ldots, x_n) = \langle \theta, \sum_{i=1}^n T(x_i) 
	- n \log Z(\theta)
\end{equation*}
where we introduced $\bar t_n = n^{-1} \sum_{i=1}^n T(X_i)$.

\begin{proposition}
	In this model the log-likelihood if striclty concave
\end{proposition}

\begin{proof}
Obvious since $\langle \theta, \sum_{i=1}^n T(x_i) \rangle$ is linear hence concave and $\log Z(\theta)$ is stricly convex
\end{proof}

\begin{proposition}
	If there exist $\wh \theta_n \argmin_{\theta \in \Theta} \ell(theta; X_1, \ldots, X_n)$ then it satisfies
	\begin{equation*}
		 \grad \log Z(\wh \theta_n) = \bar T_n
	\end{equation*}
	and $\wh \theta \in \Theta$.
\end{proposition}
This means that whenvefer the MLE exists, it is a solution in $\Theta$ of the equaiton $ \grad \log Z(\wh \theta_n) = \bar T_n$ whcih menas htat it is equal to the estimator obtain with the so-called \emph{moments method} $\wh \theta_n = S^{-1}(\bar T_n)$.
Let us give an example.

\paragraph{Example.}

Consider a iid sample $X_1, \ldots, X_n$ with $\gam(a, \lambda)$ distribution and recall that its density can be write as
\begin{equation*}
	f_{a, \lambda}(x) = \frac{\lambda^a}{\Gamma(a)} x^{a - 1} e^{-\lambda x}
	= \exp(\theta^\top T(x) - \log Z(\theta))
\end{equation*}
where $\theta = \begin{bmatrix} a - 1 \\ \lambda \end{bmatrix}$ and $T(x) = \begin{bmatrix} \log X\\ -X \end{bmatrix}$ and
\begin{equation*}
	\grad \log Z(\theta) = 
	\begin{bmatrix}
	\frac{\Gamma'(a)}{\Gamma(a)} - \log \lambda \\
	a / \lambda	
	\end{bmatrix}
\end{equation*}
so that we beed to find solutions to the Equations
\begin{align*}
	\frac{\Gamma'(a)}{\Gamma(a)} - \log \lambda &= \frac{1}{n} \sum_{i=1}^n \log X_i  \\
	\frac {a}{\lambda} &= \frac{1}{n} \sum_{i=1}^n X_i 
\end{align*}
The solutions are not explicti, but can be easily obtained using a convex optimization algorithm, since we now that the objective is convex.


So in summary, we can say about MLE in a canonical exponential model
\begin{itemize}
	\item We can compute it, when it exists, using an optimiztion algothim, since the function $\mapsto \ell(\theta ; X_1, \ldots, X_n)$ is convex and smoooth
	\item In theorey, it is, when it exists the solution to the equation $\grad \ell_n(\theta) = 0$, and strict concavity entails tha tit si a global miniminzer of $\ell_n(\theta)$.
\end{itemize}


\begin{definition}
	Consider a model BLALBA with log-likelihood function BLABLA
	We call $\grad \ell_n(\theta)$ the \emph{score} function of the model and we call \emph{Fisher information} the matrix
	\begin{equation*}
		I_n(\theta) = \var_\theta[ \grad \ell_n(\theta) ],
	\end{equation*}
	which is the covariance matrix of the score.
\end{definition}

Note that this definition goes way beyond the particular case of exponential models considered in this Chapter.
But a nice aspect of exponential models is that, as explained above, MLE corresponds to moment of methods and BLABLA

Let us give some remarks/properties concerning the score and the Fisher information.
The score is a \emph{centered} random vector, since
\begin{align*}
	\E_\theta [\grad \ell_n(\theta)] = n \E_\theta[ (\bar T_n - \grad \log Z(\theta))] = n \E_\theta[ T(X_1) - \grad \log Z(\theta))]  = 0
\end{align*}
using the previous proposition and
\begin{equation*}
	I_n(\theta) = \var_\theta[\grad \ell_n(\theta)] = n \var_\theta[\grad \ell_n(\theta)] = n \var_\theta[T(X_1)] = n I_1(\theta)
\end{equation*}
but also $=n \grad^2 \log Z(\theta)$ usign the previous propocision

We can therefore use the multivariate TCL
\begin{equation*}
	\sqrt n (\bar T_n - \grad \log Z(\theta)) \gosto \nor(0, I_1(\theta))
\end{equation*}
but since $\wh \theta_n = S^{-1} (\bar T_n)$ we want to use the $\Delta$-method with $\varphi(x) = S^{-1}(x)$ (citer). 
This is given by the following theorem

\begin{theorem}[Multivariate $\Delta$-method]
	Let $(a_n)$ be a sequence of positive number such that $a_n \rightarrow +•\infty$ and $(X_n)$ be a sequence of random vectors $X_n, X \in \R^d$ and $\varphi$ be a function that is differentiable at $x \in \R^d$.
	Then fi $a_n( X_n - x) \gosto X$ we have
	\begin{equation*}
		a_n (\varphi(X_n) - \varphi(x)) \gosto J_f(x) X
	\end{equation*}
	where $J_f(x)$ is the Jacobian matrix of $f$ at $x$
\end{theorem}
Thisis exactly the same thing as in the univariate case.
We apply this theorem with $\varphi = S^{-1}$, so that $J_\varphi(x) = (\grad^2 \log Z(\theta))^{-1} = I_1(\theta)^{-1}$ (the inverse of the Hessian matrix of $\varphi$ at $x$)
and we end up with
\begin{equation*}
	\sqrt (\wh \theta_n - \theta) \gosto \nor(0, I_1(\theta)^{-1})
\end{equation*}
since $\var[I_1(\theta)^{-1} Z] = I_1(\theta)^{-1} \var[ Z] I_1(\theta)^{-1} = I_1(\theta)^{-1}$
This proves the following theorem

\begin{theorem}
	If $\{ P_\theta : \theta \in \Theta \}$ is a canonical exponential model in which the maximum likelihood estimator $\wh \theta_n$ exists, we have
	\begin{equation*}
		\sqrt n (\wh \theta_n - \theta) \gosto \nor(0, I_1(\theta)^{-1})
	\end{equation*}
	where $I_1(\theta)^{-1}$ is the Fisher information matrix defined at ???
\end{theorem}
This shows that the MLE is asymtotically normal and hat atthe ``asymptotic variance '' is the inversr o fthe Fisher information.
In this sense, the Fisher information quantifies the difficuly of the estimation and the performance of the MLE, using an asynptotic point of view.

Once again, this asymptotic results here hold only when the model is well-specified, namely whenever the \emph{true distribution} of data $P_X$ belongs to the model, namely whenever $P_X = P_{\theta^\star}$ for some $\theta^\star \in \Theta$.
If the model is misscpecified, then MLE quicly deteriorates BLABLA

\section{Proofs} % (fold)
\label{sec:proofs_chap_mle}


% section proofs (end)

% paragraph example_ (end)



% section maximum_likelihood_estimation_in_an_exponential_model (end)

% subsection proof_of_theorem_thm:oracle-fast (end)

% subsection proof_of_theorem_thm:oracle-slow (end)

% section proofs (end)
% section oracle_inequalities_for_the_lasso (end)

% section some_tools_from_convex_optimisation (end)


% paragraph paragraph_name (end)

% \input{chap07_mle_exponential_models}

% section exponential_models (end)

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

% The bibliography needs to be compiled with biber using your LaTeX editor, or on the command line with 'biber main' from the template directory

% \defbibnote{bibnote}{Here are the references in citation order.\par\bigskip} % Prepend this text to the bibliography
\printbibliography[heading=bibintoc, title=Bibliography] % Add the bibliography heading to the ToC, set the title of the bibliography and output the bibliography note

%----------------------------------------------------------------------------------------
%	NOMENCLATURE
%----------------------------------------------------------------------------------------

% The nomenclature needs to be compiled on the command line with 'makeindex main.nlo -s nomencl.ist -o main.nls' from the template directory

% \nomenclature{$c$}{Speed of light in a vacuum inertial frame}
% \nomenclature{$h$}{Planck constant}

% \renewcommand{\nomname}{Notation} % Rename the default 'Nomenclature'
% \renewcommand{\nompreamble}{The next list describes several symbols that will be later used within the body of the document.} % Prepend this text to the nomenclature

% \printnomenclature % Output the nomenclature

%----------------------------------------------------------------------------------------
%	GREEK ALPHABET
% 	Originally from https://gitlab.com/jim.hefferon/linear-algebra
%----------------------------------------------------------------------------------------

% \vspace{1cm}

% {\usekomafont{chapter}Greek Letters with Pronounciation} \\[2ex]
% \begin{center}
% 	\newcommand{\pronounced}[1]{\hspace*{.2em}\small\textit{#1}}
% 	\begin{tabular}{l l @{\hspace*{3em}} l l}
% 		\toprule
% 		Character & Name & Character & Name \\ 
% 		\midrule
% 		$\alpha$ & alpha \pronounced{AL-fuh} & $\nu$ & nu \pronounced{NEW} \\
% 		$\beta$ & beta \pronounced{BAY-tuh} & $\xi$, $\Xi$ & xi \pronounced{KSIGH} \\ 
% 		$\gamma$, $\Gamma$ & gamma \pronounced{GAM-muh} & o & omicron \pronounced{OM-uh-CRON} \\
% 		$\delta$, $\Delta$ & delta \pronounced{DEL-tuh} & $\pi$, $\Pi$ & pi \pronounced{PIE} \\
% 		$\epsilon$ & epsilon \pronounced{EP-suh-lon} & $\rho$ & rho \pronounced{ROW} \\
% 		$\zeta$ & zeta \pronounced{ZAY-tuh} & $\sigma$, $\Sigma$ & sigma \pronounced{SIG-muh} \\
% 		$\eta$ & eta \pronounced{AY-tuh} & $\tau$ & tau \pronounced{TOW (as in cow)} \\
% 		$\theta$, $\Theta$ & theta \pronounced{THAY-tuh} & $\upsilon$, $\Upsilon$ & upsilon \pronounced{OOP-suh-LON} \\
% 		$\iota$ & iota \pronounced{eye-OH-tuh} & $\phi$, $\Phi$ & phi \pronounced{FEE, or FI (as in hi)} \\
% 		$\kappa$ & kappa \pronounced{KAP-uh} & $\chi$ & chi \pronounced{KI (as in hi)} \\
% 		$\lambda$, $\Lambda$ & lambda \pronounced{LAM-duh} & $\psi$, $\Psi$ & psi \pronounced{SIGH, or PSIGH} \\
% 		$\mu$ & mu \pronounced{MEW} & $\omega$, $\Omega$ & omega \pronounced{oh-MAY-guh} \\
% 		\bottomrule
% 	\end{tabular} \\[1.5ex]
% 	Capitals shown are the ones that differ from Roman capitals.
% \end{center}

%----------------------------------------------------------------------------------------
%	GLOSSARY
%----------------------------------------------------------------------------------------

% The glossary needs to be compiled on the command line with 'makeglossaries main' from the template directory

% \newglossaryentry{computer}{
% 	name=computer,
% 	description={is a programmable machine that receives input, stores and manipulates data, and provides output in a useful format}
% }

% Glossary entries (used in text with e.g. \acrfull{fpsLabel} or \acrshort{fpsLabel})
% \newacronym[longplural={Frames per Second}]{fpsLabel}{FPS}{Frame per Second}
% \newacronym[longplural={Tables of Contents}]{tocLabel}{TOC}{Table of Contents}

% \setglossarystyle{listgroup} % Set the style of the glossary (see https://en.wikibooks.org/wiki/LaTeX/Glossary for a reference)
% \printglossary[title=Special Terms, toctitle=List of Terms] % Output the glossary, 'title' is the chapter heading for the glossary, toctitle is the table of contents heading

%----------------------------------------------------------------------------------------
%	INDEX
%----------------------------------------------------------------------------------------

% The index needs to be compiled on the command line with 'makeindex main' from the template directory

% \printindex % Output the index

%----------------------------------------------------------------------------------------
%	BACK COVER
%----------------------------------------------------------------------------------------

% If you have a PDF/image file that you want to use as a back cover, uncomment the following lines

%\clearpage


\end{document}


