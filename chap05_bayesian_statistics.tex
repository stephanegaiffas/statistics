


\setchapterpreamble[u]{\margintoc}
\chapter{Bayesian statistics}
\label{chap:bayesian_statistics}

Let us go back to the statistical problems we considered in Chapter~\ref{chap:statistical_inference}.
In each case, we had a set of decisions $A$, for instance a set of parameter $A = \Theta \subset \R^d$ for estimation problems, a set of binary decisions $A = \{ 0, 1 \}$ for statistial tests, etc..

\section{Elements of decision theory} % (fold)
\label{sec:elements_of_decision_theory}


We have data $X \in E$ where $E$ is the values set of the observations.
Given a statistical procedure $\delta : E \go A$, we \emph{decide} $\delta(X) \in A$.
In order to assess or to quantify a decision, we need a \emph{loss function} $\ell : \Theta \times A \go \R$.
This means that if the true parameter $\theta \in \Theta$ and if we decide $a \in A$ then we incur a loss $\ell(\theta, a)$.
\begin{example}
	For the estimation of a scalar parameter we have $\Theta = \R = A$ and $\ell(\theta, \theta') = (\theta - \theta')$ which corresponds to the quadratic risk introducion in Definitino~\ref{def:quadratic_risk}. 
\end{example}

\begin{example}
	For a statistical test with hypothesis $H_0 : \theta \in \Theta_0$ and $H_1 : \theta \in \Theta_1$ with $\{ \Theta_0, \Theta_1 \}$ a partition of the set of parameters $\Theta$, we can consider the loss satisfying $\ell(\theta, 0) = 0$ if $\theta \in \Theta_0$, $\ell(\theta, 01) = 0$ if $\theta \in \Theta_1$ and $\ell(\theta, 0) = c_0$ if $\theta \in \Theta_1$, $\ell(\theta, 1) = c_1$ if $\theta \in \Theta_0$, where both constants $c_0, c_1 > 0$ allow to tune we importance we given to the Type~I and Type~II errors.
\end{example}

\begin{definition}
	Consider a statistical experiment with data $X \in E$ and a set of parameters $\Theta$, a set $A$ of decisions and a loss function $\ell : \Theta \times A \go \R$. The \emph{risk} of a statistical procedure $\delta : E \go A$ is the function $\Theta \go \R$ given by
	\begin{equation*}
		R(\theta, \delta) = \E_\theta[ \ell(\theta, \delta(X))]
	\end{equation*}
	for any $\theta \in \Theta$.
\end{definition}

\begin{example}
	For estimation with $A = \Theta = \R$ we can choose $\ell(\theta, \theta')$ so that $R(\theta, \delta)$ is the quadratic risk, but we can used $\ell(\theta, \theta') = |\theta - \theta'|^p$ for some $p \geq 1$ as well. For tests we have $R(\theta, \delta) = c_i \P_\theta[\delta(X) = 1 - i]$ if $\theta \in \Theta_i$ for $i \in \{ 0, 1\}$ (which leads to an approach to statistical testing different from what we did in Section~\ref{sec:tests}).
\end{example}


\section{Bayes risk} % (fold)


Given two statistical procedures $\delta, \delta'$ for the same problem of statistical inference, we do not have in general that $R(\theta, \delta) < R(\theta, \delta')$ uniformly for $\theta \in \Theta$.
\todo{give bernoulli or gaussian example, exercice on Stein effect ?}
We we can do instead is to consider an ``averaged risk'': we consider a distribution $\mu$ on $\Theta$ and we use it to integrate the risk over $\Theta$.
This distribution is called the \emph{prior distribution} or simply the \emph{prior}.
\begin{definition}[Bayesian risk]
	The Bayesian risk of a procedure $\delta$ associated to the prior $\mu$ is given by
	\begin{equation*}
		R_B(\mu, \delta) = \int_{\Theta} R(\theta, \delta) \mu(d \theta) = \int_{\Theta} \mu(d \theta) 
		\int_E \ell(\theta, \delta(x)) P_\theta(dx).
	\end{equation*}
\end{definition}
Note that $R_B(\mu, \delta) \leq \max_{\theta \in \Theta} R(\theta, \delta)$ which means that the Bayes risk is always smaller than the minimax risk.
We understand this risk as an average of the risk over $\Theta$ ``weighted'' by the prior $\mu$.
\begin{example}
	For tests, the Bayes risk associated to a prior $\mu$ writes
	\begin{align*}
		R_B(\mu, \delta) &= c_0 \int_{\Theta_0} \P_\theta(\delta(X) = 1) \mu(d \theta) \\
		& \quad + c_1 \int_{\Theta_1} \P_\theta(\delta(X) = 0) \mu(d \theta)
	\end{align*}
	which is a weighted combination of the Type~I and Type~II errors averaged by the prior $\mu$.
\end{example}

But we can understand it differently as well. 
Indeed, we could say instead that the parameter $\theta$ is itself a random variable distributed as $\mu$, that we denote $T$ instead of $\theta$, and that $P_\theta$ is actually the distribution of $X$ conditionally on $T = \theta$. 
In that case we can write
\begin{equation*}
	R_B(\mu, \delta) = \E[ \ell(T, \delta(X))] = \E_Q[ \ell(T, \delta(X))]
\end{equation*}
where the expectation is computed with respect to the joint distribution of $(T, X)$, namely integration with respect to $Q$ defined by
\begin{align*}
	Q(B \times C) &= \P[ T \in B, X \in C] = \int_B \mu(d \theta) \int_C P_\theta(dx) \\
	&= \int_B P_\theta[C] \mu(d \theta).
\end{align*}
We do ``as if'' $\theta$ where random and with distribution $\mu$.
The distribution $P_\theta$ becomes the conditional distribution of $X | T = \theta$, namely
\begin{equation*}
	P_\theta[A] = \E [\ind{A}(X) | T = \theta],
\end{equation*}
or even $P_T[A] = \E [\ind{A}(X) | T]$.

In what follows, we will assume that we have a dominating measure $\lambda$ on $\Theta$ such that $\mu = g \cdot \lambda$ where $g$ is the density of $T$ on $\Theta$ with resoect tp $\lambda$ (often $\lambda$ is the lEbuesgue measure).
We suppose also that thre is a measure $\nu$ on $E$ for which $P_\theta = f(\cdot | \theta) \cdot \nu$ where similarly $\frac{d P_\theta}{d \nu} = f(x | \theta)$. 
We use here the notation $f(\cot | \theta)$ instead of $f_\theta$ to stress that it will correspond to a conditional density.
We need at this point to classify things about conditional distributions and conditional densities.

\section{About conditional distributions and densities} % (fold)
\label{sec:about_conditional_distributions_and_densities}

Let $X$ and $Y$ be random variables on the same probability space and values in sets $\cX$ and $\cY$.
For any integrable function $f(X)$, we can define the conditional expectation $\E [f(X) | Y]$ as the random variable $g(Y)$ (for some measurable function $g$) which is unique almost surely, such that
\begin{equation}
	\E[f(X) h(Y)] = \E[ g(Y) h(Y)]
\end{equation}
for any measurable and bounded function $h$.
The particular value $g(y)$ of this conditional expectation is denoted $\E[f(X) | Y = y]$. 
In particular, we have that
\begin{align*}
	\E[f(X) h(Y) | Y] &= h(Y) \E[ f(X) | Y]
\end{align*}
almost surely and
\begin{align}
	\E [\E[ f(X) | Y]] = \E[f(X)].
\end{align}
Let us suppose now that the joint distribution $\P_{X, Y}$ of $X$ and $Y$ has a density $p(x, y)$ with respect to $\lambda \otimes \nu$ on $\cX \times \cY$.
We can define the marginal distributions of $X$ and $Y$ as
\begin{equation*}
	p_X(x) = \int_{\cY} p(x, y) \mu(dx), \quad
	p_Y(y) = \int_{\cX} p(x, y) \lambda(dy)
\end{equation*}
\todo{bon y'a des $\lambda$ qui est pris avant non ?}
which correpsond to the marginal distributions $\P_X$ and $\P_Y$ of $X$ and $Y$ so that
\begin{align*}
	\E[f(X)] &= \int_{\cX} f(x) P_X(dx) = \int_{\cX} f(x) p_X(x) \lambda(dx) \\ 
	\E[f(Y)] &= \int_{\cY} g(y) P_Y(dx) = \int_{\cY} g(y) p_Y(x) \nu(dy)
\end{align*}
for any $f, g$ such that $f(X)$ and $g(Y)$ are integrabel. \todo{attention $g$ c'est pas deja pris ?}
Let us remark that
\begin{align*}
	P_{X, Y} \big[ \{ y \in \cY : p_Y(y) = 0 \} \big] &= \int_{\cX \times \cY} \ind{p_Y(y) = 0} p(x, y) \lambda(dx) \nu(dy) \\
	&= \int_{y \in \cY : p_Y(y) = 0} \nu(dy) \int_{\cX} p(x, y) \lambda (dx) \\
	&= \int_{y \in \cY : p_Y(y) = 0} p_Y(y) \nu(dy) = 0
\end{align*}
namely $P_{X, Y}[ \{ y \in \cY : p_Y(y) = 0 \}] = 0$.
So, given any density $q$ we can define
\begin{equation*}
	p_{X | Y}(x | y) = \frac{p(x, y)}{p_Y(y)} \ind{p_Y(y) > 0} + k(x) \ind{p_Y(y) = 0},
\end{equation*}
so that any version of $p_{X | Y}$ associated to the choice of $q$ are all equal $P_{X, Y}$-almost surely, since we can check immediatly that $\int_{\cX} p_{X | Y}(x | y) \lambda(dx) = 1$, so that it is a probability density with respect to $\lambda$ on $\cX$.
Moreover, if we define
\begin{equation*}
	g(y) = \int_{\cX} f(x) p_{X | Y}(x | y) \lambda(dx)
\end{equation*}
we have for any measurable bounded function $h$, because of the Fubini theorem and using the fact that $\{y \in \cY : p_Y(y) = 0\}$ is a $P_{X, Y}$ -negligeable set that
\begin{align*}
	\E[ g(Y) h(Y)] &= \int_{\cY} g(y) h(y) p_Y(y) \nu(dy) \\
	&= \int_{\{ y \in \cY : p_Y(y) > 0 \}} h(y) p_Y(y) \nu(dy) \int_{\cX} f(x) \frac{p(x, y)}{p_Y(x)} \\
	&= \int_{\cX \times \cY} f(x) h(y) p(x, y) \lambda(dx) \nu(dy) \lambda (dx)
	&= \E[ f(X) h(Y)]
\end{align*}
warning $g$ is not any function but the function such that $g(Y) = \E[ f(X) | Y]$ almost surely.
which indeed corresponds to the definition ??? of $g(Y) = \E[f(X) | Y)$.

So, we proved that we can compute conditional expectation using the formula
\begin{equation*}
	\E[f(X) | Y] = \int_{\cX} f(x) p_{X | Y}(x | Y) \lambda(dx) = \int_{\cX} f(x) \P_{X | Y} (dx)
\end{equation*}
if we denote $P_{X | Y}$ as the distribution on $\cX$ fo,ction of $Y$ with density $p_{X | Y}(x | Y)$ with respect to $\lambda$. 
We can define in the say way $p_{X | Y}$ and $\P_{X | Y}$ uniquely on the set $\{ y : p_Y(y) > 0 \}$.
As explained above the complement $\{ y : p_Y(y) > 0 \}^\complement$ has mass equal to $0$: this has no iincidence since conditional expectation are themslef uniquely defined up to a negligable set .

We call $\P_{X | Y}$ the conditional distribution of $X$ conditionally on $Y$ and $p_{X | Y}(x | y)$ the density of $X$ ``conditionally on $Y=y$''. We can of couse define $P_{Y | X}$ and $p_{Y | X}$ exacly in the ame xay.
So, we end up with the fact that, by construction of these conditional densities, the density of the joint density $p_{X, Y}$ of $(X, Y)$  satisfies
\begin{equation}
	\label{eq:cond-density-formula}
	p_{X, Y}(x, y) = p_{X | Y}(x | y) p_Y(y) = p_{Y | X}(y | x) p_X(x)
\end{equation}
$P_{X, Y}$-almost surely.

\section{Posterior distribution and Bayes estimator} % (fold)
\label{sec:posterior_distribution_and_bayes_estimator}

We saw in ??? that the joint distribution $Q$ of $(T, X)$ is defined through its marginal distribution $g$ of $T$ and the conditional density $f(x | \theta)$ of $X$ conditionally on $T = \theta$ (hence the notation $f(x | \theta)$), so that
\begin{equation*}
	Q(d \theta, dx) = g(\theta) f(x | \theta) (\lambda \otimes \nu) (d\theta, dx). 	
\end{equation*} 
The marginal density $\bar f$ of $X$ can be obtained by integrating with respect to $\theta$:
\begin{equation*}
	\bar f(x) = \int_{\Theta} f(x | \theta) g(\theta) \lambda(d \theta)
\end{equation*}
and the conditional density of $T | X = x$ can therefore we written as
\begin{equation*}
	g(\theta | x) = \frac{f(x | \theta) g(\theta)}{\bar f(x)} = \frac{f(x | \theta) g(\theta)}{\int_{\Theta} f(x | \theta') g(\theta') \lambda(d \theta')}.
\end{equation*}
If is the density of the conditional distribution denoted $Q_x$ of $T | X = x$.
We simply used here the \emph{Bayes formula} on conditional densities in order to reverse the order of the conditionning: we expressed $T | X$ from $X | T$ since the distribution of $X | T$ is specified by the model we considered

\begin{definition}
	We call $\mu = g \cdot \lambda$ the \emph{prior} distribution we call the conditional distribution $Q_x$of $T | X=x$ the \emph{posterior} distribution or simply \emph{posterior}.
\end{definition}

The Bayesian reasoning is therefore as follows: we choose a prior on $\theta$, and we compute the posterior using the data. 
A nice aspect of this approach is that we can quantify uncertainty right out of the box with such an approach since instead of producing a point estimatpr $\wh \theta_n$ in the \emph{frequentist} approach (what we did in Section???) we produce a full posterior distribution $Q_x[\cdot] = \P[T \in \cdot | X = x]$.

\paragraph{Bayes estimator.} % (fold)

Let us consider the estimation problem where $A = \Theta$ and let us use the Bayes risk as a measure to assess the error of a procedure $\delta : E \go \Theta$.
An optimal Bayesian estimator should minimize the Byaes risk. 
Another beautiful aspect of this Bayesian approach is that the minimizer of the Byaes risk can be made explitic, since using Fubini toghther with thre fact that $f(x | \theta) g(\theta) = g(\theta | x) \bar f(x)$ almost surely, we can rewrite the Bayes risk as follows:
\begin{align*}
	R_B(\mu, \delta) &= \int_{\Theta} \int_E \ell(\theta, \delta(x)) g(\theta | x) \bar f(x) \nu(dx) \lambda(d \theta) \\
	&= \int_E \bar f(x) \nu(dx) \int_{\Theta} \ell(\theta, \delta(x)) g(\theta | x) \lambda(dx) \\
	&=  \int_E \bar f(x) \nu(dx) \int_{\Theta} \ell(\theta, \delta(x)) Q_x(d \theta).
\end{align*}
We is remarkable is that in order to minimize this quantity, we need to minimize for any fixed $x \in E$ the quantity
\begin{equation*}
	\int_{\Theta} \ell(\theta, \delta(x)) Q_x(d \theta) = \E_{Q_x} [\ell(T, \delta(x))] 
	= \E [\ell(T, \delta(X)) | X = x],
\end{equation*}
namely the expectation of the loss with respect to the posterior distribution $Q_x$ of $T | X = x$.
\begin{definition}
	Any estimator $\wh \theta(X)$ (not necessarily unique) defined as 
	\begin{equation*}
		\wh \theta_(x) \in \argmin_{t \in \Theta} \int_{\Theta} \ell(\theta, t) Q_x(d \theta) 
		= \argmin_{t \in \Theta} \E_{Q_x} [\ell(T, t) ],
	\end{equation*}
	namely a minimizer of the loss averaged by the posterior distribution is called a \emph{Bayes} or \emph{Bayesian estimator} associated to the prior $\mu = g \cdot \lambda$ and to the loss $\ell$.
\end{definition}

WHenever $\ell(\theta, \theta') = (\theta - \theta')^2$ then
\begin{equation*}
	\argmin_{t \in \R} \E_{Q_x}[ (T - t)^2] = \E_{Q_x}[T]
\end{equation*}
namely the \emph{Bayes estimator with the quadratic loss is the expectation of the posterior distribution.}
If $\ell(\theta, \theta') = |\theta - \theta'|$ then 
\begin{equation*}
	\argmin_{t \in \R} \E_{Q_x}[ |(T - t| ] = \med(Q_x) = F_{Q_x}^{-1}(1/2),
\end{equation*}
namely the Bayes estimator associated to the $\ell_1$ loss is given by the median of the posterior distribution. \todo{exo proof etc}

\begin{recipe}
	On simple examples, we can compute explicitly the Bayes estimator. Given the data density $f(x | \theta)$ and  the prior density $g$, we simply wrte the joint distribution of $(T, X)$ and use the fact that the posterior density is a density, namely it must integrate with respect to $\theta$ to $1$:
	\begin{equation*}
		g(\theta | x) = \mathrm{constant}(x) f(x | \theta) g(\theta)
	\end{equation*}
	where $\mathrm{constant}(x) =  1 / \int_\Theta f(x | \theta) g(\theta) \lambda(d \theta)$, so that we can identify the posterior distribution with a carefull look, and ideally some coffee at the formula fpr $f(x | \theta) g(\theta)$ and identify a density with respect to $\theta$.
\end{recipe}

Let us give some standard examples of prior and data distribution wuth that the prior can be made epxlicit .


Consider the data distribution $X \sim \bin(n, \theta)$ and with prior $\uni([0, 1])$ on $\theta$. This means that $X | T = \theta$ has density
\begin{equation*}
	f(x | \theta) = \binom{n}{x} \theta^x (1 - \theta^{n-x} \ind{\{ 0, \ldots, n\}}(x)	
\end{equation*}
with respect to the counting measure $\nu$ on $\N$ and that the prior distribution has density $g(\theta) = \ind{[0, 1]}(\theta)$ wuth respect to the Lebuesgye measure $\lambda$ on $\R$, so that the joint distribition of $(T, X)$ has density
\begin{equation*}
	f(\theta, x) = \binom{n}{x} \theta^x (1 - \theta)^{n - x} \ind{[0, 1]}(\theta) 
	\ind{\{ 0, \ldots, n\}}(x)
\end{equation*}
with respect to the product measure $\lambda \otimes \nu$. THe posterior distribution, namely the distriution of $T | X=x$ is therefore proportional to $\mapsto \theta^x (1 - \theta)^{n - x} \ind{[0, 1]}(\theta)$. We recognize the $\bet(a, b)$ distribution ???, that as density
\begin{equation*}
	\frac{1}{\beta(a, b)} t^{a-1} (1 - t)^{b-1} \ind{[0, 1]}(t)
\end{equation*}
where we recall that $\beta(a, b) = \int_0^1 t^{a-1} (1 - t)^{b-1} d t = \Gamma(a) \Gamma(b)  / \Gamma(a + b)$.
Therefore, we have that the posterior distribution is given by
\begin{equation*}
	Q_x = \bet(x + 1, n - x + 1).
\end{equation*}
Also, we have that
\begin{equation*}
	\E[Z^k] = \frac{\beta(a + k, b)}{\beta(a, b)} = \frac{\Gamma(a + k) \Gamma(a + b)}{\Gamma(a + k + b) \Gamma(a)} = \frac{a (a + 1) \cdots (a + k -1)}{(a + b) (a + b + 1) \cdots (a + k + b - 1)}
\end{equation*}
\todo{cjeck formula}. In aprticualr we get whenver $B \sim \bet(a, b)$ that
\begin{equation*}
	\E[Z] = \frac{a}{a + b} \quad \text{and} \quad \var[Z] = \frac{ab}{(a + b)^2 (a + b + 1)}.
\end{equation*}
So, using ???, the Bayes estimator of $\theta$ for the quadratic loss is given by 
\begin{equation*}
	\E_{Q_x}[T | X=x] = \E[ \bet(x+1, n - x + 1)] = \frac{x + 1}{n + 2},
\end{equation*}
\todo{formula c'est ok comem ca?}
namely
\begin{equation*}
	\wh \theta_n^B = \frac{X + 1}{n + 2},
\end{equation*}
which is an estimator different from $\wh \theta_n = X / n$, the one we use in Section~???.
The quadratic risk of $\wh \theta_n^B$ is, using the bias-variance formula from ??? given by
\begin{align*}
	\E_\theta[ (\wh \theta_n^B - \theta)^2] &= \var_\theta[\wh \theta_n^B] + (\E_\theta[\wh \theta_n^B] - \theta)^2 \\
	&= \frac{n \theta(1 - \theta)}{(n + 2)^2} + \Big( \frac{1 - 2 \theta}{n + 2} \Big)^2 = \frac{(1 - 2 \theta)^2 + n \theta (1 - \theta)}{(n+2)^2}
\end{align*}
and the Bayes ris can be computed as 
\begin{equation*}
	\int_0^1 \Big( \frac{1 - 2 \theta}{n + 2} \Big)^2 = \frac{(1 - 2 \theta)^2 + n \theta (1 - \theta)}{(n+2)^2} d \theta = ???
\end{equation*}
\todo{une facton plus direct de le calculer ?}


Followingn Example ? it is easy to see that if the prior is $\beta(a, b)$ and the data distribution
\todo{define clearly data distributin } si $\bin(n, \theta)$ then the posterio distribution is $\bet(a + x - 1, b + n - x + 1)$. Note that for this example the prior and posterior belong to the same family of $\bet$ distributions. In such a case, we say the the $\bin$ and $\bet$ distributions are conjuguate distributions: computations cna be made explicit in such case.
Note that however, this is not often the case and BLALBA
The more general case is the Dirichlet / Multinomial distributions, that we leave as an exercice.

\todo{dire aussi qu'on peut noter abusiement $\theta \sim ???$}
Another classical example is with the Gaussian distribution.
Consider data $X_1, \ldots, X_n$ iid with distribution $\nor(\theta, \sigma^2)$ and prior $\theta \sim \cN(0, \tau^2)$.
Let us find out the posterior distribution in this case
\begin{align*}
	f_{X | T}(x | \theta) g(\theta) &= c(\sigma) \exp\Big( - \frac{1}{2 \sigma^2} \sum_{i=1}^n (x_i - \mu)^2 - \frac{\mu}{\tau^2} \Big) \\
	&= c(\sigma) \exp \Big (  -\Big( \frac{1}{2 \gamma} \mu^2 + \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i + c(x_1, \ldots, x_n) \Big)  \Big ) \\
	&= c(\sigma) \exp \Big( -\Big( \frac{1}{2 \gamma} \Big( \mu -  \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i 
	\Big)^2 + c(x_1, \ldots, x_n) \Big)  \Big )
\end{align*}
where we put $\gamma = 1 / (n / \sigma^2 + 1 / \tau^2) = \sigma^2 / (n + \sigma^2 / \tau^2)$ and where $c$ stands for uninteresing constants which entails that the posterior distribution is
\begin{equation*}
	\nor\Big( \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i, \sigma^2 / (n + \sigma^2 / \tau^2) \Big)
\end{equation*}
so that the Bayes estimator for the quadratic risk is given by 
\begin{equation*}
	\wh \theta_n^B = \frac{1}{n + \sigma^2 / \tau^2} \sum_{i=1}^n X_i.
\end{equation*}
This entails also that the Gussian distribuion is conjuguated to itself.

Another very interesting example is the Gaussian linear regression model we considered in Chapter ??? where
$Y_i = X_i^\top \theta + \eps_i$ with deterministic $X_i \in R^d$ (or everythin is doe conditionally on them) and $\eps_i \sim \nor(0, \sigma^2)$ and iid.
In the Gaussian linear regression setting, we have that $\by \sim \nor(\bX \theta, \sigma^2 \bI)$ where we recall that $\by$ $\bX$ are given by ????.
We consider this in a Bayesian setting by assuming that $\by | T = \theta = \nor(\bX \theta, \sigma^2 \bI)$ and by considering the prior distribution $\theta \sim \nor(0, \frac{1}{\lambda} \bI_d)$.
The joint distribution of $(\by, T)$ is, therefor, given by
\begin{equation*}
	f_{\by | T = \theta}(y | \theta) g(\theta) = \frac{1}{(\sigma \sqrt{2 \pi})^{n}} 
	\exp \Big( -\frac{1}{2 \sigma^2} \norm{\by - \bX \theta}^2 - \frac {\lambda}{2} \norm{\theta}^2 \Big).
\end{equation*}
What is, in this setting, the posterior distribution $\P_{\theta | \by}$ ?
\todo{ en fait c'est chiant, on a vraiment envie de simplifier ces putain de notations de merde}
This is somewhat more complicated then what we did in both previous examples, and we need the following theorem about the multivatiate Gaussian distribution to handle this example.
\begin{theorem}
	Consider two matrices $\bLambda \succ 0$ and $\bL \succ 0$ and consider 
	$X$ such that $\P_X = \nor(\mu, \bLambda^{-1})$ and $Y$ such that $\P_{Y | X = x} = \nor(\bA x + b, \bL^{-1})$. Then, we have the following:
	\begin{equation*}
		\P_Y \sim \nor( \bA \mu + b, \bL^{-1} + \bA \bLambda^{-1} \bA^\top)		
	\end{equation*}
	and
	\begin{equation*}
		\P_{X | Y = y} = \nor(\bSigma (\bA^\top \bL (y - b) + \bLambda \mu), \bSigma)
	\end{equation*}
	where $\bSigma = (\bLambda + \bA^\top \bL \bA)^{-1}$.
\end{theorem}
The proof of this results can be found in ???? Bishop ??? dire endroit exact.
This is a computational proves that makes heavy use of ???
 The proof is given in ???
In the case that interests us we have $\mu = 0$, $\bLambda^{-1} = \bI_d / \lambda$, $\bLambda = \lambda \bI_d$, $\bL^{-1} = \sigma^2 \bI_n$, $\bA = \bX$ and $b = 0$ so that
\begin{equation*}
	\bSigma = \Big( \lambda \bI_d + \frac{1}{\sigma^2} \bX^\top \bX \Big)^{-1} = \sigma^2 \Big( \bX^\top \bX  + \lambda \sigma^2 \Big)
\end{equation*}
so that the posterior is given by
\begin{equation*}
	\P_{\theta | \by} = \nor \Big( (\bX^\top \bX  + \lambda \sigma^2 \bI_d)^{-1} \bX^\top \by,
	\sigma^2 (\bX^\top \bX + \lambda \sigma^2 \bI_d)^{-1} \Big)
	 \Big)
\end{equation*}
and the Bayes estimator for the quaradtic risk writes
\begin{equation*}
	\wh \theta_n^B = (\bX^\top \bX  + \lambda \sigma^2 \bI_d)^{-1} \bX^\top \by.
\end{equation*}
In this example, the Bayes estimator coincides with the so-called MAP estimator (maximum a posterior), which is given by, when it exists, the mode of the posterior, since the Gaussian distribution is symmetrical.
Indeed, the posterior distribugion is propoertional to
\begin{equation*}
	\exp \Big( -\frac{1}{2 \sigma^2} \norm{\by - \bX \theta}^2 - \frac{\lambda}{2} \norm{\theta}^2 \Big)
\end{equation*}
so that maximizing this function with respect to $\theta$ corresponds to minimizing
\begin{equation*}
	F(\theta) = \norm{\by - \bX \theta}^2 + \sigma^2 \lambda \norm{\theta}^2.
\end{equation*}
The function $F$ is strongly convex, since its Hessian satisfies $\nabla^2 F(\theta) = 2 \bX^\top \bX + 2\sigma^2 \lambda \bI_d \mgeq \sigma^2 \lambda \bI_d \succ 0$.
So, its minimizers must cancel out its gradient
\begin{equation*}
	\nabla F(\theta) = 2 \bX^\top (\bX \theta - \by) + 2 \sigma^2 \lambda \theta,
\end{equation*}
and therefore equals
\begin{equation*}
	\wh \theta_n = (\bX^\top \bX + 2 \sigma^2 \lambda \bI_d)^{-1} \bX^\top \by.
\end{equation*}
Note that this corresponds to a \emph{regularized} or \emph{penalized} version of the least-squares estimator 
since
\begin{equation*}
	\wh \theta_\lambda = \argmin_{\theta \in \R^d} \Big( \norm{\by - \bX \theta}^2 + \pen(\theta) \Big),
\end{equation*}
with $\pen(\theta) = \sigma^2 \lambda \norm{\theta}^2$ is called the \emph{ridge penalization}.
This penalization avoid the parameters, also called the \emph{model weights} to take large values, and is the most widely used form of penalization in statistics and machine learning (and is used way beyond the least-squares regression problem considered here).
\todo{also tikonov regularization blabla}
What we proved is that, for the Gaussian linear model, a Gaussian prior on the model weights acts exactly as a penalization term, forbidding these weights to be complextely \emph{free} (in eventually take arbitrary large values, when the conditioning of $\bX$ is bad, for instance). The variance term of the prior $\theta \sim \nor(0, \lambda^{-1} \bI_d)$ is parametrized by $\lambda > 0$: whenever $\lambda$ is small, then the prior is almost "flat" and the penalization in ??? is small: we expect in this case $\wh \theta_\lambda$ to be close to the least-squares estimator $\wh \theta_n$ (and $\wh \theta_\lambda = \wh \theta$ whenever $\lambda  = 0$). On the other hand, if $\lambda$ is large, the prior is highly concentrated around $0$, which is equivaluent to a strong penalization in ???

\section{Proof of the minimax lower bound ???} % (fold)
\label{sec:proof_of_the_minimax_lower_bound_}

Now, we have all the tools to prove the lower bound side of Theorem~??? namely that 
\begin{equation}
	\inf_{\wh \theta} \sup_{P \in \cG(P_X, \sigma^2)} \E_P \norm{\wh \theta - \theta}^2 \geq \frac{\sigma^2}{n} \E [ (\wt \bSigma)^{-1}] = \frac{\sigma^2}{n} \E [ (\wt \bSigma)^{-1}] = \frac{\sigma^2}{n} \E [ (\bSigma^{-1/2} \wh \bSigma \bSigma^{-1/2})^{-1}].
\end{equation}
Let us recall recall that in the setting of Theorem~??? we have $(X_1, Y_1), \ldots, (X_n, Y_n)$ iid and that $\cG(P_X, \sigma^2)$ is the set of joint distribution on $(X, Y)$ satisfying $X \sim P_X$, $Y = X^\top \theta^* + \eps$ almost surely and $\eps$ independent of $X$ and such that $\eps \sim \nor(0, \sigma^2)$.
Let us recal that the excenss risk si given by $\cE(\wh \theta) = R(\wh \theta) - R(\theta) = \norm{\wh \theta - \theta}_{\bSigma}^2$ and that $\bSigma = \E[X X^\top] \succ 0$ with $R(\theta) = \E[(Y - X^\top \theta)^2]$.

First, let us remark that $\sup_{P \in \cG(P_X, \sigma^2)}$ corresponds to $\sup_{\theta^* \in \Theta}$ denoting $\P_{\theta^*} = P_{X, Y}$ and let us denote also $\E_{\theta^*}$, so that we can to lower bound
\begin{equation*}
	\inf_{\wh \theta} \sup_{\theta \in \Theta} \E_\theta \norm{\wh \theta - \theta}_{\bSigma}^2.
\end{equation*}
The first, and certainly main trick, is to lower bound this minimax risk by the Bayes risk. Let us choose some prior distribution $\Pi$ for $\theta$ and write
\begin{align}
	\nonumber
	\inf_{\wh \theta} \sup_{\theta \in \Theta} \E_\theta \norm{\wh \theta - \theta}_{\bSigma}^2 
	&\geq \inf_{\wh \theta} \int_{\R^d} \E_\theta \norm{\wh \theta - \theta}_{\bSigma}^2 \Pi(d \theta) \\
	\label{eq:ls-bayes-risk}
	&= \inf_{\wh \theta} \E_{\theta \sim \Pi} \E_\theta \norm{\wh \theta - \theta}_{\bSigma}^2.
\end{align}
All the following computations are performed conditionally on $X_1, \ldots, X_n$ inside the expectations.
The distribution of $\by | \theta$ is $\nor(\bX \theta, \sigma^2 \bI_n)$. We choose the prior distribution
\begin{equation*}
	\theta \sim \Pi_\lambda := \nor\Big( 0, \frac{\sigma^2}{\lambda n} \bI_d \Big)
\end{equation*}
which corresponds to what we did in Exercice ??? with $\lambda' = n \lambda / \sigma^2$.
So we know, using this exerice, that 
\begin{equation*}
	\theta | \by \sim \nor\Big( \wh \theta_\lambda, \frac{\sigma^2}{n} (\bX^\top \bX + \lambda \bI_d)^{-1} \Big)
\end{equation*}
where
\begin{equation*}
	\wh \theta_\lambda = (n^{-1} \bX^\top \bX + \lambda \bI_d)^{-1} \bX^\top \by = \argmin_{\theta \in \R^d} \Big( \frac 1n \norm{\by - \bX \theta}^2 + \lambda \norm{\theta}^2 \Big).
\end{equation*}
is the ridge-penalized least squares estimator from ???.
The second trick is that we know how to minimize the Bayes risk~\eqref{eq:ls-bayes-risk}: it can be minimized by looking for
\begin{equation*}
	\wh \theta^B \in \argmin_{\theta' \in \R^d} \int_{\R^d} \norm{\theta' - \theta}_{\bSigma}^2 \Pi_{\theta | \by}(d \theta),
\end{equation*}
which is the average of the loss function with respect to the posterior distribution $\Pi_{\theta | \by}$, as explained in ???.
But, let us remakr that if $Z$ is a random vector such that $\E \norm{Z}^2 < \infty$, then the function $F : \R^d \go \R^+$ given by $F(t) = \E \norm{Z - t}^2$ is minimized at $t^* = \E[Z]$ whenever $\Sigma \succ 0$
\todo{proof en marge}.
This entails that here, the Bayes estimator is indeed
\begin{equation*}
	\wh \theta_\lambda = \Big( \frac 1n \bX^\top \bX + \lambda \bI_d\Big)^{-1} \bX^\top \by
\end{equation*}
and we end up, by combining ??? and ??? to the lower bound 
\begin{align*}
	\inf_{\wh \theta} \sup_{\theta \in \Theta} \E_\theta \norm{\wh \theta - \theta}_{\bSigma}^2 &\geq 
	\int_{\R^d} \E_\theta \norm{\wh \theta_\lambda - \theta}_{\bSigma}^2 \Pi_\lambda(d \theta) \\
	&= \E_{\theta \sim \Pi_\lambda} \E_\theta [\cE(\wh \theta_\lambda)] 
\end{align*}
on the minimax risk, for any $\lambda > 0$, that we are able to compute exactly, thanks to the next Lemma.
Let us recall that $\wh \bSigma = n^{-1} \bX^\top \bX = n^{-1} \sum_{i=1}^n X_i X_i\top$ and introduce $\wh \bSigma_\lambda = \wh \bSigma + \lambda \bI_d$.
\begin{lemma}
	\label{lem:excess_risk_ridge}
	The excess risk of the ridge estimator $\wh \theta_\lambda$ given by ? equals
	\begin{equation*}
		\E [\cE(\wh \theta_\lambda)] = \lambda^2 \E \norm{\theta^*}_{(\wh \bSigma_\lambda)^{-1} \bSigma (\wh \bSigma_\lambda)^{-1}}^2 + \frac{\sigma^2}{n} \E \tr \Big( (\wh \bSigma_\lambda)^{-1} \bSigma (\wh \bSigma_\lambda)^{-1} \wh \bSigma \Big)
	\end{equation*}
	under the assumption that $Y_i = X_i^\top \theta^* + \eps_i$ for $\eps_i \sim \nor(0, \sigma^2)$.
\end{lemma}
We inject the formula given by Lemma~\ref{lem:excess_risk_ridge} in ??? to end up with the lower bound
\begin{equation*}
	\E_{\theta^* \sim \Pi_\lambda} \Big[ \lambda^2 \E \norm{\theta^*}_{(\wh \bSigma_\lambda)^{-1} \bSigma (\wh \bSigma_\lambda)^{-1}}^2  + \frac{\sigma^2}{n} \E \tr \Big( (\wh \bSigma_\lambda)^{-1} \bSigma (\wh \bSigma_\lambda)^{-1} \wh \bSigma \big) \Big].
\end{equation*}
So, using Fubini, and since $\E_{\theta^* \sim \Pi_\lambda} [\theta^* (\theta^*)^\top] = \frac{\sigma^2}{\lambda n} \bI_d$ by definition of $\Pi_\lambda$, we end up with
\begin{align*}
	\E_{\theta^* \sim \Pi_\lambda} \Big[ \lambda^2 & \E \norm{\theta^*}_{(\wh \bSigma_\lambda)^{-1} 
	\bSigma (\wh \bSigma_\lambda)^{-1}}^2 \Big] \\
	&= \lambda^2 \; \E \; \E_{\theta^* \sim \Pi_\lambda} \tr \Big[ (\theta^*)^\top (\wh \bSigma_\lambda)^{-1} \bSigma (\wh \bSigma_\lambda)^{-1} \theta^* \Big] \\
	&= \lambda^2 \; \E \; \E_{\theta^* \sim \Pi_\lambda} \tr \Big[(\wh \bSigma_\lambda)^{-1} \bSigma (\wh \bSigma_\lambda)^{-1} \theta^*  (\theta^*)^\top \Big] \\
	&= \frac{\sigma^2}{n} \; \E \tr \Big[(\wh \bSigma_\lambda)^{-1} \bSigma (\wh \bSigma_\lambda)^{-1}
	\lambda \bI_d \Big]
\end{align*}
where we used $\tr[x] = x$ for $x \in \R$ in the second line ???? so that the minimization becomes now
\begin{equation*}
	\frac{\sigma^2}{n} \E \tr \Big[ (\wh \bSigma_\lambda)^{-1} \bSigma (\wh \bSigma_\lambda)^{-1} (\wh \bSigma + \lambda \bI_d) \Big] = \frac{\sigma^2}{n} \E \tr \big[ (\wh \bSigma_\lambda)^{-1} \bSigma \big].
\end{equation*}
So, we proved that for any $\lambda > 0$ we have the lower bound
\begin{equation*}
	\inf_{\wh \theta} \sup_{\theta \in \Theta} \E_\theta \norm{\wh \theta - \theta}_{\bSigma}^2 \geq
	\frac{\sigma^2}{n} \E \tr \big[ (\wh \bSigma_\lambda)^{-1} \bSigma \big]
\end{equation*}
for any $\lambda > 0$.
Since $P_X$ is non-degenerate, we know that $\wh \Sigma \succ 0$ almost surely and furthermore the function 
\begin{equation*}
	\lambda \mapsto \tr \big[ (\wh \bSigma + \lambda \bI_d)^{-1} \bSigma \big] 
	= \tr \big[ (\bSigma^{-1/2} \wh \bSigma \bSigma^{-1/2} + \lambda \bSigma^{-1})^{-1} \big]
\end{equation*}
is decreasing on $(0, +\infty)$ since $\lambda_2 \bSigma^{-1} \succ \lambda_1 \bSigma^{-1}$ whenever $\lambda_2 > \lambda_1$ and is positive, so that by monotone convergence we have indeed that
\begin{equation*}
	\E \tr \big[ (\wh \bSigma_\lambda)^{-1} \bSigma \big] \go 
	\E \tr \big[ (\wh \bSigma)^{-1} \bSigma \big] = \E \tr \big[ (\wt \bSigma)^{-1} \big]
\end{equation*}
as $\lambda \go 0^+$.
This proves the lower bound stated in Theorem~? up to the proof of Lemma~\ref{lem:excess_risk_ridge}.

\paragraph{Proof of Lemma~\ref{lem:excess_risk_ridge}.}

Let us recall that $Y_i = X_i^\top \theta^* + \eps_i$ with $\eps_i | X_i \sim \nor(0, \sigma^2)$ ($\eps_i$ are independent of $X_i$).
We have
\begin{equation*}
	\frac 1n \sum_{i=1}^n Y_i X_i = \frac 1n \sum_{i=1}^n X_i X_i^\top \theta^* 
	+ \frac 1n \sum_{i=1}^n \eps_i X_i = \wh \bSigma \theta^* + \frac 1n \sum_{i=1}^n \eps_i X_i,
\end{equation*}
so that
\begin{equation*}
	\E_{\theta^*} [\cE(\wh \theta_\lambda)] 
	=  \E_{\theta^*} \norm{\wh \theta_\lambda - \theta^*}_{\bSigma}^2 
	= \E \Big\| (\wh \bSigma_\lambda)^{-1} \Big(\wh \bSigma \theta^* + \frac 1n \sum_{i=1}^n \eps_i X_i \Big) - \theta^* \Big\|_{\bSigma}^2 
\end{equation*}
but using $(\wh \bSigma_\lambda)^{-1} (\wh \bSigma + \lambda \bI_d - \lambda \bI_d) = \bI_d - \lambda 
(\wh \bSigma_\lambda)^{-1}$ we obtain
\begin{align*}
	\E_{\theta^*} [\cE(\wh \theta_\lambda)] &= \E \Big\| (\wh \bSigma_\lambda)^{-1} \frac 1n \sum_{i=1}^n \eps_i X_i - \lambda (\wh \bSigma_\lambda)^{-1} \theta^* \Big\|_{\bSigma}^2 \\
	&= \E \bigg[ \E \bigg[ \Big\| \frac 1n \sum_{i=1}^n \eps_i X_i - \lambda \theta^* \Big\|_{(\wh \bSigma_\lambda)^{-1} \bSigma (\wh \bSigma_\lambda)^{-1}}^2 \bigg| X_1, \ldots, X_n \bigg] \bigg] \\
	&= \E \bigg[ \E \bigg[ \Big\| \frac 1n \sum_{i=1}^n \eps_i X_i \Big\|_{(\wh \bSigma_\lambda)^{-1} \bSigma (\wh \bSigma_\lambda)^{-1}}^2 \bigg| X_1, \ldots, X_n \bigg] \bigg]  + \lambda^2 \E \norm{\theta^*}_{(\wh \bSigma_\lambda)^{-1} \bSigma (\wh \bSigma_\lambda)^{-1}}^2 \\
	&= \frac{\sigma^2}{n^2} \E \bigg[ \sum_{i=1}^n \norm{X_i}_{(\wh \bSigma_\lambda)^{-1} \bSigma (\wh \bSigma_\lambda)^{-1}}^2 \bigg]  + \lambda^2 \E \norm{\theta^*}_{(\wh \bSigma_\lambda)^{-1} \bSigma (\wh \bSigma_\lambda)^{-1}}^2,
\end{align*}
where we used repeatedly that $\E[\eps_i | X_1, \ldots, X_n] = 0$, $\E[\eps_i \eps_j | X_1, \ldots, X_n] = 0$ for any $i \neq j$ and $\E[\eps_i^2 | X_1, \ldots, X_n] = \sigma^2$.
But 
\begin{align*}
	\frac 1n \sum_{i=1}^n \norm{X_i}_{(\wh \bSigma_\lambda)^{-1} \bSigma (\wh \bSigma_\lambda)^{-1}}^2 
	&= \frac 1n \sum_{i=1}^n \tr \Big[ (\wh \bSigma_\lambda)^{-1} \bSigma (\wh \bSigma_\lambda)^{-1} X_i X_i^\top \Big] \\
	&= \tr \big[ (\wh \bSigma_\lambda)^{-1} \bSigma (\wh \bSigma_\lambda)^{-1} \wh \bSigma \big],
\end{align*}
which concludes the proof of Lemma~\ref{lem:excess_risk_ridge}.

